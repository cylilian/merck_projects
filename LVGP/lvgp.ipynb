{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import inv\n",
    "from scipy.linalg import cholesky\n",
    "from scipy.optimize import minimize\n",
    "from scipy.stats import norm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt.sampler import Lhs\n",
    "from skopt.space import Space\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lvgp_kernel(X1, X2, phi_full):\n",
    "\n",
    "    k = X1.shape[0]\n",
    "    p = X1.shape[1]\n",
    "    kk = X2.shape[0]\n",
    "\n",
    "    R = np.zeros((k, kk))\n",
    "    phi_full = np.array(phi_full)\n",
    "\n",
    "    if p != len(phi_full):\n",
    "        print('Shapes do not match.')\n",
    "        exit(0)\n",
    "\n",
    "    if k >= kk:\n",
    "        for i in range(kk):\n",
    "            R[:, i] = (np.power(X1.T - X2[i].reshape(-1, 1), 2) * (10 ** phi_full).reshape(-1, 1)).sum(axis=0)\n",
    "    else:\n",
    "        for i in range(k):\n",
    "            R[i, :] = (np.power(X2.T - X1[i].reshape(-1, 1), 2) * (10 ** phi_full).reshape(-1, 1)).sum(axis=0)\n",
    "\n",
    "    R = np.exp(-0.5 * R)\n",
    "\n",
    "    return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lvgp_to_latent(X_qual, lvs_qual, n_lvs_qual, p_qual, z_vec, dim_z, k):\n",
    "    \"\"\"\n",
    "    Transforms qualitative/categorical variables into latent variables.\n",
    "\n",
    "    param X_qual Matrix or data frame containing (only) the qualitative/categorical data.\n",
    "    param lvs_qual List containing levels of each qualitative variable\n",
    "    param n_lvs_qual Number of levels of each qualitative variable\n",
    "    param p_qual Number of qualitative variables\n",
    "    param z_vec Latent variable parameters, i.e., latent variable values for each level of qualitative/categorical variables\n",
    "    param dim_z Dimensionality of latent variables, usually 1 or 2\n",
    "    param k Number of data points, equal to \\code{nrow(X_qual)}\n",
    "\n",
    "    return Matrix containing transformed data\n",
    "    \"\"\"\n",
    "\n",
    "    X_qual_la = np.zeros((k, p_qual * dim_z))\n",
    "    # note: the first levels of each variable are zeros in the latent space,\n",
    "    # no need to touch upon.\n",
    "\n",
    "    start = 0\n",
    "    for i in range(p_qual):\n",
    "        n_lvs = n_lvs_qual[i]\n",
    "        lvs = lvs_qual[i]\n",
    "        end = (dim_z) * (n_lvs - 1) + start\n",
    "\n",
    "        z_i = z_vec[start: end]\n",
    "        start = end\n",
    "\n",
    "        # map\n",
    "        zstart = 0\n",
    "        for j in range(1, n_lvs):\n",
    "            zend = dim_z + zstart\n",
    "            X_qual_la[X_qual[:, i] == lvs[j], i*dim_z:(i+1)*dim_z] = z_i[zstart:zend]\n",
    "            zstart = zend\n",
    "\n",
    "    return X_qual_la"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lvgp_nll(p_quant, p_qual, lvs_qual, n_lvs_qual, dim_z, X_quant, X_qual, Y, min_eig, k, M):\n",
    "    \"\"\"\n",
    "    description Calculates the negative log-likelihood (excluding all the constant terms) as described in \\code{reference 1}.\n",
    "\n",
    "    param hyperparam Hyperparameters of the LVGP model\n",
    "    param p_quant Number of quantative variables\n",
    "    param p_qual Number of qualitative variables\n",
    "    param lvs_qual Levels of each qualitative variable\n",
    "    param n_lvs_qual Number of levels of each qualitative variable\n",
    "    param dim_z Dimensionality of latent variables, usually 1 or 2\n",
    "    param X_quant Input data of quantative variables\n",
    "    param X_qual Input data of qualitative variables\n",
    "    param Y Vector containing the outputs of data points\n",
    "    param min_eig The smallest eigen value that the correlation matrix is allowed to have, which determines the nugget added to the correlation matrix.\n",
    "    param k Number of data points, \\code{nrow(X_quant)} or \\code{nrow(X_qual)}\n",
    "    param M Vector of ones with length \\code{k}\n",
    "\n",
    "    \"\"\"\n",
    "    Y = Y.reshape(-1, 1)\n",
    "    M = M.reshape(-1, 1)\n",
    "\n",
    "    def nll(hyperparams):\n",
    "        if p_qual == 0:\n",
    "            # No qualitative variables\n",
    "            R = lvgp_kernel(X_quant, X_quant, hyperparams)\n",
    "        else:\n",
    "            z_vec = hyperparams[p_quant:]\n",
    "            X_qual_la = lvgp_to_latent(X_qual, lvs_qual, n_lvs_qual,\n",
    "                                       p_qual, z_vec, dim_z, k)\n",
    "\n",
    "            if X_quant is not None:\n",
    "                X_full = np.hstack([X_quant, X_qual_la])\n",
    "            else:\n",
    "                X_full = X_qual_la\n",
    "\n",
    "            phi_full = hyperparams[:p_quant].tolist()\n",
    "            phi_full.extend([0 for i in range(p_qual * dim_z)])\n",
    "\n",
    "            R = lvgp_kernel(X_full, X_full, phi_full)\n",
    "\n",
    "        R = (R + R.T) / 2  # why?\n",
    "\n",
    "        raw_min_eig = np.linalg.eigvalsh(R, 'U').min()\n",
    "\n",
    "        #         raw_min_eig = raw_min_eig.min()\n",
    "        if raw_min_eig < min_eig:\n",
    "            R += (np.diag(np.full(k, 1)) * (min_eig - raw_min_eig))\n",
    "\n",
    "        L = cholesky(R).T\n",
    "        LinvM = np.linalg.solve(L, M)\n",
    "        beta_hat = np.dot(LinvM.T, np.linalg.solve(L, Y) / np.sum(LinvM ** 2))\n",
    "        beta_hat = float(beta_hat)\n",
    "\n",
    "        temp = np.linalg.solve(L, Y - M*beta_hat)\n",
    "        sigma2 = np.sum(temp ** 2) / k\n",
    "        if sigma2 < 1e-300:\n",
    "            sigma2 = 1e-300\n",
    "\n",
    "        det_R = np.linalg.det(R)\n",
    "\n",
    "        if det_R < 1e-300:\n",
    "            det_R = 1e-300\n",
    "\n",
    "        n_log_l = k * np.log(sigma2) + np.log(det_R)\n",
    "        return n_log_l\n",
    "\n",
    "    return nll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optim_fun(args):\n",
    "    x0, obj_fn, kwargs = args\n",
    "    x_sol_ele = minimize(fun=obj_fn(**kwargs), x0=x0)\n",
    "    return x_sol_ele"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lvgp_fit(X, Y, ind_qual=None, dim_z=2, eps=np.power(10, np.linspace(-1, -8, 15)),\n",
    "             lb_phi_ini=-2, ub_phi_ini=2, lb_phi_lat=-8, ub_phi_lat=3,\n",
    "             lb_z=-3, ub_z=3, n_opt=8, max_iter_ini=100, max_iter_lat=20,\n",
    "             seed=123, progress=True, parallel=False, noise=False, n_cores=1):\n",
    "    \"\"\"\n",
    "    description Fits a latent-variable Gaussian process (LVGP) model to a dataset as described in \\code{reference 1}.\n",
    "    The input variables can be quantitative or qualitative/categorical or mixed.\n",
    "    The output variable is quantitative and scalar.\n",
    "\n",
    "    param X Matrix or data frame containing the inputs of training data points. Each row is a data point.\n",
    "    param Y Vector containing the outputs of training data points\n",
    "    param ind_qual Vector containing the indices of columns of qualitative/categorical variables\n",
    "    param dim_z Dimensionality of latent space, usually 1 or 2 but can be higher\n",
    "    param eps The vector of smallest eigen values that the correlation matrix is allowed to have, which determines the nugget added to the correlation matrix.\n",
    "    param lb_phi_ini,ub_phi_ini The initial lower and upper search bounds of the scale/roughness parameters (\\code{phi}) of quantitative variables\n",
    "    param lb_phi_lat,ub_phi_lat The later lower and upper search bounds of the scale/roughness parameters (\\code{phi}) of quantitative variables\n",
    "    param lb_z,ub_z The lower and upper search bounds of the latent parameters (\\code{z}) of qualitative variables\n",
    "    param n_opt The number of times the log-likelihood function is optimized\n",
    "    param max_iter_ini The maximum number of iterations for each optimization run for largest (first) eps case\n",
    "    param max_iter_lat The maximum number of iterations for each optimization run for after first eps cases\n",
    "    param seed An integer for the random number generator. Use this to make the results reproducible.\n",
    "    param progress The switch determining whether to print function run details\n",
    "    param parallel The switch determining whether to use parallel computing\n",
    "    param noise The switch for whether the data are assumed noisy\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    if progress:\n",
    "        print(\"Checking and preprocessing the inputs...\")\n",
    "\n",
    "    if not (isinstance(X, np.ndarray) or isinstance(X, pd.DataFrame)):\n",
    "        print('X must be a matrix or a data frame')\n",
    "        exit(0)\n",
    "\n",
    "    k = X.shape[0]\n",
    "    p_all = X.shape[1]\n",
    "\n",
    "    # Boolean, unsigned integer, signed integer, float, complex.\n",
    "    _NUMERIC_KINDS = set('buifc')\n",
    "\n",
    "    if ind_qual is None:  # no qualitative variables\n",
    "        p_qual = 0\n",
    "        X_qual = None\n",
    "        X_quant = X\n",
    "\n",
    "        if not (np.asarray(X_quant).dtype.kind in _NUMERIC_KINDS or np.all(np.isfinite(X_quant))):\n",
    "            print('All the elements of ind_qual must be finite numbers.')\n",
    "            exit(0)\n",
    "\n",
    "        lvs_qual = None\n",
    "        n_lvs_qual = None\n",
    "        n_z = 0\n",
    "\n",
    "    else:\n",
    "        p_qual = len(ind_qual)\n",
    "        X_qual = X[:, ind_qual]\n",
    "\n",
    "        if p_qual == p_all:\n",
    "            X_quant = None\n",
    "        else:\n",
    "            X_quant = X[:, np.array([ii for ii in range(X.shape[1]) if ii not in ind_qual])]\n",
    "            if not (np.asarray(X_quant).dtype.kind in _NUMERIC_KINDS or np.all(np.isfinite(X_quant))):\n",
    "                print('All the elements of ind_qual must be finite numbers.')\n",
    "                exit(0)\n",
    "\n",
    "        lvs_qual = [None] * p_qual\n",
    "        n_lvs_qual = [0] * p_qual\n",
    "\n",
    "        for i in range(p_qual):\n",
    "            _levels = sorted(list(set(X_qual[:, i].tolist())))\n",
    "            lvs_qual[i] = _levels\n",
    "            n_lvs_qual[i] = len(_levels)\n",
    "\n",
    "        n_z = dim_z * (sum(n_lvs_qual)-p_qual)  # number of latent params, first ones are zeros\n",
    "\n",
    "    if Y is None:\n",
    "        print('Y must be provided.')\n",
    "\n",
    "    if k != Y.shape[0]:\n",
    "        print('The number of rows (i.e., observations) in X and Y should match!')\n",
    "        exit(0)\n",
    "\n",
    "    if dim_z not in [1, 2]:\n",
    "        print('The dimensionality of latent space is uncommon!')\n",
    "\n",
    "    p_quant = p_all - p_qual\n",
    "\n",
    "    # normalization of X and Y\n",
    "    if p_quant > 0:\n",
    "        X_quant_min = np.min(X_quant, axis=0)  # .reshape(-1, 1)\n",
    "        X_quant_max = np.max(X_quant, axis=0)  # .reshape(-1, 1)\n",
    "        X_quant = (X_quant - X_quant_min) / (X_quant_max - X_quant_min)\n",
    "    else:\n",
    "        X_quant_min = None\n",
    "        X_quant_max = None\n",
    "\n",
    "    Y_min = Y.min()\n",
    "    Y_max = Y.max()\n",
    "    Y = (Y - Y_min) / (Y_max - Y_min)\n",
    "\n",
    "    # initiation for optimization\n",
    "    n_hyper = p_quant + n_z\n",
    "    lb_ini = [*[lb_phi_ini for i in range(p_quant)], *[lb_z for i in range(n_z)]]\n",
    "    ub_ini = [*[ub_phi_ini for i in range(p_quant)], *[ub_z for i in range(n_z)]]\n",
    "    lb_lat = [*[lb_phi_lat for i in range(p_quant)], *[lb_z for i in range(n_z)]]\n",
    "    ub_lat = [*[ub_phi_lat for i in range(p_quant)], *[ub_z for i in range(n_z)]]\n",
    "\n",
    "    if dim_z == 2 and p_qual != 0:\n",
    "        temp_ind = p_quant - 1\n",
    "        for i in range(p_qual):\n",
    "\n",
    "            n_lvs = n_lvs_qual[i]\n",
    "            lb_ini[temp_ind + dim_z] = -1e-4\n",
    "            ub_ini[temp_ind + dim_z] = 1e-4\n",
    "            lb_lat[temp_ind + dim_z] = -1e-4\n",
    "            ub_lat[temp_ind + dim_z] = 1e-4\n",
    "            temp_ind += dim_z * n_lvs - 2\n",
    "\n",
    "    lb_ini = np.array(lb_ini)\n",
    "    lb_lat = np.array(lb_lat)\n",
    "    ub_ini = np.array(ub_ini)\n",
    "    ub_lat = np.array(ub_lat)\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    space = Space([(0., 1.) for i in range(n_hyper)])\n",
    "    lhs = Lhs(lhs_type=\"classic\", criterion=\"maximin\", iterations=1000)\n",
    "    A = np.array(lhs.generate(space.dimensions, n_opt))\n",
    "\n",
    "    hyper0 = (A.T * (ub_ini - lb_ini).reshape(-1, 1) + lb_ini.reshape(-1, 1)).T\n",
    "\n",
    "    M = np.ones((k, 1))\n",
    "\n",
    "    # optimization runs\n",
    "    # only serial\n",
    "\n",
    "    add_input = {'p_quant': p_quant, 'p_qual': p_qual, 'lvs_qual': lvs_qual, 'n_lvs_qual': n_lvs_qual,\n",
    "                 'dim_z': dim_z, 'X_quant': X_quant,'X_qual': X_qual, 'Y': Y, 'eps_i': None,\n",
    "                 'k': k, 'M': M,'lb': lb_ini, 'ub': ub_ini, 'options': {'maxiter': max_iter_ini}}\n",
    "\n",
    "    n_try = len(eps)\n",
    "    optim_hist = {}\n",
    "    optim_hist['i_try'] = []\n",
    "    optim_hist['hyper0'] = []\n",
    "    optim_hist['hyper_sol'] = []\n",
    "    optim_hist['obj_sol'] = []\n",
    "    optim_hist['flag_sol'] = []\n",
    "\n",
    "    if progress:\n",
    "        print('Starting optimization.')\n",
    "\n",
    "    for i_try in range(n_try):\n",
    "        eps_i = eps[i_try]\n",
    "        n_opt_i = len(hyper0)\n",
    "        hyper_sol = np.zeros((n_opt_i, n_hyper))\n",
    "        obj_sol = np.zeros((n_opt_i, 1))\n",
    "        flag_sol = np.zeros((n_opt_i, 1))\n",
    "        add_input['eps_i'] = eps_i\n",
    "\n",
    "        if parallel:\n",
    "            hyper0_list = []\n",
    "            for hyper_p in hyper0:\n",
    "                hyper0_list.append([hyper_p, lvgp_nll, add_input])\n",
    "\n",
    "            pool = Pool(n_cores)\n",
    "            temp_list = pool.map(optim_fun, hyper0_list)\n",
    "            print(temp_list)\n",
    "            exit(0)\n",
    "        else:\n",
    "\n",
    "            for j in range(n_opt_i):\n",
    "                if i_try == 0:\n",
    "                    # check the nll fn\n",
    "                    temp = minimize(fun=lvgp_nll(\n",
    "                        p_quant, p_qual, lvs_qual, n_lvs_qual, dim_z,\n",
    "                        X_quant, X_qual, Y, eps_i, k, M), x0=hyper0[j].squeeze(),\n",
    "                        method='L-BFGS-B',\n",
    "                        bounds=tuple(zip(lb_ini, ub_ini)),\n",
    "                        options={'maxiter': 100})\n",
    "                else:\n",
    "                    temp = minimize(fun=lvgp_nll(\n",
    "                        p_quant, p_qual, lvs_qual, n_lvs_qual, dim_z,\n",
    "                        X_quant, X_qual, Y, eps_i, k, M), x0=hyper0[j].squeeze(),\n",
    "                        method='L-BFGS-B',\n",
    "                        bounds=tuple(zip(lb_lat, ub_lat)),\n",
    "                        options={'maxiter': 20})\n",
    "\n",
    "                hyper_sol[j] = temp.x  # best params\n",
    "                obj_sol[j] = temp.fun  # loss\n",
    "                flag_sol[j] = int(temp.success)\n",
    "\n",
    "            ID = np.argsort(obj_sol.squeeze())[0]\n",
    "            obj_sol = obj_sol[ID]\n",
    "            flag_sol = flag_sol[ID]\n",
    "            hyper_sol = hyper_sol[ID]\n",
    "\n",
    "        optim_hist['i_try'].append(i_try)\n",
    "        optim_hist['hyper0'].append(hyper0)\n",
    "        optim_hist['hyper_sol'].append(hyper_sol)\n",
    "        optim_hist['obj_sol'].append(obj_sol)\n",
    "        optim_hist['flag_sol'].append(flag_sol)\n",
    "\n",
    "\n",
    "    fit_time = time.time() - t0\n",
    "\n",
    "    # post processing\n",
    "    if not noise:\n",
    "        id_best_try = n_try - 1\n",
    "    else:\n",
    "        converged = optim_hist['flag_sol'] == 1\n",
    "        id_best_try = np.argmin(optim_hist['obj_sol'][converged])\n",
    "\n",
    "    hyper_full = optim_hist['hyper_sol'][id_best_try]\n",
    "    min_n_log_l = np.min(optim_hist['obj_sol'])\n",
    "\n",
    "    if p_quant == 0:\n",
    "        phi = None\n",
    "    else:\n",
    "        phi = hyper_full[:p_quant]\n",
    "\n",
    "    if p_qual == 0:\n",
    "        z_vec = None\n",
    "        z = None\n",
    "    else:\n",
    "        z_vec = np.array(hyper_full[p_quant:])\n",
    "\n",
    "    # calc convenient quantities (stored in model$fit_detail)\n",
    "    if p_qual == 0:\n",
    "        R = lvgp_kernel(X_quant, X_quant, phi)\n",
    "        X_full = X_quant\n",
    "    else:\n",
    "        X_qual_trans = lvgp_to_latent(X_qual, lvs_qual, n_lvs_qual, p_qual, z_vec, dim_z, k)\n",
    "\n",
    "        if X_quant is not None:\n",
    "            X_full = np.hstack([X_quant, X_qual_trans])\n",
    "            omega = [*phi, *[0. for _ in range(p_qual * dim_z)]]\n",
    "        else:\n",
    "            X_full = X_qual_trans\n",
    "            omega = [0. for _ in range(p_qual * dim_z)]\n",
    "        R = lvgp_kernel(X_full, X_full, omega)\n",
    "\n",
    "    R = (R + R.T) / 2\n",
    "\n",
    "    raw_min_eig = np.linalg.eigvalsh(R, 'U').min(axis=-1)\n",
    "\n",
    "    if raw_min_eig < eps[id_best_try]:\n",
    "        nug_opt = eps[id_best_try] - raw_min_eig\n",
    "        R += (np.diag(np.full(k, 1)) * nug_opt)\n",
    "    else:\n",
    "        nug_opt = None\n",
    "\n",
    "    Y = Y.reshape(-1, 1)\n",
    "    L = cholesky(R).T\n",
    "\n",
    "    Linv = np.linalg.solve(L, np.diag(np.full(L.shape[0], 1)))\n",
    "    LinvM = np.linalg.solve(L, M)\n",
    "    MTRinvM = np.sum(np.power(LinvM, 2))\n",
    "    beta_hat = np.dot(LinvM.T, np.linalg.solve(L, Y) / MTRinvM)\n",
    "    beta_hat = float(beta_hat)\n",
    "\n",
    "    temp = np.linalg.solve(L, Y - M * beta_hat)\n",
    "    sigma2 = np.sum(temp**2) / k\n",
    "    if sigma2 < 1e-300:\n",
    "        sigma2 = 1e-300\n",
    "\n",
    "    RinvPYminusMbetaP = np.linalg.solve(L.T, temp)\n",
    "\n",
    "    # save the fitted model\n",
    "    model = {}\n",
    "\n",
    "    model['quantitative_params'] = {\n",
    "        'phi': phi, 'lb_phi_ini': lb_phi_ini, 'ub_phi_ini': ub_phi_ini,\n",
    "        'lb_phi_lat': lb_phi_lat, 'ub_phi_lat': ub_phi_lat\n",
    "    }\n",
    "\n",
    "    model['qualitative_params'] = {\n",
    "        'dim_z': dim_z, 'z_vec': z_vec, 'lb_z': lb_z, 'ub_z': ub_z\n",
    "    }\n",
    "\n",
    "    model['data'] = {\n",
    "        'X': X, 'X_quant': X_quant, 'X_qual': X_qual, 'X_full': X_full, 'Y': Y,\n",
    "        'X_quant_min': X_quant_min, 'X_quant_max': X_quant_max, 'Y_min': Y_min,\n",
    "        'Y_max': Y_max, 'ind_qual': ind_qual, 'lvs_qual': lvs_qual,\n",
    "        'n_lvs_qual': n_lvs_qual, 'p_all': p_all, 'p_quant': p_quant,\n",
    "        'p_qual': p_qual\n",
    "    }\n",
    "\n",
    "    model['fit_details'] = {\n",
    "        'beta_hat': beta_hat, 'sigma2': sigma2, 'MTRinvM': MTRinvM,  'Linv': Linv,\n",
    "        'LinvM': LinvM, 'RinvPYminusMbetaP': RinvPYminusMbetaP,\n",
    "        'raw_min_eig': raw_min_eig, 'nug_opt': nug_opt, 'min_n_log_l': min_n_log_l,\n",
    "        'fit_time': fit_time, 'optim_hist':optim_hist\n",
    "    }\n",
    "\n",
    "    model['settings'] = {\n",
    "        'max_iter_ini': max_iter_ini, 'max_iter_lat': max_iter_lat, 'seed': seed,\n",
    "        'n_opt': n_opt, 'lb_phi_ini': lb_phi_ini, 'ub_phi_ini': ub_phi_ini,\n",
    "        'lb_phi_lat': lb_phi_lat, 'ub_phi_lat': ub_phi_lat, 'lb_z': lb_z,\n",
    "        'ub_z': ub_z, 'eps': eps\n",
    "    }\n",
    "\n",
    "    model['model'] = 'LVGP model'\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lvgp_predict(X_new, model, MSE_on=False):\n",
    "    \"\"\"\n",
    "    param X_new Matrix or vector containing the input(s) where the predictions are to be made. Each row is an input vector.\n",
    "    param model The LVGP model fitted by \\code{\\link[LVGP]{LVGP_fit}}.\n",
    "    param MSE_on A scalar indicating whether the uncertainty (i.e., mean squared error \\code{MSE}) is calculated.\n",
    "       Set to a non-zero value to calculate \\code{MSE}.\n",
    "    \"\"\"\n",
    "\n",
    "    if not ('model' in model and model['model'] == 'LVGP model'):\n",
    "        print('The 2nd input should be a model of class \"LVGP model\".')\n",
    "        exit(0)\n",
    "\n",
    "    if not isinstance(X_new, np.ndarray):\n",
    "        print('X_new should be a numpy array.')\n",
    "        exit(0)\n",
    "\n",
    "    p_all = model['data']['p_all']\n",
    "\n",
    "    if X_new.shape[1] != p_all:\n",
    "        print('The dimensionality of X_new is not correct!')\n",
    "        exit(0)\n",
    "\n",
    "    # get params\n",
    "    p_qual = model['data']['p_qual']\n",
    "    X_quant_min = model['data']['X_quant_min']\n",
    "    X_quant_max = model['data']['X_quant_max']\n",
    "    Y_min = model['data']['Y_min']\n",
    "    Y_max = model['data']['Y_max']\n",
    "\n",
    "    X_old_full = model['data']['X_full']\n",
    "    lvs_qual = model['data']['lvs_qual']\n",
    "    n_lvs_qual = model['data']['n_lvs_qual']\n",
    "    ind_qual = model['data']['ind_qual']\n",
    "\n",
    "    phi = model['quantitative_params']['phi']\n",
    "    dim_z = model['qualitative_params']['dim_z']\n",
    "    z_vec = model['qualitative_params']['z_vec']\n",
    "\n",
    "    beta_hat = model['fit_details']['beta_hat']\n",
    "    RinvPYminusMbetaP = model['fit_details']['RinvPYminusMbetaP']\n",
    "\n",
    "    _NUMERIC_KINDS = {'b', 'f', 'i', 'u'}\n",
    "\n",
    "    # process X_new\n",
    "    m = X_new.shape[0]\n",
    "\n",
    "    if p_qual == 0:\n",
    "        X_new_qual = None\n",
    "        X_new_quant = X_new\n",
    "\n",
    "        if not (np.asarray(X_new_quant).dtype.kind in _NUMERIC_KINDS or np.all(np.isfinite(X_new_quant))):\n",
    "            print('All the elements of X_new must be finite numbers.')\n",
    "            exit(0)\n",
    "\n",
    "        X_new_quant = ((X_new_quant.T - X_quant_min.reshape(-1, 1)) / (X_quant_max - X_quant_min).reshape(-1, 1)).T\n",
    "\n",
    "        R_old_new = lvgp_kernel(X_old_full, X_new_quant, phi)\n",
    "        R_new_new = lvgp_kernel(X_new_quant, X_new_quant, phi)\n",
    "    else:\n",
    "        X_new_qual = X_new[:, ind_qual]\n",
    "\n",
    "        if p_qual == p_all:\n",
    "            X_new_quant = None\n",
    "        else:\n",
    "            X_new_quant = X_new[:, [ii for ii in range(p_all) if ii not in ind_qual]]\n",
    "            if not (np.asarray(X_new_quant).dtype.kind in _NUMERIC_KINDS or np.all(np.isfinite(X_new_quant))):\n",
    "                print('All the elements of X_new must be finite numbers.')\n",
    "                exit(0)\n",
    "\n",
    "            X_new_quant = ((X_new_quant.T - X_quant_min.reshape(-1, 1)) / (X_quant_max - X_quant_min).reshape(-1, 1)).T\n",
    "\n",
    "        X_new_qual_la = lvgp_to_latent(X_new_qual, lvs_qual, n_lvs_qual, p_qual, z_vec, dim_z, m)\n",
    "\n",
    "        if X_new_quant is not None:\n",
    "            X_new_full = np.hstack([X_new_quant, X_new_qual_la])\n",
    "            phi_full = np.array([*phi, *[0. for _ in range(p_qual * dim_z)]])\n",
    "        else:\n",
    "            X_new_full = X_new_qual_la\n",
    "            phi_full = np.array([0. for _ in range(p_qual * dim_z)])\n",
    "\n",
    "        R_old_new = lvgp_kernel(X_old_full, X_new_full, phi_full)\n",
    "        R_new_new = lvgp_kernel(X_new_full, X_new_full, phi_full)\n",
    "\n",
    "    R_new_new = (R_new_new + R_new_new.T) / 2\n",
    "\n",
    "    # calc predictions\n",
    "    predictions = {}\n",
    "\n",
    "    Y_hat = beta_hat + np.dot(R_old_new.T, RinvPYminusMbetaP)\n",
    "    Y_hat = ((Y_hat.T * (Y_max - Y_min)) + Y_min).T\n",
    "    predictions['Y_hat'] = Y_hat\n",
    "\n",
    "    if MSE_on:\n",
    "        # calculate the uncertainty\n",
    "        sigma2 = model['fit_details']['sigma2']\n",
    "        Linv = model['fit_details']['Linv']\n",
    "        MTRinvM = model['fit_details']['MTRinvM']\n",
    "        LinvM = model['fit_details']['LinvM']\n",
    "\n",
    "        temp = np.dot(Linv, R_old_new)\n",
    "        W = 1 - np.dot(LinvM.T, temp)\n",
    "        MSE = sigma2 * (R_new_new - np.dot(temp.T, temp) + np.dot(W.T, W)/MTRinvM) * (Y_max-Y_min)**2\n",
    "        predictions['MSE'] = np.nan_to_num(np.sqrt(np.diag(MSE))) # MSE\n",
    "    # print('do')\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ei(mu, sigma, loss_optimum, scaling_factor=1):\n",
    "\n",
    "        if float(sigma) == 0.0:\n",
    "            return 0.\n",
    "        with np.errstate(divide='ignore'):\n",
    "            Z = scaling_factor * (loss_optimum - mu) / sigma\n",
    "            expected_improvement = scaling_factor * (loss_optimum - mu) * norm.cdf(Z) + sigma * norm.pdf(Z)\n",
    "            # expected_improvement[sigma == 0.0] = 0.0\n",
    "\n",
    "            return 1 * expected_improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C</th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>S</th>\n",
       "      <th>BE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>20.34220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>9.80905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>18.38690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>8.63246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>5.71800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>12.99670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>20.44450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>36.09940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>13.05060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>40.96570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>18.41520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>5</td>\n",
       "      <td>8.77638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>5</td>\n",
       "      <td>12.42980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>15.55830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>10.55730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>19.73020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>12.17780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>13.13690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>14.19600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>18.33740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>10.86890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>40.36010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>14.96740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>13.61130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>37.77230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>37.85140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>37.60520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>12.51720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>11.95930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>13.44990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>37.67860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>9.40231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>13.41880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>10.52800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>15.63080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>18.57340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>15.13960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>40.66080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>15.86790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>14.79000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>36.74300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>8.63716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>32.75350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>12.17110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>39.24190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>38.37900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>38.20230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>41.03250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     C  X1  X2  X3  S        BE\n",
       "0    8  11  13  13  1  20.34220\n",
       "1    9  13  13  11  7   9.80905\n",
       "2    9  12  12  12  1  18.38690\n",
       "3   10  13  13  12  7   8.63246\n",
       "4    8  12  12  13  7   5.71800\n",
       "5    9  13  13  11  3  12.99670\n",
       "6    8  13  13  13  1  20.44450\n",
       "7    8  12  13  12  6  36.09940\n",
       "8    8  11  13  12  5  13.05060\n",
       "9   10  13  13  13  6  40.96570\n",
       "10   8  12  12  11  1  18.41520\n",
       "11   8  11  13  13  5   8.77638\n",
       "12   8  12  12  13  5  12.42980\n",
       "13   8  12  13  13  4  15.55830\n",
       "14   8  11  11  11  3  10.55730\n",
       "15   8  11  13  11  1  19.73020\n",
       "16   8  12  12  12  5  12.17780\n",
       "17   8  12  13  12  5  13.13690\n",
       "18   9  13  13  12  5  14.19600\n",
       "19   8  11  11  13  1  18.33740\n",
       "20   8  11  11  12  5  10.86890\n",
       "21  10  13  13  12  6  40.36010\n",
       "22   9  13  13  11  4  14.96740\n",
       "23   8  12  13  11  5  13.61130\n",
       "24   9  12  12  13  0  37.77230\n",
       "25   8  12  13  11  6  37.85140\n",
       "26   8  11  11  13  6  37.60520\n",
       "27   8  11  13  12  3  12.51720\n",
       "28   9  12  12  12  3  11.95930\n",
       "29   9  12  12  12  5  13.44990\n",
       "30   8  11  12  13  0  37.67860\n",
       "31  10  13  13  11  7   9.40231\n",
       "32  10  13  13  13  3  13.41880\n",
       "33   9  12  13  12  7  10.52800\n",
       "34   9  12  12  13  4  15.63080\n",
       "35   8  11  11  11  2  18.57340\n",
       "36   9  12  12  11  4  15.13960\n",
       "37   8  13  13  13  6  40.66080\n",
       "38  10  13  13  11  4  15.86790\n",
       "39   9  12  13  11  4  14.79000\n",
       "40   9  12  13  12  0  36.74300\n",
       "41   9  12  12  13  7   8.63716\n",
       "42   8  11  12  12  0  32.75350\n",
       "43   8  11  11  11  5  12.17110\n",
       "44   8  13  13  13  0  39.24190\n",
       "45   8  13  13  12  6  38.37900\n",
       "46   8  12  13  13  0  38.20230\n",
       "47  10  13  13  11  6  41.03250"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df10 = pd.read_csv('hoip_train.csv') #.sample(10)\n",
    "df10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.655715</td>\n",
       "      <td>0.341517</td>\n",
       "      <td>1</td>\n",
       "      <td>4.968010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.230932</td>\n",
       "      <td>0.687191</td>\n",
       "      <td>1</td>\n",
       "      <td>-6.026685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.798489</td>\n",
       "      <td>0.155806</td>\n",
       "      <td>1</td>\n",
       "      <td>5.847720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.388887</td>\n",
       "      <td>0.798749</td>\n",
       "      <td>1</td>\n",
       "      <td>-3.546119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.071492</td>\n",
       "      <td>0.902112</td>\n",
       "      <td>1</td>\n",
       "      <td>-2.462696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.555596</td>\n",
       "      <td>0.027249</td>\n",
       "      <td>1</td>\n",
       "      <td>2.225438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.937787</td>\n",
       "      <td>0.419238</td>\n",
       "      <td>1</td>\n",
       "      <td>2.181156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.331248</td>\n",
       "      <td>0.546839</td>\n",
       "      <td>1</td>\n",
       "      <td>-5.817442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.316096</td>\n",
       "      <td>0.296497</td>\n",
       "      <td>2</td>\n",
       "      <td>-18.854136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.508226</td>\n",
       "      <td>0.606796</td>\n",
       "      <td>2</td>\n",
       "      <td>8.444852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.688801</td>\n",
       "      <td>0.651250</td>\n",
       "      <td>2</td>\n",
       "      <td>17.065743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.171980</td>\n",
       "      <td>0.775921</td>\n",
       "      <td>2</td>\n",
       "      <td>6.652336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.807405</td>\n",
       "      <td>0.030188</td>\n",
       "      <td>2</td>\n",
       "      <td>4.098560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.014836</td>\n",
       "      <td>0.134059</td>\n",
       "      <td>2</td>\n",
       "      <td>-10.352014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.961374</td>\n",
       "      <td>0.910870</td>\n",
       "      <td>2</td>\n",
       "      <td>8.587908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.393677</td>\n",
       "      <td>0.496554</td>\n",
       "      <td>2</td>\n",
       "      <td>-4.617646</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          V1        V2  V3         V4\n",
       "0   0.655715  0.341517   1   4.968010\n",
       "1   0.230932  0.687191   1  -6.026685\n",
       "2   0.798489  0.155806   1   5.847720\n",
       "3   0.388887  0.798749   1  -3.546119\n",
       "4   0.071492  0.902112   1  -2.462696\n",
       "5   0.555596  0.027249   1   2.225438\n",
       "6   0.937787  0.419238   1   2.181156\n",
       "7   0.331248  0.546839   1  -5.817442\n",
       "8   0.316096  0.296497   2 -18.854136\n",
       "9   0.508226  0.606796   2   8.444852\n",
       "10  0.688801  0.651250   2  17.065743\n",
       "11  0.171980  0.775921   2   6.652336\n",
       "12  0.807405  0.030188   2   4.098560\n",
       "13  0.014836  0.134059   2 -10.352014\n",
       "14  0.961374  0.910870   2   8.587908\n",
       "15  0.393677  0.496554   2  -4.617646"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train= pd.read_csv('train.csv').drop('Unnamed: 0', axis=1)\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking and preprocessing the inputs...\n",
      "Starting optimization.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lk/7pxby8y145q_94s_03vn5cv80000gn/T/ipykernel_89272/3961927316.py:52: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  beta_hat = float(beta_hat)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "21.92 18.25 2.55 0.0\n",
      "5.41 11.98 0.95 0.0\n",
      "12.73 14.82 1.37 0.0\n",
      "13.91 13.59 1.11 0.0\n",
      "13.36 13.72 1.11 0.0\n",
      "12.13 15.09 0.94 0.0\n",
      "20.69 19.66 1.3 0.0\n",
      "37.79 39.1 0.9 0.0\n",
      "14.17 15.09 0.97 0.0\n",
      "36.83 36.87 0.92 0.0\n",
      "15.96 18.16 2.17 0.0\n",
      "14.45 14.87 1.31 0.0\n",
      "13.78 12.73 0.79 0.0\n",
      "20.85 18.22 2.34 0.0\n",
      "31.76 36.75 0.94 0.0\n",
      "32.19 37.24 1.28 0.0\n",
      "12.26 15.1 0.94 0.0\n",
      "13.97 15.44 1.01 0.0\n",
      "20.71 18.26 2.57 0.0\n",
      "17.68 19.1 1.13 0.0\n",
      "11.75 12.4 1.01 0.0\n",
      "8.42 12.2 0.91 0.0\n",
      "14.01 12.88 0.79 0.0\n",
      "9.39 9.05 1.04 0.0\n",
      "11.21 12.45 0.98 0.0\n",
      "9.09 9.13 1.02 0.0\n",
      "11.78 12.37 1.25 0.0\n",
      "9.01 8.77 1.39 0.007\n",
      "19.91 18.2 2.27 0.0\n",
      "12.11 12.88 1.22 0.0\n",
      "19.06 19.17 1.07 0.0\n",
      "14.45 14.87 1.28 0.0\n",
      "21.57 19.81 1.28 0.0\n",
      "29.99 38.31 0.86 0.0\n",
      "18.29 19.54 0.94 0.0\n",
      "9.33 9.27 1.02 0.0\n",
      "37.08 36.14 1.14 0.0\n",
      "7.49 8.98 1.44 0.006\n",
      "19.56 18.11 2.07 0.0\n",
      "16.57 15.45 1.01 0.0\n",
      "8.07 12.92 0.81 0.0\n",
      "15.3 15.13 0.92 0.0\n",
      "36.03 36.58 1.07 0.0\n",
      "11.86 12.78 1.2 0.0\n",
      "15.25 15.15 1.01 0.0\n",
      "13.08 12.84 0.74 0.0\n",
      "35.6 38.38 0.84 0.0\n",
      "39.25 37.35 1.25 0.0\n",
      "33.83 38.33 0.86 0.0\n",
      "12.71 14.83 1.31 0.0\n",
      "9.07 9.03 0.97 0.0\n",
      "10.59 12.19 1.22 0.0\n",
      "13.06 12.51 1.06 0.0\n",
      "19.46 19.73 1.24 0.0\n",
      "7.0 8.88 1.52 0.01\n",
      "35.97 38.32 0.93 0.0\n",
      "15.52 15.44 1.02 0.0\n",
      "9.38 9.12 1.03 0.0\n",
      "12.12 12.52 0.99 0.0\n",
      "21.43 19.81 1.27 0.0\n",
      "40.27 37.34 1.19 0.0\n",
      "21.2 18.28 2.62 0.0\n",
      "36.87 38.29 0.89 0.0\n",
      "37.34 37.33 1.21 0.0\n",
      "21.46 18.28 2.62 0.0\n",
      "14.37 15.11 1.02 0.0\n",
      "14.89 14.89 1.33 0.0\n",
      "7.62 8.97 1.01 0.0\n",
      "13.49 14.81 1.39 0.0\n",
      "12.63 12.87 0.79 0.0\n",
      "39.52 37.06 1.31 0.0\n",
      "14.87 18.09 2.07 0.0\n",
      "41.31 39.13 0.83 0.0\n",
      "8.58 9.26 1.03 0.0\n",
      "37.49 36.02 1.15 0.0\n",
      "37.58 38.35 0.88 0.0\n",
      "13.66 13.73 1.11 0.0\n",
      "38.1 37.22 1.21 0.0\n",
      "36.45 37.32 1.27 0.0\n",
      "34.31 35.85 1.26 0.0\n",
      "22.17 18.28 2.63 0.0\n",
      "16.28 18.12 2.09 0.0\n",
      "13.01 12.51 1.05 0.0\n",
      "7.01 8.83 1.38 0.006\n",
      "7.69 8.97 0.98 0.0\n",
      "37.2 35.97 1.17 0.0\n",
      "18.74 18.2 2.27 0.0\n",
      "21.91 18.25 2.56 0.0\n",
      "22.12 18.26 2.57 0.0\n",
      "14.65 15.12 0.95 0.0\n",
      "37.0 38.33 0.88 0.0\n",
      "6.85 8.92 1.44 0.007\n",
      "11.81 12.58 1.07 0.0\n",
      "11.2 14.84 1.28 0.0\n",
      "20.26 18.15 2.14 0.0\n",
      "15.83 15.47 1.1 0.0\n",
      "8.39 12.0 0.87 0.0\n",
      "19.93 19.62 0.98 0.0\n",
      "14.77 14.86 1.33 0.0\n",
      "10.92 12.25 1.29 0.0\n",
      "36.0 37.38 1.27 0.0\n",
      "13.26 12.41 1.01 0.0\n",
      "22.26 19.73 1.25 0.0\n",
      "6.8 9.06 1.04 0.0\n",
      "19.14 19.37 1.0 0.0\n",
      "34.29 37.39 1.22 0.0\n",
      "8.18 12.77 0.81 0.0\n",
      "36.41 36.66 1.07 0.0\n",
      "19.61 18.11 2.14 0.0\n",
      "12.3 12.82 1.16 0.0\n",
      "15.12 15.47 1.1 0.0\n",
      "37.15 39.12 0.9 0.0\n",
      "20.85 18.18 2.25 0.0\n",
      "13.4 15.13 0.93 0.0\n",
      "37.49 36.66 1.06 0.0\n",
      "12.26 12.47 1.03 0.0\n",
      "21.5 18.22 2.33 0.0\n",
      "13.96 14.83 1.41 0.0\n",
      "35.64 36.58 1.07 0.0\n",
      "37.88 39.1 0.88 0.0\n",
      "15.08 15.46 1.1 0.0\n",
      "19.99 18.14 2.18 0.0\n",
      "14.99 13.66 1.07 0.0\n",
      "20.47 19.47 1.01 0.0\n",
      "21.23 19.66 1.3 0.0\n",
      "20.48 18.14 2.17 0.0\n",
      "8.72 8.97 0.97 0.0\n",
      "17.13 18.16 2.21 0.0\n",
      "20.6 19.52 0.96 0.0\n",
      "20.46 19.47 1.01 0.0\n",
      "18.8 18.14 2.18 0.0\n",
      "39.86 37.35 1.18 0.0\n",
      "13.9 13.75 1.12 0.0\n",
      "36.95 36.22 1.08 0.0\n",
      "21.92 18.26 2.57 0.0\n",
      "13.43 12.82 1.16 0.0\n",
      "13.19 13.64 1.06 0.0\n",
      "36.73 37.22 1.2 0.0\n",
      "14.56 12.52 0.99 0.0\n",
      "14.16 13.6 1.12 0.0\n",
      "14.84 15.45 1.01 0.0\n",
      "22.95 19.73 1.26 0.0\n",
      "19.07 19.52 0.96 0.0\n",
      "7.46 8.89 1.38 0.005\n",
      "36.36 36.05 1.19 0.0\n",
      "7.46 8.73 1.46 0.011\n",
      "7.63 8.83 1.4 0.006\n",
      "16.99 19.2 1.05 0.0\n",
      "36.32 38.37 0.91 0.0\n",
      "12.22 12.2 1.17 0.0\n",
      "16.84 19.27 0.99 0.0\n",
      "12.68 12.83 1.16 0.0\n",
      "33.76 36.83 0.93 0.0\n",
      "13.0 12.15 0.88 0.0\n",
      "11.92 12.48 1.02 0.0\n",
      "20.81 19.62 0.97 0.0\n",
      "9.38 8.98 1.0 0.0\n",
      "38.32 36.34 1.07 0.0\n",
      "33.72 36.75 0.94 0.0\n",
      "19.67 18.18 2.27 0.0\n",
      "18.17 19.45 0.92 0.0\n",
      "20.98 18.25 2.55 0.0\n",
      "19.96 18.18 2.24 0.0\n",
      "20.49 18.18 2.25 0.0\n",
      "20.08 19.54 0.94 0.0\n",
      "41.2 36.95 0.91 0.0\n",
      "9.04 9.32 0.96 0.0\n",
      "13.11 12.79 1.2 0.0\n",
      "13.58 12.27 1.2 0.0\n",
      "12.25 12.58 1.06 0.0\n",
      "7.18 12.07 0.81 0.0\n",
      "12.02 12.3 1.23 0.0\n",
      "37.14 37.06 1.31 0.0\n",
      "9.58 9.28 1.02 0.0\n",
      "40.59 37.05 1.32 0.0\n",
      "20.54 18.18 2.27 0.0\n",
      "13.24 13.57 1.11 0.0\n",
      "7.51 8.79 1.46 0.009\n",
      "9.27 9.41 1.0 0.0\n",
      "37.83 37.22 1.2 0.0\n",
      "19.96 19.66 1.31 0.0\n",
      "17.59 18.16 2.2 0.0\n",
      "38.29 37.3 1.23 0.0\n",
      "35.99 37.23 1.34 0.0\n",
      "10.01 9.33 0.96 0.0\n",
      "12.86 12.76 0.81 0.0\n",
      "13.49 12.88 1.21 0.0\n",
      "35.3 38.3 0.93 0.0\n",
      "11.86 12.24 1.16 0.0\n",
      "19.54 19.36 1.01 0.0\n",
      "20.03 19.34 1.02 0.0\n",
      "36.41 38.27 0.89 0.0\n",
      "RMSE:  0.05561588752649131\n",
      "R2 :  0.9624621999889449\n",
      "22.45114278793335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lk/7pxby8y145q_94s_03vn5cv80000gn/T/ipykernel_89272/790223486.py:260: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  beta_hat = float(beta_hat)\n"
     ]
    }
   ],
   "source": [
    "if True:    \n",
    "    import sklearn.gaussian_process as gp\n",
    "    \n",
    "    kernel = gp.kernels.Matern()\n",
    "    model = gp.GaussianProcessRegressor(kernel=kernel, alpha=1e-5,\n",
    "                                         n_restarts_optimizer=10, normalize_y=True,\n",
    "                                         random_state=123)\n",
    "\n",
    "    #df = pd.read_csv('hoip.csv').drop('Unnamed: 0', axis=1)\n",
    "    df10 = pd.read_csv('hoip_train.csv') #.sample(10)\n",
    "    # df10 = df[df['BE'] > -30].sample(10)\n",
    "    X = df10[['C', 'X1', 'X2', 'X3', 'S']].to_numpy()\n",
    "    Y = df10['BE'].to_numpy()\n",
    "    \n",
    "    #model = lvgp_fit(X, Y, ind_qual=[0, 1, 2, 4], n_opt=8, noise=True)\n",
    "    model = lvgp_fit(X, Y, ind_qual=[0], n_opt=8, noise=True, dim_z = 1)\n",
    "    \n",
    "    dfte = pd.read_csv('hoip_test.csv')\n",
    "    X_te = dfte[['C', 'X1', 'X2', 'X3', 'S']].to_numpy()\n",
    "    Y_te = dfte['BE'].to_numpy()\n",
    "    pred = lvgp_predict(X_te, model, MSE_on=True)\n",
    "    MSE = pred['MSE']\n",
    "    pred = pred['Y_hat'].reshape(-1)\n",
    "    #\n",
    "    # model.fit(X, Y)\n",
    "    # pred, MSE = model.predict(X_te, return_std=True)\n",
    "    \n",
    "    for i in range(Y_te.shape[0]):\n",
    "         e = ei(pred[i], MSE[i], loss_optimum=np.min(Y))\n",
    "         print(round(Y_te[i], 2), round(pred[i], 2), round(MSE[i], 2), round(e, 3))\n",
    "    \n",
    "     # print(Y_te.shape, pred.shape)\n",
    "    print('RMSE: ', np.sqrt(mean_squared_error(Y_te, pred))/(max(Y_te)-min(Y_te)))\n",
    "    print('R2 : ', r2_score(Y_te, pred))\n",
    "    print(model['fit_details']['fit_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_quant = 4\n",
    "n_z = 2\n",
    "n_hyper = p_quant + n_z\n",
    "n_opt = 8#number of times optimized\n",
    "lb_phi_ini = -2\n",
    "ub_phi_ini = 2\n",
    "lb_phi_lat = -8\n",
    "ub_phi_lat = 3\n",
    "lb_z = -3\n",
    "ub_z = 3\n",
    "lb_ini = [*[lb_phi_ini for i in range(p_quant)], *[lb_z for i in range(n_z)]]\n",
    "ub_ini = [*[ub_phi_ini for i in range(p_quant)], *[ub_z for i in range(n_z)]]\n",
    "lb_lat = [*[lb_phi_lat for i in range(p_quant)], *[lb_z for i in range(n_z)]]\n",
    "ub_lat = [*[ub_phi_lat for i in range(p_quant)], *[ub_z for i in range(n_z)]]\n",
    "\n",
    "lb_ini = np.array(lb_ini)\n",
    "lb_lat = np.array(lb_lat)\n",
    "ub_ini = np.array(ub_ini)\n",
    "ub_lat = np.array(ub_lat)\n",
    "\n",
    "space = Space([(0., 1.) for i in range(n_hyper)])\n",
    "lhs = Lhs(lhs_type=\"classic\", criterion=\"maximin\", iterations=1000)\n",
    "A = np.array(lhs.generate(space.dimensions, n_opt))\n",
    "\n",
    "hyper0 = (A.T * (ub_ini - lb_ini).reshape(-1, 1) + lb_ini.reshape(-1, 1)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'phi': array([-1.28463302, -2.        , -2.        ,  2.        ]),\n",
       " 'lb_phi_ini': -2,\n",
       " 'ub_phi_ini': 2,\n",
       " 'lb_phi_lat': -8,\n",
       " 'ub_phi_lat': 3}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['quantitative_params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dim_z': 1, 'z_vec': array([0.01444064, 0.03080615]), 'lb_z': -3, 'ub_z': 3}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['qualitative_params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lk/7pxby8y145q_94s_03vn5cv80000gn/T/ipykernel_89272/3961927316.py:52: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  beta_hat = float(beta_hat)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "RMSE:  0.1292053086957725\n",
      "R2 :  0.6509533930006519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lk/7pxby8y145q_94s_03vn5cv80000gn/T/ipykernel_89272/4116560059.py:260: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  beta_hat = float(beta_hat)\n"
     ]
    }
   ],
   "source": [
    "if True:\n",
    "    df = pd.read_csv('train.csv').drop('Unnamed: 0', axis=1)\n",
    "    X = df[['V1', 'V2', 'V3']].to_numpy()\n",
    "    Y = df['V4'].to_numpy()\n",
    "\n",
    "    dft = pd.read_csv('test.csv').drop('Unnamed: 0', axis=1)\n",
    "    X_te = dft[['V1', 'V2', 'V3']].to_numpy()\n",
    "    Y_te = dft['V4'].to_numpy()\n",
    "    model = lvgp_fit(X, Y, ind_qual=[2], n_opt=8, progress=False, noise=False)\n",
    "    pred = lvgp_predict(X_te, model, MSE_on=True)\n",
    "    MSE = pred['MSE']\n",
    "    pred = pred['Y_hat'].reshape(-1)\n",
    "    # print(MSE)\n",
    "    # print(Y_te.shape, pred.shape)\n",
    "    print('RMSE: ', np.sqrt(mean_squared_error(Y_te, pred))/(max(Y_te)-min(Y_te)))\n",
    "    print('R2 : ', r2_score(Y_te, pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local-gp-mac-no-tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
