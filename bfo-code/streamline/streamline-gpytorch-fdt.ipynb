{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import pandas as pd\n",
    "from pandas import factorize\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "#from sklearn.model_selection import KFold\n",
    "\n",
    "import gpytorch\n",
    "import itertools\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from m3gpr.config import get_cfg_defaults_all\n",
    "from m3gpr.load_data import Data_set\n",
    "from m3gpr.models import singleGP_gpytorch,rf_sklearn,singleGP_gpytorch_train,singleGP_gpytorch_reference,MultitaskGPModel,MTMOGP\n",
    "from m3gpr.train import CV_Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "singleGP\n",
      "y-stand\n",
      "(454, 55)\n",
      "(454, 5)\n"
     ]
    }
   ],
   "source": [
    "cfg = get_cfg_defaults_all()\n",
    "os.chdir('/Users/chenya68/Documents/GitHub/BFO')\n",
    "parser = argparse.ArgumentParser(description=\"Configure files\")\n",
    "parser.add_argument(\"--cfg\", required=True, help=\"path to config file\", type=str)\n",
    "#args = parser.parse_args(['--cfg', 'data/harpoon/harpoon_dataset.yaml'])\n",
    "args = parser.parse_args(['--cfg', 'bfo-data/fdt_data/fdt_dataset.yaml'])\n",
    "cfg.merge_from_file(args.cfg)\n",
    "\n",
    "cfg.freeze()\n",
    "setup_data = Data_set(cfg, 1)\n",
    "print(cfg.MODEL.MODEL_NAME)\n",
    "print(cfg.MODEL.Y_SCALE)\n",
    "\n",
    "#df_x, df_y_wide, df_y_long = setup_data.Load_data_set()\n",
    "df_x, df_y,ls_X_train,ls_X_test,ls_y_train,ls_y_test, obj_y_scaler= setup_data.Load_data_set()\n",
    "print(df_x.shape)\n",
    "print(df_y.shape)\n",
    "#print(df_y_wide.shape)\n",
    "#print(df_y_long.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        HARDNESS  FRIABILITY  Drug_content  \\\n",
      "HARDNESS                1.000000    0.091377      0.048112   \n",
      "FRIABILITY              0.091377    1.000000      0.053990   \n",
      "Drug_content            0.048112    0.053990      1.000000   \n",
      "Water_absorption_ratio  0.003404    0.041980     -0.005044   \n",
      "DISINTEGRATION_TIME    -0.095159   -0.108785     -0.065873   \n",
      "\n",
      "                        Water_absorption_ratio  DISINTEGRATION_TIME  \n",
      "HARDNESS                              0.003404            -0.095159  \n",
      "FRIABILITY                            0.041980            -0.108785  \n",
      "Drug_content                         -0.005044            -0.065873  \n",
      "Water_absorption_ratio                1.000000            -0.183772  \n",
      "DISINTEGRATION_TIME                  -0.183772             1.000000  \n"
     ]
    }
   ],
   "source": [
    "df_y_corr = df_y.corr()\n",
    "df_y_corr.to_csv(cfg.PATH.DATA_PATH + 'df_y_corr.csv')\n",
    "print(df_y_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Covariance Matrix:\n",
      " Parameter containing:\n",
      "tensor([[ 9.9152e-01,  2.6691e-02,  2.3423e+00,  1.4601e-01, -3.4544e+00],\n",
      "        [ 2.6691e-02,  8.6049e-02,  7.7433e-01,  5.3046e-01, -1.1634e+00],\n",
      "        [ 2.3423e+00,  7.7433e-01,  2.3904e+03, -1.0624e+01, -1.1741e+02],\n",
      "        [ 1.4601e-01,  5.3046e-01, -1.0624e+01,  1.8556e+03, -2.8860e+02],\n",
      "        [-3.4544e+00, -1.1634e+00, -1.1741e+02, -2.8860e+02,  1.3290e+03]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def correlation_to_covariance(corr_matrix, std_devs):\n",
    "    \"\"\"\n",
    "    Convert a correlation matrix to a covariance matrix.\n",
    "\n",
    "    Parameters:\n",
    "        corr_matrix (np.ndarray): Square correlation matrix (n x n).\n",
    "        std_devs (np.ndarray): 1D array of standard deviations (length n).\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Covariance matrix (n x n).\n",
    "    \"\"\"\n",
    "    # Validate inputs\n",
    "    corr_matrix = np.array(corr_matrix, dtype=float)\n",
    "    std_devs = np.array(std_devs, dtype=float)\n",
    "\n",
    "    if corr_matrix.shape[0] != corr_matrix.shape[1]:\n",
    "        raise ValueError(\"Correlation matrix must be square.\")\n",
    "    if corr_matrix.shape[0] != std_devs.shape[0]:\n",
    "        raise ValueError(\"Length of std_devs must match matrix size.\")\n",
    "    if not np.allclose(corr_matrix, corr_matrix.T, atol=1e-8):\n",
    "        raise ValueError(\"Correlation matrix must be symmetric.\")\n",
    "    if np.any(np.diag(corr_matrix) != 1):\n",
    "        raise ValueError(\"Diagonal elements of correlation matrix must be 1.\")\n",
    "\n",
    "    # Create diagonal matrix of standard deviations\n",
    "    D = np.diag(std_devs)\n",
    "\n",
    "    # Covariance = D * Corr * D\n",
    "    cov_matrix = D @ corr_matrix @ D\n",
    "    return cov_matrix\n",
    "\n",
    "\n",
    "# Example usage\n",
    "corr = df_y_corr.values\n",
    "std_devs = df_y.std().to_list()\n",
    "\n",
    "cov = correlation_to_covariance(corr, std_devs)\n",
    "cov = torch.nn.Parameter(torch.Tensor(cov))\n",
    "print(\"Covariance Matrix:\\n\", cov)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single GP Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "\n",
    "        model_name = cfg.MODEL.MODEL_NAME\n",
    "        n_cv = cfg.MODEL.N_CV#cfg.MODEL.N_CV is number of cv\n",
    "        num_outputs = cfg.DATA.NUM_OUTPUTS\n",
    "        if cfg.MODEL.SPLIT == 'by-task':\n",
    "            num_tasks = cfg.DATA.NUM_TASKS\n",
    "        else:\n",
    "            num_tasks = 1\n",
    "        num_total_output = num_outputs*num_tasks\n",
    "        num_cols = num_total_output*n_cv\n",
    "        y_scale=cfg.MODEL.Y_SCALE\n",
    "        metric_row = 3\n",
    "\n",
    "        \n",
    "        ls_model_from_combs = [[] for _ in range(num_total_output)]\n",
    "        ls_init_len = [2.0,3.0,4.0]\n",
    "        ls_lr = [0.1,0.15,0.2,0.25]\n",
    "        ls_n_iter = [200,300]\n",
    "        combs = list(itertools.product(ls_init_len,ls_lr,ls_n_iter))\n",
    "\n",
    "        mean_arr_cv_mae = np.zeros((2,len(combs))) #train,test\n",
    "        mean_arr_cv_r2 = np.zeros((2,len(combs))) #train,test\n",
    "        mean_arr_cv_err = np.zeros((2,len(combs))) #train,test\n",
    "\n",
    "        ls_arr_cv_mae = [np.zeros((2,len(combs))) for _ in range(num_total_output)]\n",
    "        ls_arr_cv_r2 = [np.zeros((2,len(combs))) for _ in range(num_total_output)]\n",
    "        ls_arr_cv_err = [np.zeros((2,len(combs))) for _ in range(num_total_output)]\n",
    "\n",
    "        \n",
    "        for i in range(0,len(combs)):\n",
    "            init_len,lr,n_iter = combs[i]\n",
    "            arr_train_metrics = np.zeros((11,num_cols))\n",
    "            arr_test_metrics = np.zeros((11,num_cols))\n",
    "            \n",
    "            for batch_ind in range(n_cv):\n",
    "                t_train_x = ls_X_train[batch_ind]\n",
    "                t_test_x = ls_X_test[batch_ind]\n",
    "                \n",
    "                #for j in range(self.num_outputs):\n",
    "                for j in range(num_total_output):\n",
    "                    a = batch_ind + n_cv*j\n",
    "                    t_train_y = ls_y_train[batch_ind][:,j].flatten()\n",
    "                    t_test_y = ls_y_test[batch_ind][:,j].flatten()\n",
    "\n",
    "                    train_mean,train_lower,train_upper,model = singleGP_gpytorch_train(t_train_x,t_train_y,training_iter = n_iter,init_len_scale = init_len,my_lr = lr)\n",
    "                    #print(train_mean)\n",
    "                    ls_model_from_combs[j].append(model)\n",
    "                    \n",
    "                    # Set into eval mode\n",
    "                    model.eval()\n",
    "                    model.likelihood.eval()\n",
    "\n",
    "                    with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "                        test_pred = model.likelihood(model(t_test_x))   \n",
    "                        test_mean = test_pred.mean\n",
    "                        test_lower, test_upper = test_pred.confidence_region()\n",
    "\n",
    "                    if y_scale=='no-y-scale':\n",
    "                        arr_train_y = t_train_y.detach().numpy()\n",
    "                        arr_test_y = t_test_y.detach().numpy()\n",
    "                        arr_test_mean = test_mean.detach().numpy()\n",
    "                        arr_test_lower = test_lower.detach().numpy()\n",
    "                        arr_test_upper = test_upper.detach().numpy()\n",
    "\n",
    "                        arr_train_mean = train_mean.detach().numpy()\n",
    "                        arr_train_lower = train_lower.detach().numpy()\n",
    "                        arr_train_upper = train_upper.detach().numpy()\n",
    "                    else:\n",
    "                        y_scaler = obj_y_scaler[batch_ind]\n",
    "                        tmp_mean = y_scaler.mean_\n",
    "                        tmp_scale = y_scaler.scale_\n",
    "\n",
    "                        arr_train_y = t_train_y.detach().numpy()\n",
    "                        arr_test_y = t_test_y.detach().numpy()\n",
    "                        arr_test_mean = test_mean.detach().numpy()\n",
    "                        arr_test_lower = test_lower.detach().numpy()\n",
    "                        arr_train_mean = train_mean.detach().numpy()\n",
    "                        arr_train_lower = train_lower.detach().numpy()\n",
    "\n",
    "                        arr_train_y = arr_train_y*tmp_scale[j] + tmp_mean[j]\n",
    "                        arr_test_mean = arr_test_mean*tmp_scale[j] + tmp_mean[j]\n",
    "                        arr_test_lower = arr_test_lower*tmp_scale[j] + tmp_mean[j]\n",
    "                        arr_train_mean = arr_train_mean*tmp_scale[j] + tmp_mean[j]\n",
    "                        arr_train_lower = arr_train_lower*tmp_scale[j] + tmp_mean[j]\n",
    "\n",
    "                    y_true = arr_test_y\n",
    "                    y_pred = arr_test_mean\n",
    "                    y_train = arr_train_y\n",
    "                    mean_train = arr_train_mean\n",
    "                    std_train = arr_train_mean - arr_train_lower\n",
    "                    std_test = arr_test_mean - arr_test_lower\n",
    "\n",
    "                    train_comp = np.concatenate((y_train.reshape(-1,1),mean_train.reshape(-1,1)),axis = 1)\n",
    "                    df_train_comp = pd.DataFrame(train_comp,columns = ['true','pred'])\n",
    "                    df_train_comp['upper'] = df_train_comp['pred'] + std_train.squeeze()\n",
    "                    df_train_comp['lower'] = df_train_comp['pred'] - std_train.squeeze()\n",
    "                    df_train_comp['mode'] = 'train'\n",
    "    \n",
    "                    test_comp = np.concatenate((y_true.reshape(-1,1),y_pred.reshape(-1,1)),axis = 1)\n",
    "                    df_test_comp = pd.DataFrame(test_comp,columns = ['true','pred'])\n",
    "                    df_test_comp['upper'] = df_test_comp['pred'] + std_test.squeeze()\n",
    "                    df_test_comp['lower'] = df_test_comp['pred'] - std_test.squeeze()\n",
    "                    df_test_comp['mode'] = 'test'\n",
    "    \n",
    "\n",
    "                    arr_test_metrics[0,a] = metrics.mean_absolute_error(y_true, y_pred)\n",
    "                    arr_test_metrics[1,a] = metrics.median_absolute_error(y_true, y_pred)\n",
    "                    arr_test_metrics[2,a] = metrics.mean_squared_error(y_true, y_pred)\n",
    "                    arr_test_metrics[3,a] = metrics.root_mean_squared_error(y_true, y_pred)\n",
    "                    arr_test_metrics[4,a] = metrics.mean_absolute_percentage_error(y_true, y_pred)\n",
    "                    arr_test_metrics[5,a] = metrics.max_error(y_true, y_pred)\n",
    "                    arr_test_metrics[6,a] = metrics.explained_variance_score(y_true, y_pred)\n",
    "                    arr_test_metrics[7,a] = metrics.r2_score(y_true, y_pred)\n",
    "                    arr_test_metrics[8,a] = np.mean(std_test)\n",
    "                    arr_test_metrics[9,a] = np.min(std_test)\n",
    "                    arr_test_metrics[10,a] = np.max(std_test)\n",
    "\n",
    "                    arr_train_metrics[0,a] = metrics.mean_absolute_error(y_train, mean_train)\n",
    "                    arr_train_metrics[1,a] = metrics.median_absolute_error(y_train, mean_train)\n",
    "                    arr_train_metrics[2,a] = metrics.mean_squared_error(y_train, mean_train)\n",
    "                    arr_train_metrics[3,a] = metrics.root_mean_squared_error(y_train, mean_train)\n",
    "                    arr_train_metrics[4,a] = metrics.mean_absolute_percentage_error(y_train, mean_train)\n",
    "                    arr_train_metrics[5,a] = metrics.max_error(y_train, mean_train)\n",
    "                    arr_train_metrics[6,a] = metrics.explained_variance_score(y_train, mean_train)\n",
    "                    arr_train_metrics[7,a] = metrics.r2_score(y_train, mean_train)\n",
    "                    arr_train_metrics[8,a] = np.mean(std_train)\n",
    "                    arr_train_metrics[9,a] = np.min(std_train)\n",
    "                    arr_train_metrics[10,a] = np.max(std_train)\n",
    "                    arr_train_metrics[8,a] = np.mean(std_train)\n",
    "                    arr_train_metrics[9,a] = np.min(std_train)\n",
    "                    arr_train_metrics[10,a] = np.max(std_train)\n",
    "      \n",
    "                    ls_arr_cv_mae[j][0,i] = np.mean(arr_train_metrics[metric_row,n_cv*j:n_cv*(j+1)])\n",
    "                    ls_arr_cv_mae[j][1,i] = np.mean(arr_test_metrics[metric_row,n_cv*j:n_cv*(j+1)])\n",
    "\n",
    "                    ls_arr_cv_r2[j][0,i] = np.mean(arr_train_metrics[6,n_cv*j:n_cv*(j+1)])\n",
    "                    ls_arr_cv_r2[j][1,i] = np.mean(arr_test_metrics[6,n_cv*j:n_cv*(j+1)])\n",
    "\n",
    "                    ls_arr_cv_err[j][0,i] = np.mean(arr_train_metrics[8,n_cv*j:n_cv*(j+1)])\n",
    "                    ls_arr_cv_err[j][1,i] = np.mean(arr_test_metrics[8,n_cv*j:n_cv*(j+1)])\n",
    "\n",
    "            mean_arr_cv_mae[0,i] = np.mean(arr_train_metrics[metric_row,:])\n",
    "            mean_arr_cv_mae[1,i] = np.mean(arr_test_metrics[metric_row,:])\n",
    "\n",
    "            mean_arr_cv_r2[0,i] = np.mean(arr_train_metrics[6,:])\n",
    "            mean_arr_cv_r2[1,i] = np.mean(arr_test_metrics[6,:])\n",
    "\n",
    "            mean_arr_cv_err[0,i] = np.mean(arr_train_metrics[8,:])\n",
    "            mean_arr_cv_err[1,i] = np.mean(arr_test_metrics[8,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    sgp_cv_trainer = CV_Trainer(cfg,ls_X_train,ls_X_test,ls_y_train,ls_y_test,obj_y_scaler,cov)\n",
    "    combs,ls_model_from_combs,ls_arr_cv_mae,ls_arr_cv_r2,ls_arr_cv_err,mean_arr_cv_mae,mean_arr_cv_r2,mean_arr_cv_err = sgp_cv_trainer.set_up_sgp_cv()\n",
    "    \n",
    "    if cfg.MODEL.SPLIT == 'by-task':\n",
    "        num_tasks = cfg.DATA.NUM_TASKS\n",
    "    else:\n",
    "        num_tasks = 1\n",
    "    \n",
    "    #num_tasks = cfg.DATA.NUM_TASKS\n",
    "    num_outputs = cfg.DATA.NUM_OUTPUTS\n",
    "    num_total_outputs = num_outputs*num_tasks\n",
    "    arr_metrics = np.zeros((3,1+num_total_outputs)) #row:mae, r2, avg_std; col: total mean, task1(output1), task2(output2),...\n",
    "\n",
    "    print('number of tasks that are being compared: ',num_total_outputs)\n",
    "\n",
    "    best_idx = np.argmin(mean_arr_cv_mae[1,:])\n",
    "    #best_idx = np.argmax(mean_arr_cv_r2[1,:])\n",
    "\n",
    "    print('best_idx',best_idx)\n",
    "    print('best combinations: ',combs[best_idx])\n",
    "\n",
    "    arr_metrics[0,0] = np.min(mean_arr_cv_mae[1,:])\n",
    "    arr_metrics[1,0] = np.max(mean_arr_cv_r2[1,:])\n",
    "    arr_metrics[2,0] = mean_arr_cv_err[1,best_idx]\n",
    "\n",
    "    print('best mean mae %.3f' % arr_metrics[0,0])\n",
    "    print('best mean r2 %.3f' % arr_metrics[1,0])\n",
    "    print('best mean err %.3f' % arr_metrics[2,0])\n",
    "    \n",
    "    cols_target_new = ['total']\n",
    "    for task_ind in range(num_tasks):\n",
    "        for out_ind in range(num_outputs):\n",
    "            j = out_ind + num_outputs*task_ind\n",
    "    #for j in range(num_total_outputs):\n",
    "            j_name = 'Task_'+str(task_ind+1) + '_Output_'+str(out_ind+1)\n",
    "            cols_target_new.append(j_name)\n",
    "            arr_metrics[0,j+1] = ls_arr_cv_mae[j][1,best_idx]\n",
    "            arr_metrics[1,j+1] = ls_arr_cv_r2[j][1,best_idx]\n",
    "            arr_metrics[2,j+1] = ls_arr_cv_err[j][1,best_idx]\n",
    "            print('best mae of target %d is %.3f' %(j,ls_arr_cv_mae[j][1,best_idx]))\n",
    "            print('best r2 of target %d is %.3f' %(j,ls_arr_cv_r2[j][1,best_idx]))\n",
    "            print('best err of target %d is %.3f' % (j,ls_arr_cv_err[j][1,best_idx]))\n",
    "    \n",
    "    df_test_metrics = pd.DataFrame(np.round(arr_metrics,3),columns = cols_target_new, \n",
    "                               index = [cfg.MODEL.METRIC,'R2','AVG_STD'])\n",
    "    print(df_test_metrics)\n",
    "    \n",
    "    #output_path = '/'.join([cfg.PATH.RESULT,cfg.MISC.DATA_NAME,cfg.MODEL.MODEL_NAME,cfg.MODEL.SPLIT,cfg.MISC.DATE])\n",
    "    df_test_metrics.to_csv(setup_data.output_path+'/df_test_metrics.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MO Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([62, 5])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_outputs = 5\n",
    "mean_module = gpytorch.means.MultitaskMean(\n",
    "            gpytorch.means.ConstantMean(), num_tasks=num_outputs\n",
    "        )\n",
    "x = ls_X_train[0]\n",
    "mean_x = mean_module(x)\n",
    "mean_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([62, 62])\n",
      "torch.Size([310, 310])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MultitaskMultivariateNormal(mean shape: torch.Size([62, 5]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_kernel = gpytorch.kernels.IndexKernel(num_tasks=num_outputs, rank = 1)\n",
    "output_x = output_kernel.covar_matrix\n",
    "data_kernel = gpytorch.kernels.MaternKernel()\n",
    "data_x = data_kernel(x)\n",
    "print(data_x.shape)\n",
    "covar_x = gpytorch.lazy.KroneckerProductLazyTensor(data_x, output_x)\n",
    "print(covar_x.shape)\n",
    "result = gpytorch.distributions.MultitaskMultivariateNormal(mean_x, covar_x)\n",
    "result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mo_cv_trainer = CV_Trainer(cfg,ls_X_train,ls_X_test,ls_y_train,ls_y_test,obj_y_scaler,cov)\n",
    "combs,ls_model_from_combs,ls_arr_cv_mae,ls_arr_cv_r2,ls_arr_cv_err,mean_arr_cv_mae,mean_arr_cv_r2,mean_arr_cv_err = mo_cv_trainer.set_up_cv_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W W^T + diag(v) is the output kernel covariance matrix: \n",
      "[[ 5.4112635e+00  2.4229001e-02  9.3053007e+00 -3.8525000e+00\n",
      "  -1.6408001e-01]\n",
      " [ 2.4229001e-02  1.4141234e+00 -1.7058998e+00 -2.9240999e-01\n",
      "  -3.2570000e+00]\n",
      " [ 9.3053007e+00 -1.7058998e+00  2.4771016e+03 -9.1609998e+02\n",
      "   1.3841000e+02]\n",
      " [-3.8525000e+00 -2.9240999e-01 -9.1609998e+02  1.3570074e+03\n",
      "  -8.3879986e+00]\n",
      " [-1.6408001e-01 -3.2570000e+00  1.3841000e+02 -8.3879986e+00\n",
      "   1.6869198e+03]]\n",
      "Correlation matrix between outputs\n",
      "[[ 1.0000001   0.00875876  0.08037282 -0.04495751 -0.00171735]\n",
      " [ 0.00875876  1.0000001  -0.02882292 -0.00667509 -0.06668481]\n",
      " [ 0.08037282 -0.02882292  1.         -0.4996661   0.0677093 ]\n",
      " [-0.04495751 -0.00667509 -0.49966606  1.         -0.00554396]\n",
      " [-0.00171735 -0.0666848   0.0677093  -0.00554396  1.        ]]\n"
     ]
    }
   ],
   "source": [
    "model = ls_model_from_combs[0]\n",
    "output_kern = model.covar_module\n",
    "covar_factor = output_kern.task_covar_module.covar_factor\n",
    "print('W W^T + diag(v) is the output kernel covariance matrix: ')\n",
    "B = output_kern.task_covar_module._eval_covar_matrix().detach().numpy()\n",
    "print(B)\n",
    "\"\"\"\n",
    "print('---Manually calculating covariance matrix---')\n",
    "W = covar_factor.detach().numpy()\n",
    "B2 = np.dot(W,W.T)+np.diag(output_kern.task_covar_module.raw_var.detach().numpy())\n",
    "print('W W^T + diag(v) is the output kernel covariance matrix: ')\n",
    "print(B2)\n",
    "\"\"\"\n",
    "\n",
    "print('Correlation matrix between outputs')\n",
    "C = (B/np.sqrt(np.diag(B))).T/np.sqrt(np.diag(B))\n",
    "print(C)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of tasks that are being compared:  5\n",
      "best_idx 0\n",
      "best combinations:  (0.1, 150)\n",
      "best mean mae 0.774\n",
      "best mean r2 0.284\n",
      "best mean err 32.871\n",
      "best mae of target 0 is 0.440\n",
      "best r2 of target 0 is 0.610\n",
      "best err of target 0 is 0.632\n",
      "best mae of target 1 is 0.935\n",
      "best r2 of target 1 is 0.089\n",
      "best err of target 1 is 0.438\n",
      "best mae of target 2 is 0.133\n",
      "best r2 of target 2 is 0.882\n",
      "best err of target 2 is 51.950\n",
      "best mae of target 3 is 1.123\n",
      "best r2 of target 3 is -0.054\n",
      "best err of target 3 is 26.731\n",
      "best mae of target 4 is 1.239\n",
      "best r2 of target 4 is -0.108\n",
      "best err of target 4 is 84.607\n",
      "          total  Task_1_Output_1  Task_1_Output_2  Task_1_Output_3  \\\n",
      "NMSE      0.774            0.440            0.935            0.133   \n",
      "R2        0.284            0.610            0.089            0.882   \n",
      "AVG_STD  32.871            0.632            0.438           51.950   \n",
      "\n",
      "         Task_1_Output_4  Task_1_Output_5  \n",
      "NMSE               1.123            1.239  \n",
      "R2                -0.054           -0.108  \n",
      "AVG_STD           26.731           84.607  \n"
     ]
    }
   ],
   "source": [
    "if True:\n",
    "    if cfg.MODEL.SPLIT == 'by-task':\n",
    "        num_tasks = cfg.DATA.NUM_TASKS\n",
    "    else:\n",
    "        num_tasks = 1\n",
    "    \n",
    "    #num_tasks = cfg.DATA.NUM_TASKS\n",
    "    num_outputs = cfg.DATA.NUM_OUTPUTS\n",
    "    num_total_outputs = num_outputs*num_tasks\n",
    "    arr_metrics = np.zeros((3,1+num_total_outputs)) #row:mae, r2, avg_std; col: total mean, task1(output1), task2(output2),...\n",
    "\n",
    "    print('number of tasks that are being compared: ',num_total_outputs)\n",
    "\n",
    "    best_idx = np.argmin(mean_arr_cv_mae[1,:])\n",
    "    #best_idx = np.argmax(mean_arr_cv_r2[1,:])\n",
    "\n",
    "    print('best_idx',best_idx)\n",
    "    print('best combinations: ',combs[best_idx])\n",
    "\n",
    "    arr_metrics[0,0] = np.min(mean_arr_cv_mae[1,:])\n",
    "    arr_metrics[1,0] = np.max(mean_arr_cv_r2[1,:])\n",
    "    arr_metrics[2,0] = mean_arr_cv_err[1,best_idx]\n",
    "\n",
    "    print('best mean mae %.3f' % arr_metrics[0,0])\n",
    "    print('best mean r2 %.3f' % arr_metrics[1,0])\n",
    "    print('best mean err %.3f' % arr_metrics[2,0])\n",
    "        \n",
    "    cols_target_new = ['total']\n",
    "    for task_ind in range(num_tasks):\n",
    "        for out_ind in range(num_outputs):\n",
    "            j = out_ind + num_outputs*task_ind\n",
    "    #for j in range(num_total_outputs):\n",
    "            j_name = 'Task_'+str(task_ind+1) + '_Output_'+str(out_ind+1)\n",
    "            cols_target_new.append(j_name)\n",
    "            arr_metrics[0,j+1] = ls_arr_cv_mae[j][1,best_idx]\n",
    "            arr_metrics[1,j+1] = ls_arr_cv_r2[j][1,best_idx]\n",
    "            arr_metrics[2,j+1] = ls_arr_cv_err[j][1,best_idx]\n",
    "            print('best mae of target %d is %.3f' %(j,ls_arr_cv_mae[j][1,best_idx]))\n",
    "            print('best r2 of target %d is %.3f' %(j,ls_arr_cv_r2[j][1,best_idx]))\n",
    "            print('best err of target %d is %.3f' % (j,ls_arr_cv_err[j][1,best_idx]))\n",
    "    \n",
    "    df_test_metrics = pd.DataFrame(np.round(arr_metrics,3),columns = cols_target_new, \n",
    "                               index = [cfg.MODEL.METRIC,'R2','AVG_STD'])\n",
    "    print(df_test_metrics)\n",
    "    \n",
    "    df_test_metrics.to_csv(setup_data.output_path+'/df_test_metrics.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MTMO Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_up_mtmo_cv(self):\n",
    "        ls_model_from_combs = []\n",
    "\n",
    "        ls_lr = [0.1,0.15,0.25]#[0.1,0.15,0.2,0.25,0.3,0.35,0.40]\n",
    "        ls_n_iter = [500,700,900,1100]#[200,300,400,500,700]\n",
    "        combs = list(itertools.product(ls_lr,ls_n_iter))\n",
    "\n",
    "        mean_arr_cv_mae = np.zeros((2,len(combs))) #train,test\n",
    "        mean_arr_cv_r2 = np.zeros((2,len(combs))) #train,test\n",
    "        mean_arr_cv_err = np.zeros((2,len(combs))) #train,test\n",
    "\n",
    "        ls_arr_cv_mae = [np.zeros((2,len(combs))) for _ in range(self.num_total_output)]\n",
    "        ls_arr_cv_r2 = [np.zeros((2,len(combs))) for _ in range(self.num_total_output)]\n",
    "        ls_arr_cv_err = [np.zeros((2,len(combs))) for _ in range(self.num_total_output)]\n",
    "\n",
    "        for i in range(0,len(combs)):\n",
    "            lr,n_iter = combs[i]\n",
    "            arr_train_metrics = np.zeros((11,self.num_cols))\n",
    "            arr_test_metrics = np.zeros((11,self.num_cols))\n",
    "            pre_model = None\n",
    "            for batch_ind in range(self.n_cv):\n",
    "                t_train_x = self.ls_X_train[batch_ind]\n",
    "                t_train_y = self.ls_y_train[batch_ind]\n",
    "                t_test_x = self.ls_X_test[batch_ind]\n",
    "                t_test_y = self.ls_y_test[batch_ind]\n",
    "\n",
    "                likelihood = gpytorch.likelihoods.MultitaskGaussianLikelihood(\n",
    "                    num_tasks=self.num_outputs, \n",
    "                    rank=self.link_rank if self.link_rank is not None else 0,\n",
    "                )\n",
    "                model = MTMOGP(t_train_x, t_train_y,likelihood,\n",
    "                               task_rank = self.num_tasks,output_rank = self.num_outputs)\n",
    "\n",
    "                if batch_ind>0:\n",
    "                    source_state_dict = pre_model.state_dict()\n",
    "                    model.load_state_dict(source_state_dict)\n",
    "                # Find optimal model hyperparameters\n",
    "                model.train()\n",
    "                likelihood.train()\n",
    "\n",
    "                # Use the adam optimizer\n",
    "                optimizer = torch.optim.Adam(model.parameters(), lr=lr)  # Includes GaussianLikelihood parameters\n",
    "\n",
    "                # \"Loss\" for GPs - the marginal log likelihood\n",
    "                mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "                training_iterations = n_iter#500\n",
    "                for _ in range(training_iterations):\n",
    "                    optimizer.zero_grad()\n",
    "                    output = model(t_train_x)\n",
    "                    loss = -mll(output, t_train_y)\n",
    "                    loss.backward()\n",
    "                    #print('Iter %d/%d - Loss: %.3f' % (i + 1, training_iterations, loss.item()))\n",
    "                    optimizer.step()\n",
    "\n",
    "                ls_model_from_combs.append(model)\n",
    "                if batch_ind == 0:\n",
    "                    pre_model = model\n",
    "\n",
    "                # Set into eval mode\n",
    "                model.eval()\n",
    "                likelihood.eval()\n",
    "\n",
    "                arr_train_x = t_train_x.detach().numpy()\n",
    "                arr_test_x = t_test_x.detach().numpy()\n",
    "                with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "                    test_pred = likelihood(model(t_test_x))   \n",
    "                    test_mean = test_pred.mean\n",
    "                    test_lower, test_upper = test_pred.confidence_region()\n",
    "                    train_pred = likelihood(model(t_train_x))\n",
    "                    train_mean = train_pred.mean\n",
    "                    train_lower, train_upper = train_pred.confidence_region()\n",
    "    \n",
    "                if self.y_scale=='no-y-scale':\n",
    "                    arr_train_y = t_train_y.detach().numpy()\n",
    "                    arr_test_y = t_test_y.detach().numpy()\n",
    "                    arr_test_mean = test_mean.detach().numpy()\n",
    "                    arr_test_lower = test_lower.detach().numpy()\n",
    "                    arr_test_upper = test_upper.detach().numpy()\n",
    "\n",
    "                    arr_train_mean = train_mean.detach().numpy()\n",
    "                    arr_train_lower = train_lower.detach().numpy()\n",
    "                    arr_train_upper = train_upper.detach().numpy()\n",
    "                else:\n",
    "                    y_scaler = self.obj_y_scaler[batch_ind]\n",
    "                    arr_train_y = y_scaler.inverse_transform(t_train_y)\n",
    "                    arr_test_y = t_test_y.detach().numpy()\n",
    "\n",
    "                    arr_train_mean = y_scaler.inverse_transform(train_mean)\n",
    "                    arr_train_lower = y_scaler.inverse_transform(train_lower)\n",
    "                    arr_train_upper = y_scaler.inverse_transform(train_upper)\n",
    "\n",
    "                    arr_test_mean = y_scaler.inverse_transform(test_mean)\n",
    "                    arr_test_lower = y_scaler.inverse_transform(test_lower)\n",
    "                    arr_test_upper = y_scaler.inverse_transform(test_upper)\n",
    "\n",
    "                for task_ind in range(self.num_tasks):\n",
    "                    for j in range(self.num_outputs):\n",
    "                        mtmo_ind = j + self.num_outputs*task_ind\n",
    "                        row_train_inds = arr_train_x[:,-1]==task_ind\n",
    "                        r_train_y = arr_train_y[row_train_inds,j]\n",
    "                        r_train_mean_y =arr_train_mean[row_train_inds,j]\n",
    "                        r_train_lower_y =arr_train_lower[row_train_inds,j]\n",
    "                        r_train_upper_y =arr_train_upper[row_train_inds,j]\n",
    "                        r_train_std = r_train_mean_y - r_train_lower_y\n",
    "\n",
    "                        row_test_inds = arr_test_x[:,-1]==task_ind\n",
    "                        r_test_y = arr_test_y[row_test_inds,j]\n",
    "                        r_test_mean_y =arr_test_mean[row_test_inds,j]\n",
    "                        r_test_lower_y =arr_test_lower[row_test_inds,j]\n",
    "                        r_test_upper_y =arr_test_upper[row_test_inds,j]\n",
    "                        r_test_std = r_test_mean_y - r_test_lower_y\n",
    "    \n",
    "                        train_comp = np.concatenate((r_train_y.reshape(-1,1),r_train_mean_y.reshape(-1,1)),axis = 1)\n",
    "                        df_train_comp = pd.DataFrame(train_comp,columns = ['true','pred'])\n",
    "                        df_train_comp['upper'] = r_train_upper_y\n",
    "                        df_train_comp['lower'] = r_train_lower_y\n",
    "                        df_train_comp['mode'] = 'train'\n",
    "    \n",
    "                        test_comp = np.concatenate((r_test_y.reshape(-1,1),r_test_mean_y.reshape(-1,1)),axis = 1)\n",
    "                        df_test_comp = pd.DataFrame(test_comp,columns = ['true','pred'])\n",
    "                        df_test_comp['upper'] = r_test_upper_y\n",
    "                        df_test_comp['lower'] = r_test_lower_y\n",
    "                        df_test_comp['mode'] = 'test'\n",
    "\n",
    "                        y_true = r_test_y\n",
    "                        y_pred = r_test_mean_y\n",
    "                        y_train = r_train_y\n",
    "                        mean_train = r_train_mean_y\n",
    "    \n",
    "                        a = batch_ind + self.n_cv*mtmo_ind\n",
    "                        #a = mtmo_ind + self.num_total_output*batch_ind\n",
    "                        arr_test_metrics[0,a] = metrics.mean_absolute_error(y_true, y_pred)\n",
    "                        arr_test_metrics[1,a] = metrics.median_absolute_error(y_true, y_pred)\n",
    "                        arr_test_metrics[2,a] = metrics.mean_squared_error(y_true, y_pred)\n",
    "                        arr_test_metrics[3,a] = metrics.root_mean_squared_error(y_true, y_pred)\n",
    "                        arr_test_metrics[4,a] = metrics.mean_absolute_percentage_error(y_true, y_pred)\n",
    "                        arr_test_metrics[5,a] = metrics.max_error(y_true, y_pred)\n",
    "                        arr_test_metrics[6,a] = metrics.explained_variance_score(y_true, y_pred)\n",
    "                        arr_test_metrics[7,a] = metrics.r2_score(y_true, y_pred)\n",
    "                        arr_test_metrics[8,a] = np.mean(r_test_std)\n",
    "                        arr_test_metrics[9,a] = np.min(r_test_std)\n",
    "                        arr_test_metrics[10,a] = np.max(r_test_std)\n",
    "\n",
    "                        arr_train_metrics[0,a] = metrics.mean_absolute_error(y_train, mean_train)\n",
    "                        arr_train_metrics[1,a] = metrics.median_absolute_error(y_train, mean_train)\n",
    "                        arr_train_metrics[2,a] = metrics.mean_squared_error(y_train, mean_train)\n",
    "                        arr_train_metrics[3,a] = metrics.root_mean_squared_error(y_train, mean_train)\n",
    "                        arr_train_metrics[4,a] = metrics.mean_absolute_percentage_error(y_train, mean_train)\n",
    "                        arr_train_metrics[5,a] = metrics.max_error(y_train, mean_train)\n",
    "                        arr_train_metrics[6,a] = metrics.explained_variance_score(y_train, mean_train)\n",
    "                        arr_train_metrics[7,a] = metrics.r2_score(y_train, mean_train)\n",
    "                        arr_train_metrics[8,a] = np.mean(r_train_std)\n",
    "                        arr_train_metrics[9,a] = np.min(r_train_std)\n",
    "                        arr_train_metrics[10,a] = np.max(r_train_std)\n",
    "                        arr_train_metrics[8,a] = np.mean(r_train_std)\n",
    "                        arr_train_metrics[9,a] = np.min(r_train_std)\n",
    "                        arr_train_metrics[10,a] = np.max(r_train_std)\n",
    "      \n",
    "                        ls_arr_cv_mae[mtmo_ind][0,i] = np.mean(arr_train_metrics[self.metric_row,self.n_cv*mtmo_ind:self.n_cv*(mtmo_ind+1)])\n",
    "                        ls_arr_cv_mae[mtmo_ind][1,i] = np.mean(arr_test_metrics[self.metric_row,self.n_cv*mtmo_ind:self.n_cv*(mtmo_ind+1)])\n",
    "\n",
    "                        ls_arr_cv_r2[mtmo_ind][0,i] = np.mean(arr_train_metrics[6,self.n_cv*mtmo_ind:self.n_cv*(mtmo_ind+1)])\n",
    "                        ls_arr_cv_r2[mtmo_ind][1,i] = np.mean(arr_test_metrics[6,self.n_cv*mtmo_ind:self.n_cv*(mtmo_ind+1)])\n",
    "\n",
    "                        ls_arr_cv_err[mtmo_ind][0,i] = np.mean(arr_train_metrics[8,self.n_cv*mtmo_ind:self.n_cv*(mtmo_ind+1)])\n",
    "                        ls_arr_cv_err[mtmo_ind][1,i] = np.mean(arr_test_metrics[8,self.n_cv*mtmo_ind:self.n_cv*(mtmo_ind+1)])\n",
    "\n",
    "            mean_arr_cv_mae[0,i] = np.mean(arr_train_metrics[self.metric_row,:])\n",
    "            mean_arr_cv_mae[1,i] = np.mean(arr_test_metrics[self.metric_row,:])\n",
    "\n",
    "            mean_arr_cv_r2[0,i] = np.mean(arr_train_metrics[6,:])\n",
    "            mean_arr_cv_r2[1,i] = np.mean(arr_test_metrics[6,:])\n",
    "\n",
    "            mean_arr_cv_err[0,i] = np.mean(arr_train_metrics[8,:])\n",
    "            mean_arr_cv_err[1,i] = np.mean(arr_test_metrics[8,:])\n",
    "\n",
    "\n",
    "        return combs,ls_model_from_combs,ls_arr_cv_mae,ls_arr_cv_r2,ls_arr_cv_err,mean_arr_cv_mae,mean_arr_cv_r2,mean_arr_cv_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    mtmo_cv_trainer = CV_Trainer(cfg,ls_X_train,ls_X_test,ls_y_train,ls_y_test,obj_y_scaler)\n",
    "    combs,ls_model_from_combs,ls_arr_cv_mae,ls_arr_cv_r2,ls_arr_cv_err,mean_arr_cv_mae,mean_arr_cv_r2,mean_arr_cv_err = mtmo_cv_trainer.set_up_cv_model()\n",
    "\n",
    "    if cfg.MODEL.SPLIT == 'by-task':\n",
    "        num_tasks = cfg.DATA.NUM_TASKS\n",
    "    else:\n",
    "        num_tasks = 1\n",
    "    num_outputs = cfg.DATA.NUM_OUTPUTS\n",
    "    num_total_outputs = num_outputs*num_tasks\n",
    "    arr_metrics = np.zeros((3,1+num_total_outputs)) #row:mae, r2, avg_std; col: total mean, task1(output1), task2(output2),...\n",
    "\n",
    "    print('number of tasks that are being compared: ',num_total_outputs)\n",
    "\n",
    "    best_idx = np.argmin(mean_arr_cv_mae[1,:])\n",
    "    #best_idx = np.argmax(mean_arr_cv_r2[1,:])\n",
    "\n",
    "    print('best_idx',best_idx)\n",
    "    print('best combinations: ',combs[best_idx])\n",
    "\n",
    "    arr_metrics[0,0] = np.min(mean_arr_cv_mae[1,:])\n",
    "    arr_metrics[1,0] = np.max(mean_arr_cv_r2[1,:])\n",
    "    arr_metrics[2,0] = mean_arr_cv_err[1,best_idx]\n",
    "\n",
    "    print('best mean mae %.3f' % arr_metrics[0,0])\n",
    "    print('best mean r2 %.3f' % arr_metrics[1,0])\n",
    "    print('best mean err %.3f' % arr_metrics[2,0])\n",
    "    \n",
    "    cols_target_new = ['total']\n",
    "    #for j in range(cfg.DATA.NUM_OUTPUTS):\n",
    "    for task_ind in range(num_tasks):\n",
    "        for out_ind in range(num_outputs):\n",
    "            j = out_ind + num_outputs*task_ind\n",
    "            j_name = 'Task_'+str(task_ind+1) + '_Output_'+str(out_ind+1)\n",
    "            cols_target_new.append(j_name)\n",
    "            arr_metrics[0,j+1] = ls_arr_cv_mae[j][1,best_idx]\n",
    "            arr_metrics[1,j+1] = ls_arr_cv_r2[j][1,best_idx]\n",
    "            arr_metrics[2,j+1] = ls_arr_cv_err[j][1,best_idx]\n",
    "            print('best mae of target %d is %.3f' %(j,ls_arr_cv_mae[j][1,best_idx]))\n",
    "            print('best r2 of target %d is %.3f' %(j,ls_arr_cv_r2[j][1,best_idx]))\n",
    "            print('best err of target %d is %.3f' % (j,ls_arr_cv_err[j][1,best_idx]))\n",
    "    \n",
    "    df_test_metrics = pd.DataFrame(np.round(arr_metrics,3),columns = cols_target_new, \n",
    "                               index = [cfg.MODEL.METRIC,'R2','AVG_STD'])\n",
    "    print(df_test_metrics)\n",
    "    \n",
    "    output_path = '/'.join([cfg.PATH.RESULT,cfg.MISC.DATA_NAME,cfg.MODEL.MODEL_NAME,cfg.MODEL.SPLIT,cfg.MISC.DATE,'num_tasks_'+str(cfg.DATA.NUM_TASKS)])\n",
    "    df_test_metrics.to_csv(output_path+'/df_test_metrics.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MTMO-LMGP CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    mtmolmgp_cv_trainer = CV_Trainer(cfg,ls_X_train,ls_X_test,ls_y_train,ls_y_test,obj_y_scaler)\n",
    "    combs,ls_model_from_combs,ls_arr_cv_mae,ls_arr_cv_r2,ls_arr_cv_err,mean_arr_cv_mae,mean_arr_cv_r2,mean_arr_cv_err = mtmolmgp_cv_trainer.set_up_cv_model()\n",
    "\n",
    "    if cfg.MODEL.SPLIT == 'by-task':\n",
    "        num_tasks = cfg.DATA.NUM_TASKS\n",
    "    else:\n",
    "        num_tasks = 1\n",
    "    num_outputs = cfg.DATA.NUM_OUTPUTS\n",
    "    num_total_outputs = num_outputs*num_tasks\n",
    "    arr_metrics = np.zeros((3,1+num_total_outputs)) #row:mae, r2, avg_std; col: total mean, task1(output1), task2(output2),...\n",
    "\n",
    "    print('number of tasks that are being compared: ',num_total_outputs)\n",
    "\n",
    "    best_idx = np.argmin(mean_arr_cv_mae[1,:])\n",
    "    #best_idx = np.argmax(mean_arr_cv_r2[1,:])\n",
    "\n",
    "    print('best_idx',best_idx)\n",
    "    print('best combinations: ',combs[best_idx])\n",
    "\n",
    "    arr_metrics[0,0] = np.min(mean_arr_cv_mae[1,:])\n",
    "    arr_metrics[1,0] = np.max(mean_arr_cv_r2[1,:])\n",
    "    arr_metrics[2,0] = mean_arr_cv_err[1,best_idx]\n",
    "\n",
    "    print('best mean mae %.3f' % arr_metrics[0,0])\n",
    "    print('best mean r2 %.3f' % arr_metrics[1,0])\n",
    "    print('best mean err %.3f' % arr_metrics[2,0])\n",
    "    \n",
    "    cols_target_new = ['total']\n",
    "    #for j in range(cfg.DATA.NUM_OUTPUTS):\n",
    "    for task_ind in range(num_tasks):\n",
    "        for out_ind in range(num_outputs):\n",
    "            j = out_ind + num_outputs*task_ind\n",
    "            j_name = 'Task_'+str(task_ind+1) + '_Output_'+str(out_ind+1)\n",
    "            cols_target_new.append(j_name)\n",
    "            arr_metrics[0,j+1] = ls_arr_cv_mae[j][1,best_idx]\n",
    "            arr_metrics[1,j+1] = ls_arr_cv_r2[j][1,best_idx]\n",
    "            arr_metrics[2,j+1] = ls_arr_cv_err[j][1,best_idx]\n",
    "            print('best mae of target %d is %.3f' %(j,ls_arr_cv_mae[j][1,best_idx]))\n",
    "            print('best r2 of target %d is %.3f' %(j,ls_arr_cv_r2[j][1,best_idx]))\n",
    "            print('best err of target %d is %.3f' % (j,ls_arr_cv_err[j][1,best_idx]))\n",
    "    \n",
    "    df_test_metrics = pd.DataFrame(np.round(arr_metrics,3),columns = cols_target_new, \n",
    "                               index = ['MAE','R2','AVG_STD'])\n",
    "    print(df_test_metrics)\n",
    "    \n",
    "    output_path = '/'.join([cfg.PATH.RESULT,cfg.MISC.DATA_NAME,cfg.MODEL.MODEL_NAME,cfg.MODEL.SPLIT,cfg.MISC.DATE,'num_tasks_'+str(cfg.DATA.NUM_TASKS)])\n",
    "    df_test_metrics.to_csv(output_path+'/df_test_metrics.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local-gp-mac-no-tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
