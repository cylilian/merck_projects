{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom gpplus.models import GP_Plus\\nfrom gpplus.test_functions.analytical import borehole_mixed_variables\\nfrom gpplus.preprocessing import train_test_split_normalizeX\\nfrom gpplus.utils import set_seed\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from gpplus.models import GP_Plus\n",
    "from gpplus.test_functions.analytical import borehole_mixed_variables\n",
    "from gpplus.preprocessing import train_test_split_normalizeX\n",
    "from gpplus.utils import set_seed\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chenya68/anaconda3/envs/gpplus-py39/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Copyright Â© 2023, University of California, Irvine.\n",
    "#\n",
    "# GP_Plus is a proprietary software of the University of California, Irvine. This software\n",
    "# is available for use free of charge for educational and research purposes by non-profit\n",
    "# institutions and US government agencies. Other organizations are permitted to use \n",
    "# GP_Plus solely for evaluation purposes. Any further utilization requires explicit, prior\n",
    "# written consent. Sale or unauthorized redistribution of this software is strictly \n",
    "# prohibited. Users may create copies for personal use, provided that these copies are \n",
    "# not sold or distributed and are subject to the same terms and conditions as outlined \n",
    "# herein.\n",
    "#\n",
    "# This software is provided as research software. As such, it is made available \"as is\" \n",
    "# without any warranty of any kind, either express or implied. By downloading or using \n",
    "# any part of this software, the user implicitly agrees to these terms. Please note that\n",
    "# these terms and conditions are subject to modification at any time without prior notice.\n",
    "\n",
    "import torch\n",
    "import gpytorch\n",
    "import math\n",
    "from gpytorch.models import ExactGP\n",
    "from gpytorch import settings as gptsettings\n",
    "from gpytorch.priors import NormalPrior,LogNormalPrior\n",
    "from gpytorch.constraints import GreaterThan,Positive\n",
    "from gpytorch.distributions import MultivariateNormal,MultitaskMultivariateNormal\n",
    "from gpplus import kernels\n",
    "from gpplus.priors import LogHalfHorseshoePrior,MollifiedUniformPrior\n",
    "from gpplus.utils.transforms import softplus,inv_softplus\n",
    "from typing import List,Tuple,Union\n",
    "from gpplus.likelihoods_noise.multifidelity import Multifidelity_likelihood\n",
    "from botorch.models.utils import gpt_posterior_settings\n",
    "from botorch.posteriors.gpytorch import GPyTorchPosterior\n",
    "from botorch.models.gpytorch import BatchedMultiOutputGPyTorchModel, GPyTorchModel\n",
    "from botorch import settings\n",
    "from botorch.models.utils import fantasize as fantasize_flag, validate_input_scaling\n",
    "from botorch.sampling.samplers import MCSampler\n",
    "from torch import Tensor\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "\n",
    "class GPR2(ExactGP, GPyTorchModel):\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_x:torch.Tensor,\n",
    "        train_y:torch.Tensor,\n",
    "        correlation_kernel,\n",
    "        noise_indices:List[int],\n",
    "        noise:float=1e-4,\n",
    "        fix_noise:bool=False,\n",
    "        lb_noise:float=1e-12,\n",
    "    ) -> None:\n",
    "        # check inputs\n",
    "        if not torch.is_tensor(train_x):\n",
    "            raise RuntimeError(\"'train_x' must be a tensor\")\n",
    "        if not torch.is_tensor(train_y):\n",
    "            raise RuntimeError(\"'train_y' must be a tensor\")\n",
    "\n",
    "        if train_x.shape[0] != train_y.shape[0]:\n",
    "            raise RuntimeError(\"Inputs and output have different number of observations\")\n",
    "        \n",
    "        # initializing likelihood\n",
    "        noise_constraint=GreaterThan(lb_noise,transform=torch.exp,inv_transform=torch.log)\n",
    "        \n",
    "        if len(noise_indices) == 0:\n",
    "\n",
    "            likelihood = gpytorch.likelihoods.GaussianLikelihood(noise_constraint=noise_constraint)\n",
    "        else:\n",
    "\n",
    "            likelihood = Multifidelity_likelihood(noise_constraint=noise_constraint, noise_indices=noise_indices, fidel_indices=train_x[:,-1])\n",
    "        y_mean= train_y.min(dim = 0).values\n",
    "        y_std=train_y.max(dim = 0).values-train_y.min(dim = 0).values\n",
    "        train_y_sc = (train_y-y_mean)/y_std\n",
    "\n",
    "        ExactGP.__init__(self, train_x,train_y_sc, likelihood)\n",
    "        \n",
    "        # registering mean and std of the raw response\n",
    "        self.register_buffer('y_mean',y_mean)\n",
    "        self.register_buffer('y_std',y_std)\n",
    "        self.register_buffer('y_scaled',train_y_sc)\n",
    "\n",
    "        self._num_outputs = 1\n",
    "\n",
    "        # initializing and fixing noise\n",
    "        if noise is not None:\n",
    "            self.likelihood.initialize(noise=noise)\n",
    "        \n",
    "        self.likelihood.register_prior('noise_prior',LogHalfHorseshoePrior(0.01,lb_noise),'raw_noise')\n",
    "        if fix_noise:\n",
    "            self.likelihood.raw_noise.requires_grad_(False)\n",
    "            self.likelihood.noise_covar.noise =torch.tensor(4.9901e-05)\n",
    "\n",
    "        if isinstance(correlation_kernel,str):\n",
    "            try:\n",
    "                correlation_kernel_class = getattr(kernels,correlation_kernel)\n",
    "                correlation_kernel = correlation_kernel_class(\n",
    "                    ard_num_dims = self.train_inputs[0].size(1),\n",
    "                    lengthscale_constraint=Positive(transform=torch.exp,inv_transform=torch.log),\n",
    "                )\n",
    "                correlation_kernel.register_prior(\n",
    "                    'lengthscale_prior',MollifiedUniformPrior(math.log(0.1),math.log(10)),'raw_lengthscale'\n",
    "                )\n",
    "            except:\n",
    "                raise RuntimeError(\n",
    "                    \"%s not an allowed kernel\" % correlation_kernel\n",
    "                )\n",
    "        elif not isinstance(correlation_kernel,gpytorch.kernels.Kernel):\n",
    "            raise RuntimeError(\n",
    "                \"specified correlation kernel is not a `gpytorch.kernels.Kernel` instance\"\n",
    "            )\n",
    "\n",
    "        self.covar_module = kernels.ScaleKernel(\n",
    "            base_kernel = correlation_kernel,\n",
    "            outputscale_constraint=Positive(transform=softplus,inv_transform=inv_softplus),\n",
    "        )\n",
    "        # register priors\n",
    "        self.covar_module.register_prior(\n",
    "            'outputscale_prior',LogNormalPrior(1e-6,1.),'outputscale'\n",
    "        )\n",
    "    \n",
    "    def forward(self,x:torch.Tensor)->MultivariateNormal:\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return MultivariateNormal(mean_x,covar_x)\n",
    "    \n",
    "    def predict(\n",
    "        self,x:torch.Tensor,return_std:bool=False,include_noise:bool=False\n",
    "    )-> Union[torch.Tensor,Tuple[torch.Tensor]]:\n",
    "\n",
    "        self.eval()\n",
    "        with gptsettings.fast_computations(log_prob=False):\n",
    "            # determine if batched or not\n",
    "            ndim = self.train_targets.ndim\n",
    "            if ndim == 1:\n",
    "                output = self(x)\n",
    "            else:\n",
    "                # for batched GPs \n",
    "                num_samples = self.train_targets.shape[0]\n",
    "                output = self(x.unsqueeze(0).repeat(num_samples,1,1))\n",
    "            self.fidel_indices=x[:,-1]\n",
    "            if return_std and include_noise:\n",
    "                # x=self.likelihood(x)\n",
    "                output = self.likelihood(output)\n",
    "\n",
    "            out_mean = self.y_mean + self.y_std*output.mean\n",
    "            \n",
    "            # standard deviation may not always be needed\n",
    "            if return_std:\n",
    "                out_std = output.variance.sqrt()*self.y_std\n",
    "                return out_mean,out_std\n",
    "\n",
    "            return out_mean\n",
    "\n",
    "    def posterior(\n",
    "        self,\n",
    "        X,\n",
    "        output_indices = None,\n",
    "        observation_noise= True,\n",
    "        posterior_transform= None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "\n",
    "        self.eval()\n",
    "        with gpt_posterior_settings() and gptsettings.fast_computations(log_prob=False):\n",
    "    \n",
    "            if observation_noise:\n",
    "                return GPyTorchPosterior(mvn = self.likelihood(self(X.double())))\n",
    "            else:\n",
    "                return GPyTorchPosterior(mvn = self(X.double()))\n",
    "    \n",
    "    def reset_parameters(self) -> None:\n",
    "        \"\"\"Reset parameters by sampling from prior\n",
    "        \"\"\"\n",
    "        for _,module,prior,closure,setting_closure in self.named_priors():\n",
    "            if not closure(module).requires_grad:\n",
    "                continue\n",
    "            setting_closure(module,prior.expand(closure(module).shape).sample().to(**self.tkwargs))\n",
    "\n",
    "\n",
    "    def fantasize(\n",
    "            self,\n",
    "            X: Tensor,\n",
    "            sampler: MCSampler,\n",
    "            observation_noise: Union[bool, Tensor] = True,\n",
    "            **kwargs: Any,\n",
    "        ):\n",
    "            r\"\"\"Constructs a fantasy model using a specified procedure.\n",
    "\n",
    "            This method constructs a fantasy model by following these steps:\n",
    "            1. Compute the model's posterior at `X`. If `observation_noise=True`, the posterior\n",
    "            includes observation noise, which is determined as the mean of the observation\n",
    "            noise in the training data. If `observation_noise` is a Tensor, it is used directly\n",
    "            as the observation noise.\n",
    "            2. Sample from this posterior using the provided `sampler` to create \"fake\" observations.\n",
    "            3. Update (condition) the model with these new fake observations.\n",
    "\n",
    "            Args:\n",
    "                X: A Tensor of dimensions `batch_shape x n' x d`, where `d` represents the feature\n",
    "                space dimension, `n'` is the number of points per batch, and `batch_shape`\n",
    "                is the batch shape. This batch shape must be compatible with the model's\n",
    "                existing batch shape.\n",
    "                sampler: A sampler used for drawing samples from the model's posterior at `X`.\n",
    "                observation_noise: A boolean or a Tensor. If True, the mean of the observation\n",
    "                                noise from the training data is used in the posterior. If a\n",
    "                                Tensor, it specifies the observation noise directly.\n",
    "\n",
    "            Returns:\n",
    "                A fantasy model, updated based on the sampled fake observations.\n",
    "            \"\"\"\n",
    "            propagate_grads = kwargs.pop(\"propagate_grads\", False)\n",
    "            with fantasize_flag():\n",
    "                with settings.propagate_grads(propagate_grads):\n",
    "                    post_X = self.posterior(\n",
    "                        X, observation_noise=observation_noise, **kwargs\n",
    "                    )\n",
    "                Y_fantasized = sampler(post_X)  # num_fantasies x batch_shape x n' x m\n",
    "                # Use the mean of the previous noise values (TODO: be smarter here).\n",
    "                # noise should be batch_shape x q x m when X is batch_shape x q x d, and\n",
    "                # Y_fantasized is num_fantasies x batch_shape x q x m.\n",
    "                noise_shape = Y_fantasized.shape[1:]\n",
    "                noise = self.likelihood.noise.mean().expand(noise_shape)\n",
    "                return self.condition_on_observations(\n",
    "                    X=self.transform_inputs(X), Y=Y_fantasized, noise=noise\n",
    "                )\n",
    "\n",
    "\n",
    "\n",
    "class MultitaskGPModel(ExactGP, GPyTorchModel):\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_x:torch.Tensor,\n",
    "        train_y:torch.Tensor,\n",
    "        noise_indices:List[int],\n",
    "        correlation_kernel,\n",
    "        noise:float=1e-4,\n",
    "        fix_noise:bool=False,\n",
    "        lb_noise:float=1e-12,\n",
    "        task_rank:int=None,\n",
    "    ) -> None:\n",
    "        # check inputs\n",
    "        if not torch.is_tensor(train_x):\n",
    "            raise RuntimeError(\"'train_x' must be a tensor\")\n",
    "        if not torch.is_tensor(train_y):\n",
    "            raise RuntimeError(\"'train_y' must be a tensor\")\n",
    "\n",
    "        if train_x.shape[0] != train_y.shape[0]:\n",
    "            raise RuntimeError(\"Inputs and output have different number of observations\")\n",
    "        \n",
    "        self._num_tasks = train_y.shape[1]\n",
    "        self._task_rank = task_rank if task_rank is not None else self._num_tasks\n",
    "\n",
    "        # initializing likelihood\n",
    "        noise_constraint=GreaterThan(lb_noise,transform=torch.exp,inv_transform=torch.log)\n",
    "        \n",
    "        \n",
    "\n",
    "        #likelihood = gpytorch.likelihoods.GaussianLikelihood(noise_constraint=noise_constraint)\n",
    "        likelihood = gpytorch.likelihoods.MultitaskGaussianLikelihood(num_tasks=self._num_tasks,\n",
    "                                                                          noise_constraint=noise_constraint)\n",
    "        y_mean= train_y.min(dim = 0).values\n",
    "        y_std=train_y.max(dim = 0).values-train_y.min(dim = 0).values\n",
    "        train_y_sc = (train_y-y_mean)/y_std\n",
    "\n",
    "        super(MultitaskGPModel, self).__init__(train_x, train_y_sc,likelihood)\n",
    "        #ExactGP.__init__(self, train_x,train_y_sc, likelihood)\n",
    "        \n",
    "        # registering mean and std of the raw response\n",
    "        self.register_buffer('y_mean',y_mean)\n",
    "        self.register_buffer('y_std',y_std)\n",
    "        self.register_buffer('y_scaled',train_y_sc)\n",
    "\n",
    "        \n",
    "\n",
    "        # initializing and fixing noise\n",
    "        if noise is not None:\n",
    "            self.likelihood.initialize(noise=noise)\n",
    "        \n",
    "        self.likelihood.register_prior('noise_prior',LogHalfHorseshoePrior(0.01,lb_noise),'raw_noise')\n",
    "        if fix_noise:\n",
    "            self.likelihood.raw_noise.requires_grad_(False)\n",
    "            self.likelihood.noise_covar.noise =torch.tensor(4.9901e-05)\n",
    "\n",
    "        self.mean_module = gpytorch.means.MultitaskMean(\n",
    "            gpytorch.means.ConstantMean(), num_tasks=self._num_tasks\n",
    "        )\n",
    "\n",
    "        \n",
    "        if isinstance(correlation_kernel,str):\n",
    "            try:\n",
    "                correlation_kernel_class = getattr(kernels,correlation_kernel)\n",
    "                correlation_kernel = correlation_kernel_class(\n",
    "                    ard_num_dims = self.train_inputs[0].size(1),\n",
    "                    lengthscale_constraint=Positive(transform=torch.exp,inv_transform=torch.log),\n",
    "                )\n",
    "                correlation_kernel.register_prior(\n",
    "                    'lengthscale_prior',MollifiedUniformPrior(math.log(0.1),math.log(10)),'raw_lengthscale'\n",
    "                )\n",
    "            except:\n",
    "                raise RuntimeError(\n",
    "                    \"%s not an allowed kernel\" % correlation_kernel\n",
    "                )\n",
    "        elif not isinstance(correlation_kernel,gpytorch.kernels.Kernel):\n",
    "            raise RuntimeError(\n",
    "                \"specified correlation kernel is not a `gpytorch.kernels.Kernel` instance\"\n",
    "            )\n",
    "        \n",
    "        self.covar_module = gpytorch.kernels.MultitaskKernel(\n",
    "            correlation_kernel, num_tasks=self._num_tasks, rank=1)\n",
    "        \n",
    "    \n",
    "    def forward(self,x:torch.Tensor):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultitaskMultivariateNormal(mean_x,covar_x)\n",
    "    \n",
    "    def predict(\n",
    "        self,x:torch.Tensor,return_std:bool=False,include_noise:bool=False\n",
    "    )-> Union[torch.Tensor,Tuple[torch.Tensor]]:\n",
    "\n",
    "        self.eval()\n",
    "        with gptsettings.fast_computations(log_prob=False):\n",
    "            # determine if batched or not\n",
    "            ndim = self.train_targets.ndim\n",
    "            if ndim == 1:\n",
    "                output = self(x)\n",
    "            else:\n",
    "                # for batched GPs \n",
    "                num_samples = self.train_targets.shape[0]\n",
    "                output = self(x.unsqueeze(0).repeat(num_samples,1,1))\n",
    "            self.fidel_indices=x[:,-1]\n",
    "            if return_std and include_noise:\n",
    "                # x=self.likelihood(x)\n",
    "                output = self.likelihood(output)\n",
    "\n",
    "            out_mean = self.y_mean + self.y_std*output.mean\n",
    "            \n",
    "            # standard deviation may not always be needed\n",
    "            if return_std:\n",
    "                out_std = output.variance.sqrt()*self.y_std\n",
    "                return out_mean,out_std\n",
    "\n",
    "            return out_mean\n",
    "\n",
    "    def posterior(\n",
    "        self,\n",
    "        X,\n",
    "        output_indices = None,\n",
    "        observation_noise= True,\n",
    "        posterior_transform= None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "\n",
    "        self.eval()\n",
    "        with gpt_posterior_settings() and gptsettings.fast_computations(log_prob=False):\n",
    "    \n",
    "            if observation_noise:\n",
    "                return GPyTorchPosterior(mvn = self.likelihood(self(X.double())))\n",
    "            else:\n",
    "                return GPyTorchPosterior(mvn = self(X.double()))\n",
    "    \n",
    "    def reset_parameters(self) -> None:\n",
    "        \"\"\"Reset parameters by sampling from prior\n",
    "        \"\"\"\n",
    "        for _,module,prior,closure,setting_closure in self.named_priors():\n",
    "            if not closure(module).requires_grad:\n",
    "                continue\n",
    "            setting_closure(module,prior.expand(closure(module).shape).sample().to(**self.tkwargs))\n",
    "\n",
    "\n",
    "    def fantasize(\n",
    "            self,\n",
    "            X: Tensor,\n",
    "            sampler: MCSampler,\n",
    "            observation_noise: Union[bool, Tensor] = True,\n",
    "            **kwargs: Any,\n",
    "        ):\n",
    "            r\"\"\"Constructs a fantasy model using a specified procedure.\n",
    "\n",
    "            This method constructs a fantasy model by following these steps:\n",
    "            1. Compute the model's posterior at `X`. If `observation_noise=True`, the posterior\n",
    "            includes observation noise, which is determined as the mean of the observation\n",
    "            noise in the training data. If `observation_noise` is a Tensor, it is used directly\n",
    "            as the observation noise.\n",
    "            2. Sample from this posterior using the provided `sampler` to create \"fake\" observations.\n",
    "            3. Update (condition) the model with these new fake observations.\n",
    "\n",
    "            Args:\n",
    "                X: A Tensor of dimensions `batch_shape x n' x d`, where `d` represents the feature\n",
    "                space dimension, `n'` is the number of points per batch, and `batch_shape`\n",
    "                is the batch shape. This batch shape must be compatible with the model's\n",
    "                existing batch shape.\n",
    "                sampler: A sampler used for drawing samples from the model's posterior at `X`.\n",
    "                observation_noise: A boolean or a Tensor. If True, the mean of the observation\n",
    "                                noise from the training data is used in the posterior. If a\n",
    "                                Tensor, it specifies the observation noise directly.\n",
    "\n",
    "            Returns:\n",
    "                A fantasy model, updated based on the sampled fake observations.\n",
    "            \"\"\"\n",
    "            propagate_grads = kwargs.pop(\"propagate_grads\", False)\n",
    "            with fantasize_flag():\n",
    "                with settings.propagate_grads(propagate_grads):\n",
    "                    post_X = self.posterior(\n",
    "                        X, observation_noise=observation_noise, **kwargs\n",
    "                    )\n",
    "                Y_fantasized = sampler(post_X)  # num_fantasies x batch_shape x n' x m\n",
    "                # Use the mean of the previous noise values (TODO: be smarter here).\n",
    "                # noise should be batch_shape x q x m when X is batch_shape x q x d, and\n",
    "                # Y_fantasized is num_fantasies x batch_shape x q x m.\n",
    "                noise_shape = Y_fantasized.shape[1:]\n",
    "                noise = self.likelihood.noise.mean().expand(noise_shape)\n",
    "                return self.condition_on_observations(\n",
    "                    X=self.transform_inputs(X), Y=Y_fantasized, noise=noise\n",
    "                )\n",
    "\n",
    "\n",
    "\n",
    "class MultiOutputMultiTaskGP(ExactGP, GPyTorchModel):\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_x:torch.Tensor,\n",
    "        train_y:torch.Tensor,\n",
    "        data_kernel,\n",
    "        noise_indices:List[int],\n",
    "        noise:float=1e-4,\n",
    "        fix_noise:bool=False,\n",
    "        lb_noise:float=1e-12,\n",
    "        task_rank:int=None,\n",
    "        output_rank:int=None,\n",
    "    ) -> None:\n",
    "        # check inputs\n",
    "        if not torch.is_tensor(train_x):\n",
    "            raise RuntimeError(\"'train_x' must be a tensor\")\n",
    "        if not torch.is_tensor(train_y):\n",
    "            raise RuntimeError(\"'train_y' must be a tensor\")\n",
    "\n",
    "        if train_x.shape[0] != train_y.shape[0]:\n",
    "            raise RuntimeError(\"Inputs and output have different number of observations\")\n",
    "        \n",
    "        # initializing likelihood\n",
    "        noise_constraint=GreaterThan(lb_noise,transform=torch.exp,inv_transform=torch.log)\n",
    "        \n",
    "        if len(noise_indices) == 0:\n",
    "\n",
    "            likelihood = gpytorch.likelihoods.GaussianLikelihood(noise_constraint=noise_constraint)\n",
    "        else:\n",
    "\n",
    "            likelihood = Multifidelity_likelihood(noise_constraint=noise_constraint, noise_indices=noise_indices, fidel_indices=train_x[:,-1])\n",
    "        y_mean= train_y.min(dim = 0).values\n",
    "        y_std=train_y.max(dim = 0).values-train_y.min(dim = 0).values\n",
    "        train_y_sc = (train_y-y_mean)/y_std\n",
    "\n",
    "        super(MultiOutputMultiTaskGP, self).__init__(train_x, train_y_sc,likelihood)\n",
    "        #ExactGP.__init__(self, train_x,train_y_sc, likelihood)\n",
    "        \n",
    "        # registering mean and std of the raw response\n",
    "        self.register_buffer('y_mean',y_mean)\n",
    "        self.register_buffer('y_std',y_std)\n",
    "        self.register_buffer('y_scaled',train_y_sc)\n",
    "\n",
    "        self._num_outputs = train_y_sc.shape[-1]\n",
    "        self._num_tasks = len(torch.unique(train_x[..., -1]))\n",
    "\n",
    "        self._task_rank = task_rank if task_rank is not None else self._num_tasks\n",
    "        self._output_rank = output_rank if output_rank is not None else self._num_outputs\n",
    "\n",
    "        # initializing and fixing noise\n",
    "        if noise is not None:\n",
    "            self.likelihood.initialize(noise=noise)\n",
    "        \n",
    "        self.likelihood.register_prior('noise_prior',LogHalfHorseshoePrior(0.01,lb_noise),'raw_noise')\n",
    "        if fix_noise:\n",
    "            self.likelihood.raw_noise.requires_grad_(False)\n",
    "            self.likelihood.noise_covar.noise =torch.tensor(4.9901e-05)\n",
    "\n",
    "        \"\"\"\n",
    "        if isinstance(correlation_kernel,str):\n",
    "            try:\n",
    "                correlation_kernel_class = getattr(kernels,correlation_kernel)\n",
    "                correlation_kernel = correlation_kernel_class(\n",
    "                    ard_num_dims = self.train_inputs[0].size(1),\n",
    "                    lengthscale_constraint=Positive(transform=torch.exp,inv_transform=torch.log),\n",
    "                )\n",
    "                correlation_kernel.register_prior(\n",
    "                    'lengthscale_prior',MollifiedUniformPrior(math.log(0.1),math.log(10)),'raw_lengthscale'\n",
    "                )\n",
    "            except:\n",
    "                raise RuntimeError(\n",
    "                    \"%s not an allowed kernel\" % correlation_kernel\n",
    "                )\n",
    "        elif not isinstance(correlation_kernel,gpytorch.kernels.Kernel):\n",
    "            raise RuntimeError(\n",
    "                \"specified correlation kernel is not a `gpytorch.kernels.Kernel` instance\"\n",
    "            )\n",
    "        \n",
    "        self.covar_module = kernels.ScaleKernel(\n",
    "            base_kernel = correlation_kernel,\n",
    "            outputscale_constraint=Positive(transform=softplus,inv_transform=inv_softplus),\n",
    "        )\n",
    "        # register priors\n",
    "        self.covar_module.register_prior(\n",
    "            'outputscale_prior',LogNormalPrior(1e-6,1.),'outputscale'\n",
    "        )\n",
    "        \n",
    "        \"\"\"\n",
    "        self.mean_module = gpytorch.means.MultitaskMean(\n",
    "            gpytorch.means.ConstantMean(), num_tasks=self._num_outputs\n",
    "        )\n",
    "\n",
    "        \n",
    "        if data_kernel == 'Matern':\n",
    "            self.data_kernel = gpytorch.kernels.MaternKernel()\n",
    "        else:\n",
    "            self.data_kernel = gpytorch.kernels.RBFKernel()\n",
    "        self.task_kernel = gpytorch.kernels.IndexKernel(num_tasks=self._num_tasks, rank = self._task_rank) #default rank is 1\n",
    "        self.output_kernel = gpytorch.kernels.IndexKernel(num_tasks=self._num_outputs, rank = self._output_rank) #default rank is 1\n",
    "    \n",
    "    def forward(self,x:torch.Tensor):\n",
    "        mean_x = self.mean_module(x)\n",
    "        task_term = self.task_kernel(x[..., -1].long())\n",
    "        data_and_task_x = self.data_kernel(x[..., :-1]).mul(task_term)\n",
    "        output_x = self.output_kernel.covar_matrix\n",
    "        covar_x = gpytorch.lazy.KroneckerProductLazyTensor(data_and_task_x, output_x)\n",
    "        #covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultitaskMultivariateNormal(mean_x,covar_x)\n",
    "    \n",
    "    def predict(\n",
    "        self,x:torch.Tensor,return_std:bool=False,include_noise:bool=False\n",
    "    )-> Union[torch.Tensor,Tuple[torch.Tensor]]:\n",
    "\n",
    "        self.eval()\n",
    "        with gptsettings.fast_computations(log_prob=False):\n",
    "            # determine if batched or not\n",
    "            ndim = self.train_targets.ndim\n",
    "            if ndim == 1:\n",
    "                output = self(x)\n",
    "            else:\n",
    "                # for batched GPs \n",
    "                num_samples = self.train_targets.shape[0]\n",
    "                output = self(x.unsqueeze(0).repeat(num_samples,1,1))\n",
    "            self.fidel_indices=x[:,-1]\n",
    "            if return_std and include_noise:\n",
    "                # x=self.likelihood(x)\n",
    "                output = self.likelihood(output)\n",
    "\n",
    "            out_mean = self.y_mean + self.y_std*output.mean\n",
    "            \n",
    "            # standard deviation may not always be needed\n",
    "            if return_std:\n",
    "                out_std = output.variance.sqrt()*self.y_std\n",
    "                return out_mean,out_std\n",
    "\n",
    "            return out_mean\n",
    "\n",
    "    def posterior(\n",
    "        self,\n",
    "        X,\n",
    "        output_indices = None,\n",
    "        observation_noise= True,\n",
    "        posterior_transform= None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "\n",
    "        self.eval()\n",
    "        with gpt_posterior_settings() and gptsettings.fast_computations(log_prob=False):\n",
    "    \n",
    "            if observation_noise:\n",
    "                return GPyTorchPosterior(mvn = self.likelihood(self(X.double())))\n",
    "            else:\n",
    "                return GPyTorchPosterior(mvn = self(X.double()))\n",
    "    \n",
    "    def reset_parameters(self) -> None:\n",
    "        \"\"\"Reset parameters by sampling from prior\n",
    "        \"\"\"\n",
    "        for _,module,prior,closure,setting_closure in self.named_priors():\n",
    "            if not closure(module).requires_grad:\n",
    "                continue\n",
    "            setting_closure(module,prior.expand(closure(module).shape).sample().to(**self.tkwargs))\n",
    "\n",
    "\n",
    "    def fantasize(\n",
    "            self,\n",
    "            X: Tensor,\n",
    "            sampler: MCSampler,\n",
    "            observation_noise: Union[bool, Tensor] = True,\n",
    "            **kwargs: Any,\n",
    "        ):\n",
    "            r\"\"\"Constructs a fantasy model using a specified procedure.\n",
    "\n",
    "            This method constructs a fantasy model by following these steps:\n",
    "            1. Compute the model's posterior at `X`. If `observation_noise=True`, the posterior\n",
    "            includes observation noise, which is determined as the mean of the observation\n",
    "            noise in the training data. If `observation_noise` is a Tensor, it is used directly\n",
    "            as the observation noise.\n",
    "            2. Sample from this posterior using the provided `sampler` to create \"fake\" observations.\n",
    "            3. Update (condition) the model with these new fake observations.\n",
    "\n",
    "            Args:\n",
    "                X: A Tensor of dimensions `batch_shape x n' x d`, where `d` represents the feature\n",
    "                space dimension, `n'` is the number of points per batch, and `batch_shape`\n",
    "                is the batch shape. This batch shape must be compatible with the model's\n",
    "                existing batch shape.\n",
    "                sampler: A sampler used for drawing samples from the model's posterior at `X`.\n",
    "                observation_noise: A boolean or a Tensor. If True, the mean of the observation\n",
    "                                noise from the training data is used in the posterior. If a\n",
    "                                Tensor, it specifies the observation noise directly.\n",
    "\n",
    "            Returns:\n",
    "                A fantasy model, updated based on the sampled fake observations.\n",
    "            \"\"\"\n",
    "            propagate_grads = kwargs.pop(\"propagate_grads\", False)\n",
    "            with fantasize_flag():\n",
    "                with settings.propagate_grads(propagate_grads):\n",
    "                    post_X = self.posterior(\n",
    "                        X, observation_noise=observation_noise, **kwargs\n",
    "                    )\n",
    "                Y_fantasized = sampler(post_X)  # num_fantasies x batch_shape x n' x m\n",
    "                # Use the mean of the previous noise values (TODO: be smarter here).\n",
    "                # noise should be batch_shape x q x m when X is batch_shape x q x d, and\n",
    "                # Y_fantasized is num_fantasies x batch_shape x q x m.\n",
    "                noise_shape = Y_fantasized.shape[1:]\n",
    "                noise = self.likelihood.noise.mean().expand(noise_shape)\n",
    "                return self.condition_on_observations(\n",
    "                    X=self.transform_inputs(X), Y=Y_fantasized, noise=noise\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultitaskGaussianLikelihood(\n",
      "  (raw_task_noises_constraint): GreaterThan(1.000E-12)\n",
      "  (raw_noise_constraint): GreaterThan(1.000E-12)\n",
      "  (noise_prior): LogHalfHorseshoePrior(scale: 0.009999999776482582, lb: 9.999999960041972e-13)\n",
      ")\n",
      "torch.Size([16, 6])\n",
      "torch.Size([96, 96])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(-121.9556, grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_train_x = torch.rand(16,6)\n",
    "t_train_y = torch.rand(16,6)\n",
    "mt_model = MultitaskGPModel(t_train_x, t_train_y,noise_indices = [], correlation_kernel = 'Rough_RBF')\n",
    "\n",
    "print(mt_model.likelihood)\n",
    "\n",
    "print(mt_model.mean_module(t_train_x).shape)\n",
    "print(mt_model.covar_module(t_train_x).shape)\n",
    "\n",
    "mt_model_output = mt_model(*mt_model.train_inputs)\n",
    "mt_out = mt_model.likelihood(mt_model_output).log_prob(mt_model.train_targets)\n",
    "mt_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.named_priors of MultitaskGPModel(\n",
       "  (likelihood): MultitaskGaussianLikelihood(\n",
       "    (raw_task_noises_constraint): GreaterThan(1.000E-12)\n",
       "    (raw_noise_constraint): GreaterThan(1.000E-12)\n",
       "    (noise_prior): LogHalfHorseshoePrior(scale: 0.009999999776482582, lb: 9.999999960041972e-13)\n",
       "  )\n",
       "  (mean_module): MultitaskMean(\n",
       "    (base_means): ModuleList(\n",
       "      (0): ConstantMean()\n",
       "      (1): ConstantMean()\n",
       "      (2): ConstantMean()\n",
       "      (3): ConstantMean()\n",
       "      (4): ConstantMean()\n",
       "      (5): ConstantMean()\n",
       "    )\n",
       "  )\n",
       "  (covar_module): MultitaskKernel(\n",
       "    (task_covar_module): IndexKernel(\n",
       "      (raw_var_constraint): Positive()\n",
       "    )\n",
       "    (data_covar_module): Rough_RBF(\n",
       "      (raw_lengthscale_constraint): Positive()\n",
       "      (lengthscale_prior): MollifiedUniformPrior(a: -2.3025851249694824, b: 2.3025851249694824, tail_sigma: 0.10000000149011612)\n",
       "      (distance_module): Distance()\n",
       "    )\n",
       "  )\n",
       ")>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mt_model.named_priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import gpytorch\n",
    "from typing import Dict,List,Optional\n",
    "from gpytorch.constraints import Positive\n",
    "from gpytorch.priors import NormalPrior\n",
    "from gpytorch.distributions import MultivariateNormal, MultitaskMultivariateNormal\n",
    "from gpplus.priors import MollifiedUniformPrior\n",
    "from gpplus.visual.plot_latenth import plot_sep\n",
    "from gpplus.models.gpregression import GPR\n",
    "from gpplus import kernels\n",
    "from gpplus.priors import MollifiedUniformPrior\n",
    "import numpy as np\n",
    "from gpplus.preprocessing import setlevels\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from gpytorch.means import Mean\n",
    "import torch\n",
    "from tabulate import tabulate\n",
    "from gpplus.utils import set_seed\n",
    "from gpplus.optim import fit_model_scipy, noise_tune2, fit_model_torch\n",
    "import torch\n",
    "import numpy as np\n",
    "import sobol_seq\n",
    "import warnings\n",
    "from torch import Tensor\n",
    "from gpytorch.means import Mean\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn import init\n",
    "from torch import nn\n",
    "import torch.nn.functional as F \n",
    "from scipy.stats import norm\n",
    "\n",
    "class GP_Plus(MultitaskGPModel):\n",
    "    \"\"\"The GP_Plus which extends GPs to learn nonlinear and probabilistic nmanifold, handle categorical inputs, and  ... ...\n",
    "\n",
    "    :note: Binary categorical variables should not be treated as qualitative inputs. There is no \n",
    "        benefit from applying a latent variable treatment for such variables. Instead, treat them\n",
    "        as numerical inputs.\n",
    "\n",
    "    :param train_x: The training inputs (size N x d). Qualitative inputs needed to be encoded as \n",
    "        integers 0,...,L-1 where L is the number of levels. For best performance, scale the \n",
    "        numerical variables to the unit hypercube.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_x:torch.Tensor,\n",
    "        train_y:torch.Tensor,\n",
    "        qual_ind_lev = {},\n",
    "        multiple_noise = False,\n",
    "        lv_dim:int=2,\n",
    "        quant_correlation_class:str='Rough_RBF',\n",
    "        noise:float=1e-4,\n",
    "        fix_noise:bool=False,\n",
    "        fixed_length_scale:bool=False,\n",
    "        fixed_omega=torch.tensor([1.0]),\n",
    "        lb_noise:float=1e-8,\n",
    "        NN_layers:list = [],\n",
    "        encoding_type = 'one-hot',\n",
    "        manifold_type='deterministic',\n",
    "        uniform_encoding_columns = 2,\n",
    "        lv_columns = [] ,\n",
    "        base='single_constant',\n",
    "        base_hf='zero',\n",
    "        NN_layers_base=[],\n",
    "        base_function_size=None,\n",
    "        calibration_id=[],\n",
    "        seed_number=1,\n",
    "        mean_prior_cal=0,\n",
    "        std_prior_cal=1,\n",
    "        calibration_type='deterministic',\n",
    "        device=\"cpu\",\n",
    "        dtype= torch.float,\n",
    "        IS=False\n",
    "    ) -> None:\n",
    "        \n",
    "        self.IS=IS\n",
    "        tkwargs = {}  # or dict()\n",
    "        tkwargs['dtype'] = dtype\n",
    "        tkwargs['device'] =torch.device(device)\n",
    "        self.tkwargs=tkwargs\n",
    "\n",
    "\n",
    "        self.mean_prior_cal=mean_prior_cal\n",
    "        self.std_prior_cal=std_prior_cal\n",
    "        if fixed_length_scale:\n",
    "            self.fixed_omega=fixed_omega.to(**self.tkwargs)\n",
    "        else:\n",
    "            self.fixed_omega=None\n",
    "            \n",
    "        ## The checks lists:\n",
    "        if not isinstance(train_x, torch.Tensor):\n",
    "            original_type = type(train_x).__name__\n",
    "            warnings.warn(f\"'train_x' was not a torch.Tensor (type: {original_type}). It is converted to torch.Tensor to proceed with the emulation.\")\n",
    "            train_x = torch.tensor(train_x)\n",
    "\n",
    "        if not isinstance(train_y, torch.Tensor):\n",
    "            original_type = type(train_y).__name__\n",
    "            warnings.warn(f\"'train_y' was not a torch.Tensor (type: {original_type}). It is converted to torch.Tensor to proceed with the emulation.\")\n",
    "            train_y = torch.tensor(train_y)\n",
    "\n",
    "        if not isinstance(qual_ind_lev, dict):\n",
    "            raise ValueError(\"qual_ind_lev should be a dictionary\")\n",
    "\n",
    "        if multiple_noise not in [True, False]:\n",
    "            raise ValueError(\"multiple_noise should be either True or False\")\n",
    "\n",
    "        if not isinstance(lv_dim, int):\n",
    "            raise ValueError(\"lv_dim should be an integer\")\n",
    "\n",
    "        if quant_correlation_class not in ['Rough_RBF', 'RBFKernel', 'Matern32Kernel', 'Matern12Kernel','Matern52Kernel']:\n",
    "            raise ValueError(\"quant_correlation_class should be 'Rough_RBF', 'RBFKernel', 'Matern32Kernel', 'Matern12Kernel','Matern52Kernel'\")\n",
    "\n",
    "        if fix_noise not in [True, False]:\n",
    "            raise ValueError(\"fix_noise should be either True or False\")\n",
    "\n",
    "        if not isinstance(NN_layers, list) or not all(isinstance(i, int) for i in NN_layers):\n",
    "            raise ValueError(\"NN_layers should be a list with integers representing the number of neurons in each layer for mapping the manifold\")\n",
    "\n",
    "        if encoding_type != 'one-hot':\n",
    "            raise ValueError(\"encoding_type should be 'one-hot'\")\n",
    "\n",
    "        if manifold_type not in ['deterministic', 'probabilistic']:\n",
    "            raise ValueError(\"manifold_type should be either 'deterministic' or 'probabilistic'\")\n",
    "\n",
    "        if not isinstance(lv_columns, list) or not all(isinstance(i, int) for i in lv_columns):\n",
    "            raise ValueError(\"lv_columns should be a list with integers showing the number of categorical inputs to be considered in a separate manifold in each layer\")\n",
    "\n",
    "    \n",
    "        supported_singl_functions = ['single_sin', 'single_cos', 'single_exp', 'single_log', 'single_tan', 'single_asin', 'single_acos', 'single_atan', \n",
    "                                    'single_sinh', 'single_cosh', 'single_tanh', 'single_asinh', 'single_acosh', 'single_atanh', 'single_sqrt', \n",
    "                                    'single_abs', 'single_ceil', 'single_floor', 'single_round']\n",
    "\n",
    "        self.supported_singl_base_functions=supported_singl_functions\n",
    "        \n",
    "        supported_multi_base_functions=['single_zero', 'single_polynomial', 'single_constant', 'multiple_polynomial_2d', 'multiple_constant', 'neural_network']\n",
    "\n",
    "        #if base not in supported_multi_base_functions and base not in supported_singl_functions:\n",
    "        #     raise ValueError(\"base is not valied'\")\n",
    "\n",
    "        if not isinstance(NN_layers_base, list) or not all(isinstance(i, int) for i in NN_layers_base):\n",
    "            raise ValueError(\"NN_layers_base should be a list with integers representing the number of neurons in each layer for the mean function\")\n",
    "\n",
    "        if not isinstance(calibration_id, list) or not all(isinstance(i, int) for i in calibration_id):\n",
    "            raise ValueError(\"calibration_id should be a list where each entry shows the column number in the dataset that the calibration parameters are assigned to\")\n",
    "        \n",
    "        train_x=self.fill_nan_with_mean(train_x,calibration_id)\n",
    "        ###############################################################################################\n",
    "        ###############################################################################################\n",
    "        self.seed=seed_number\n",
    "        self.calibration_id=calibration_id\n",
    "        self.calibration_source_index=0    ## It is supposed the calibration parameter is for high fidelity needs\n",
    "        qual_index = list(qual_ind_lev.keys())\n",
    "        all_index = set(range(train_x.shape[-1]))\n",
    "        quant_index = list(all_index.difference(qual_index))\n",
    "        num_levels_per_var = list(qual_ind_lev.values())\n",
    "        #------------------- lm columns --------------------------\n",
    "        lm_columns = list(set(qual_index).difference(lv_columns))\n",
    "        if len(lm_columns) > 0:\n",
    "            qual_kernel_columns = [*lv_columns, lm_columns]\n",
    "        else:\n",
    "            qual_kernel_columns = lv_columns\n",
    "        #########################\n",
    "        if len(qual_index) > 0:\n",
    "            train_x = torch.tensor(setlevels(train_x, qual_index=qual_index))#.to(**self.tkwargs)\n",
    "        #\n",
    "        # train_x=train_x.to(**self.tkwargs)\n",
    "        #train_y=train_y.reshape(-1)#.to(**self.tkwargs)\n",
    "        \n",
    "        if multiple_noise:\n",
    "            noise_indices = list(range(0,num_levels_per_var[-1]))\n",
    "        else:\n",
    "            noise_indices = []\n",
    "\n",
    "        if len(qual_index) == 1 and num_levels_per_var[0] < 2:\n",
    "            temp = quant_index.copy()\n",
    "            temp.append(qual_index[0])\n",
    "            quant_index = temp.copy()\n",
    "            qual_index = []\n",
    "            lv_dim = 0\n",
    "        elif len(qual_index) == 0:\n",
    "            lv_dim = 0\n",
    "\n",
    "        quant_correlation_class_name = quant_correlation_class\n",
    "\n",
    "        if len(qual_index) == 0:\n",
    "            lv_dim = 0\n",
    "        if quant_correlation_class_name == 'Rough_RBF':\n",
    "            quant_correlation_class = 'RBFKernel'\n",
    "\n",
    "        if quant_correlation_class_name == 'Matern32Kernel':\n",
    "            quant_correlation_class = 'Matern32Kernel'\n",
    "        \n",
    "        if quant_correlation_class_name == 'Matern52Kernel':\n",
    "            quant_correlation_class = 'Matern52Kernel'\n",
    "\n",
    "        if quant_correlation_class_name == 'Matern12Kernel':\n",
    "            quant_correlation_class = 'Matern12Kernel'\n",
    "\n",
    "        if len(qual_index) > 0:\n",
    "            ####################### Defined multiple kernels for seperate variables ###################\n",
    "            qual_kernels = []\n",
    "            for i in range(len(qual_kernel_columns)):\n",
    "                qual_kernels.append(kernels.RBFKernel(\n",
    "                    active_dims=torch.arange(lv_dim) + lv_dim * i) )\n",
    "                qual_kernels[i].initialize(**{'lengthscale':1.0})\n",
    "                qual_kernels[i].raw_lengthscale.requires_grad_(False)\n",
    "                \n",
    "\n",
    "        if len(quant_index) == 0:\n",
    "            correlation_kernel = qual_kernels[0]\n",
    "            for i in range(1, len(qual_kernels)):\n",
    "                correlation_kernel *= qual_kernels[i]\n",
    "        else:\n",
    "            try:\n",
    "                quant_correlation_class = getattr(kernels,quant_correlation_class)\n",
    "            except:\n",
    "                raise RuntimeError(\n",
    "                    \"%s not an allowed kernel\" % quant_correlation_class\n",
    "                )\n",
    "            if quant_correlation_class_name == 'RBFKernel':\n",
    "                quant_kernel = quant_correlation_class(\n",
    "                    ard_num_dims=len(quant_index),\n",
    "                    active_dims=len(qual_kernel_columns) * lv_dim+torch.arange(len(quant_index)),\n",
    "                    lengthscale_constraint= Positive(transform= torch.exp,inv_transform= torch.log)\n",
    "                )\n",
    "            elif quant_correlation_class_name == 'Rough_RBF':\n",
    "                quant_kernel = quant_correlation_class(\n",
    "                    ard_num_dims=len(quant_index),\n",
    "                    active_dims=len(qual_kernel_columns)*lv_dim+torch.arange(len(quant_index)),\n",
    "                    lengthscale_constraint= Positive(transform= lambda x: 2.0**(-0.5) * torch.pow(10,-x/2),inv_transform= lambda x: -2.0*torch.log10(x/2.0))\n",
    "                )\n",
    "            elif quant_correlation_class_name == 'Matern12Kernel':\n",
    "                quant_kernel = quant_correlation_class(\n",
    "                    ard_num_dims=len(quant_index),\n",
    "                    active_dims=len(qual_kernel_columns)*lv_dim+torch.arange(len(quant_index)),\n",
    "                    lengthscale_constraint= Positive(transform= lambda x: 2.0**(-0.5) * torch.pow(10,-x/2),inv_transform= lambda x: -2.0*torch.log10(x/2.0))\n",
    "                )\n",
    "            \n",
    "            elif quant_correlation_class_name == 'Matern32Kernel':\n",
    "                quant_kernel = quant_correlation_class(\n",
    "                    ard_num_dims=len(quant_index),\n",
    "                    active_dims=len(qual_kernel_columns)*lv_dim+torch.arange(len(quant_index)),\n",
    "                    #lengthscale_constraint= Positive(transform= torch.exp,inv_transform= torch.log)\n",
    "                    lengthscale_constraint= Positive(transform= lambda x: 2.0**(-0.5) * torch.pow(10,-x/2),inv_transform= lambda x: -2.0*torch.log10(x/2.0))             \n",
    "                )\n",
    "            elif quant_correlation_class_name == 'Matern52Kernel':\n",
    "                quant_kernel = quant_correlation_class(\n",
    "                    ard_num_dims=len(quant_index),\n",
    "                    active_dims=len(qual_kernel_columns)*lv_dim+torch.arange(len(quant_index)),\n",
    "                    #lengthscale_constraint= Positive(transform= torch.exp,inv_transform= torch.log)  \n",
    "                    lengthscale_constraint= Positive(transform= lambda x: 2.0**(-0.5) * torch.pow(10,-x/2),inv_transform= lambda x: -2.0*torch.log10(x/2.0))       \n",
    "                )\n",
    "                #####################\n",
    "            if quant_correlation_class_name == 'RBFKernel':\n",
    "                quant_kernel.register_prior(\n",
    "                    'lengthscale_prior', MollifiedUniformPrior(math.log(0.1),math.log(10)),'raw_lengthscale'\n",
    "                )\n",
    "                \n",
    "            elif quant_correlation_class_name == 'Rough_RBF':\n",
    "                quant_kernel.register_prior(\n",
    "                    'lengthscale_prior',NormalPrior(-3.0,3.0),'raw_lengthscale'\n",
    "                )\n",
    "            elif quant_correlation_class_name == 'Matern12Kernel':\n",
    "                quant_kernel.register_prior(\n",
    "                    #'lengthscale_prior', MollifiedUniformPrior(math.log(0.1),math.log(10)),'raw_lengthscale'\n",
    "                    'lengthscale_prior',NormalPrior(-3.0,3.0),'raw_lengthscale'\n",
    "                )\n",
    "\n",
    "            elif quant_correlation_class_name == 'Matern32Kernel':\n",
    "                quant_kernel.register_prior(\n",
    "                    #'lengthscale_prior', MollifiedUniformPrior(math.log(0.1),math.log(10)),'raw_lengthscale'\n",
    "                    'lengthscale_prior',NormalPrior(-3.0,3.0),'raw_lengthscale'\n",
    "                )\n",
    "\n",
    "            elif quant_correlation_class_name == 'Matern52Kernel':\n",
    "                quant_kernel.register_prior(\n",
    "                    #'lengthscale_prior', MollifiedUniformPrior(math.log(0.1),math.log(10)),'raw_lengthscale'\n",
    "                    'lengthscale_prior',NormalPrior(-3.0,3.0),'raw_lengthscale'\n",
    "                )\n",
    "            if len(qual_index) > 0:\n",
    "                temp = qual_kernels[0]\n",
    "                for i in range(1, len(qual_kernels)):\n",
    "                    temp *= qual_kernels[i]\n",
    "                correlation_kernel = temp*quant_kernel #+ qual_kernel + quant_kernel\n",
    "            else:\n",
    "                correlation_kernel = quant_kernel\n",
    "\n",
    "        super(GP_Plus,self).__init__(\n",
    "            train_x=train_x,train_y=train_y,noise_indices=noise_indices,\n",
    "            correlation_kernel=correlation_kernel,\n",
    "            noise=noise,fix_noise=fix_noise,lb_noise=lb_noise\n",
    "        )\n",
    "\n",
    "        self.calibration_type=calibration_type\n",
    "        for n in self.calibration_id:\n",
    "            if self.calibration_type=='probabilistic':\n",
    "                setattr(self,'Theta_'+str(n), LinearVariational(batch_shape=torch.Size([]),mean_prior=self.mean_prior_cal,std_prior=0*self.std_prior_cal).to(**tkwargs)) \n",
    "                setattr(self,'calibration_element'+str(n), torch.where(train_x[:, -1]==self.calibration_source_index)[0]) \n",
    "            else:\n",
    "                setattr(self,'Theta_'+str(n), gpytorch.means.ConstantMean(prior=NormalPrior(self.mean_prior_cal,self.std_prior_cal))) \n",
    "                setattr(self,'calibration_element'+str(n), torch.where(train_x[:, -1]==self.calibration_source_index)[0]) \n",
    "            train_x[getattr(self,'calibration_element'+str(n)),n]=torch.zeros_like(train_x[getattr(self,'calibration_element'+str(n)),n])\n",
    "        \n",
    "        # register index and transforms\n",
    "        self.register_buffer('quant_index',torch.tensor(quant_index))\n",
    "        self.register_buffer('qual_index',torch.tensor(qual_index))\n",
    "\n",
    "        self.qual_kernel_columns = qual_kernel_columns\n",
    "        # latent variable mapping\n",
    "        self.num_levels_per_var = num_levels_per_var\n",
    "        self.lv_dim = lv_dim\n",
    "        self.uniform_encoding_columns = uniform_encoding_columns\n",
    "        self.encoding_type = encoding_type\n",
    "        self.manifold_type=manifold_type\n",
    "        self.perm =[]\n",
    "        self.zeta = []\n",
    "        self.random_zeta=[]\n",
    "        self.perm_dict = []\n",
    "        self.A_matrix = []\n",
    "        self.epsilon=None\n",
    "        self.epsilon_f=None\n",
    "        self.embeddings_Dtrain=[]\n",
    "        self.count=train_x.size()[0]\n",
    "        if len(qual_kernel_columns) > 0:\n",
    "            for i in range(len(qual_kernel_columns)):\n",
    "                if type(qual_kernel_columns[i]) == int:\n",
    "                    num = self.num_levels_per_var[qual_index.index(qual_kernel_columns[i])]\n",
    "                    cat = [num]\n",
    "                else:\n",
    "                    cat = [self.num_levels_per_var[qual_index.index(k)] for k in qual_kernel_columns[i]]\n",
    "                    num = sum(cat)\n",
    "\n",
    "                zeta, perm, perm_dict = self.zeta_matrix(num_levels=cat, lv_dim = self.lv_dim)\n",
    "                self.zeta.append(zeta)\n",
    "                self.perm.append(perm)\n",
    "                self.perm_dict.append(perm_dict)       \n",
    "                ###################################  latent map (manifold) #################################   \n",
    "                if self.manifold_type=='probabilistic':\n",
    "                    setattr(self,'A_matrix', Variational_Encoder(self, input_size= num, num_classes=5, \n",
    "                        layers =NN_layers, name = str(qual_kernel_columns[i])).to(**tkwargs))\n",
    "                else:\n",
    "                    model_temp = FFNN(self, input_size= num, num_classes=lv_dim, \n",
    "                        layers = NN_layers, name ='latent'+ str(qual_kernel_columns[i])).to(**self.tkwargs)\n",
    "                    self.A_matrix.append(model_temp)\n",
    "\n",
    "        ##################################################################################\n",
    "        if fixed_length_scale == True:\n",
    "            self.covar_module.base_kernel.raw_lengthscale.data = self.fixed_omega #torch.tensor([self.omega, self.omega])  # Set the desired value\n",
    "            self.covar_module.base_kernel.raw_lengthscale.requires_grad = False  # Fix the hyperparameter\n",
    "        ###################################  Mean Function #################################   \n",
    "        i=0\n",
    "        self.base=base\n",
    "        self.base_hf=base_hf\n",
    "        self.num_sources=int(torch.max(train_x[:,-1]))\n",
    "        size=train_x.shape[1]\n",
    "        if self.base.startswith('single'):\n",
    "            self.single_base_register(size,base_type=self.base,wm='mean_module')\n",
    "        elif self.base.startswith('multi'):\n",
    "            self.multi_base_register(train_x,supported_multi_base_functions,self.base_hf)\n",
    "        elif self.base=='neural_network': ###### One NN for ALL \n",
    "            setattr(self,'mean_module_NN_All', FFNN_as_Mean(self, input_size= train_x.shape[1]+2*len(qual_index)-len(qual_index), num_classes=1,layers =NN_layers_base, name = str('mean_module_'+str(i)+'_')).to(**tkwargs)) \n",
    "        else: \n",
    "            raise ValueError('The \"base\" argument must start with \"multi\", \"single\", or \"neural_network\".')\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self,x:torch.Tensor):\n",
    "        if self.manifold_type=='probabilistic' or self.calibration_type=='probabilistic':\n",
    "            set_seed(self.seed)\n",
    "            if self.training:\n",
    "                Numper_of_pass=5 #20\n",
    "            else:\n",
    "                Numper_of_pass=10 #30\n",
    "        else:\n",
    "            Numper_of_pass=1\n",
    "        \n",
    "        size_sigma_sum = x.size(0)*self._num_tasks\n",
    "        Sigma_sum=torch.zeros(size_sigma_sum,size_sigma_sum, dtype=torch.float64).to(self.tkwargs['device'])\n",
    "        #mean_x_sum=torch.zeros(x.size(0), dtype=torch.float64).to(self.tkwargs['device'])\n",
    "        mean_x_sum=torch.zeros(x.size(0),self._num_tasks, dtype=torch.float64).to(self.tkwargs['device'])\n",
    "        #print('mean_x_sum.shape',mean_x_sum.shape)\n",
    "\n",
    "        for NP in range(Numper_of_pass):\n",
    "            x_forward_raw=x.clone()\n",
    "            nd_flag = 0\n",
    "            if x.dim() > 2:\n",
    "                xsize = x.shape\n",
    "                x = x.reshape(-1, x.shape[-1])\n",
    "                nd_flag = 1\n",
    "            \n",
    "            x_new= x\n",
    "            if len(self.qual_kernel_columns) > 0:\n",
    "                embeddings = []\n",
    "                for i in range(len(self.qual_kernel_columns)):\n",
    "                    temp= self.transform_categorical(x=x[:,self.qual_kernel_columns[i]].clone().type(torch.int64).to(self.tkwargs['device']), \n",
    "                        perm_dict = self.perm_dict[i], zeta = self.zeta[i])\n",
    "                dimm=x_forward_raw.size()[0]\n",
    "                if self.manifold_type=='probabilistic': \n",
    "                    # Convert to list of tuples\n",
    "                    x_raw=torch.zeros(temp.size(0),2)\n",
    "                    # Find unique rows\n",
    "                    unique_rows, indices = torch.unique(temp, dim=0, return_inverse=True)\n",
    "                    temp= unique_rows\n",
    "                    dimm=unique_rows.size()[0]\n",
    "                    if self.training:\n",
    "                        epsilon=torch.normal(mean=0,std=1,size=[dimm,2])## use np instead of torch \n",
    "                        embeddings.append(getattr(self,'A_matrix')(x=temp.float().to(**self.tkwargs),epsilon=epsilon))\n",
    "                    else:\n",
    "                        if x.size()[0]==self.count:\n",
    "                            epsilon=torch.normal(mean=0,std=1,size=[dimm,2])\n",
    "                            embeddings.append(getattr(self,'A_matrix')(x=temp.float().to(**self.tkwargs),epsilon=epsilon))\n",
    "                            self.embeddings_Dtrain.append(embeddings[0])\n",
    "                        else:\n",
    "                            embeddings.append(self.embeddings_Dtrain[NP])\n",
    "                    for i, index in enumerate(indices):\n",
    "                        x_raw[i] = embeddings[0][index]\n",
    "                    embeddings=x_raw\n",
    "                    x_new= torch.cat([embeddings,x[...,self.quant_index.long()]],dim=-1)\n",
    "                else:\n",
    "                    embeddings.append(self.A_matrix[i](temp.float().to(**self.tkwargs)))\n",
    "                    x_new= torch.cat([embeddings[0],x[...,self.quant_index.long()].to(**self.tkwargs)],dim=-1)\n",
    "                #print('x_new.shape',x_new.shape)\n",
    "                ## For Calibration\n",
    "                if len(self.calibration_id)>0:\n",
    "                    if self.training:\n",
    "                        for n in self.calibration_id:\n",
    "                            if self.calibration_type=='probabilistic':\n",
    "                                s=torch.ones_like(x_new[getattr(self,'calibration_element'+str(n)),embeddings[0].size(1)+n]).shape\n",
    "                                epsilon=torch.normal(mean=0,std=1,size=[s[0],1])\n",
    "                                Theta=(getattr(self,'Theta_'+str(n))(epsilon.clone().reshape(-1,1)))\n",
    "                                x_new[getattr(self,'calibration_element'+str(n)),embeddings[0].size(1)+n]=\\\n",
    "                                    torch.ones_like(x_new[getattr(self,'calibration_element'+str(n)),embeddings[0].size(1)+n])*(Theta.reshape(-1))\n",
    "                                x_new[getattr(self,'calibration_element'+str(n)),embeddings[0].size(1)+n]=\\\n",
    "                                    torch.ones_like(x_new[getattr(self,'calibration_element'+str(n)),embeddings[0].size(1)+n])*(getattr(self,'Theta_'+str(n))(x[i,-1].clone().flatten().reshape(-1,1)))\n",
    "                    else:\n",
    "                        for n in self.calibration_id:\n",
    "                            if self.calibration_type=='probabilistic':\n",
    "                                epsilon=torch.normal(mean=0,std=1,size=[1,1])\n",
    "                                calibration_element=torch.where(x[:, -1]==self.calibration_source_index)[0]\n",
    "                                x_new[calibration_element,embeddings[0].size(1)+n]=\\\n",
    "                                        torch.ones_like(x_new[calibration_element,embeddings[0].size(1)+n])*(getattr(self,'Theta_'+str(n))(epsilon.clone().reshape(-1,1)))\n",
    "                            else:\n",
    "                                calibration_element=torch.where(x[:, -1]==self.calibration_source_index)[0]\n",
    "                                x_new[calibration_element,embeddings[0].size(1)+n]=\\\n",
    "                                    torch.ones_like(x_new[calibration_element,embeddings[0].size(1)+n])*(getattr(self,'Theta_'+str(n))(x[i,-1].clone().reshape(-1,1)))               \n",
    "            if nd_flag == 1:\n",
    "                x_new = x_new.reshape(*xsize[:-1], -1)\n",
    "            \n",
    "        #################### Multiple baises (General Case) ####################################  \n",
    "            if self.base.startswith('multi'):\n",
    "                mean_x = self.multi_mean(x_new,x_forward_raw).to(**self.tkwargs) \n",
    "            elif self.base.startswith('neural_network'):\n",
    "                mean_x = getattr(self, 'mean_module_NN_All')(x_new.clone().detach()).reshape(-1)\n",
    "            else:\n",
    "                #mean_x = self.single_mean(x_new).to(**self.tkwargs)\n",
    "                mean_x = self.mean_module(x_new).to(**self.tkwargs)\n",
    "            #print('mean_x.shape',mean_x.shape)\n",
    "            covar_x = self.covar_module(x_new).to(**self.tkwargs)\n",
    "            #print('covar_x.shape',covar_x.evaluate().shape)\n",
    "            mean_x_sum+=mean_x\n",
    "            #Sigma_sum += covar_x.evaluate()+ torch.outer(mean_x, mean_x)\n",
    "            Sigma_sum += covar_x.evaluate()\n",
    "\n",
    "        # End of the loop for forward pasess ----> Compute ensemble mean and covariance\n",
    "        k = Numper_of_pass\n",
    "        ensemble_mean = mean_x_sum/k\n",
    "        ensemble_covar = torch.zeros_like(Sigma_sum) \n",
    "        ensemble_covar= Sigma_sum/k\n",
    "        #ensemble_covar -= torch.outer(ensemble_mean, ensemble_mean)\n",
    "        ensemble_covar=gpytorch.lazy.NonLazyTensor(ensemble_covar)\n",
    "        Sigma_sum=0\n",
    "        #print('ensemble_mean.shape',ensemble_mean.shape)\n",
    "        #print('ensemble_covar.shape',ensemble_covar.evaluate().shape)\n",
    "        return gpytorch.distributions.MultitaskMultivariateNormal(ensemble_mean,ensemble_covar)\n",
    "    \n",
    "    ################################################################ Mean Functions #####################################################################################\n",
    "    \n",
    "    def single_base_register(self,size=1,base_type='single_zero',wm='mean_module'):\n",
    "        if base_type in self.supported_singl_base_functions:\n",
    "            setattr(self,wm, LinearMean_with_prior(input_size=size, batch_shape=torch.Size([]), bias=False)) \n",
    "        elif base_type.startswith('single_polynomial'):\n",
    "            degree = int(base_type.split('d')[-1])\n",
    "            setattr(self,wm, LinearMean_with_prior(input_size=degree*(size), batch_shape=torch.Size([]), bias=True)) \n",
    "        elif base_type=='single_constant':\n",
    "            #setattr(self,wm, gpytorch.means.ConstantMean(prior=NormalPrior(0.,1)) )\n",
    "            setattr(self,wm, gpytorch.means.MultitaskMean(\n",
    "            gpytorch.means.ConstantMean(), num_tasks=self._num_tasks\n",
    "        ) )\n",
    "        elif base_type=='single_zero':\n",
    "            setattr(self,wm, gpytorch.means.ZeroMean())  \n",
    "    \n",
    "    def multi_base_register(self,train_x,supported_multi_base_functions,base_hf):\n",
    "        size=train_x.shape[1]\n",
    "        if self.base in supported_multi_base_functions:\n",
    "            for i in range(self.num_sources +1):\n",
    "                if i==0:\n",
    "                    base_type= 'single_'+base_hf\n",
    "                else:\n",
    "                    base_type='single'+self.base[8:]\n",
    "                self.single_base_register(size,base_type=base_type,wm='mean_module_'+str(i))\n",
    "\n",
    "    def single_mean(self, x):\n",
    "        base_type=self.base\n",
    "        supported_functions = ['sin', 'cos', 'exp', 'log', 'tan', 'asin', 'acos', 'atan', \n",
    "                               'sinh', 'cosh', 'tanh', 'asinh', 'acosh', 'atanh', 'sqrt', \n",
    "                               'abs', 'ceil', 'floor', 'round']\n",
    "        \n",
    "        if base_type in supported_functions:\n",
    "            # Dynamically call the PyTorch function based on base_type\n",
    "            transformed_x = getattr(torch, base_type)(x.clone()).float()\n",
    "        elif base_type.startswith('polynomial-d'):\n",
    "            degree = int(base_type.split('d')[-1])\n",
    "            transformed_x = x.clone().double()\n",
    "            polynomial_terms = [transformed_x.pow(n).float() for n in range(1, degree + 1)]\n",
    "            transformed_x = torch.cat(polynomial_terms, dim=1)\n",
    "        else:\n",
    "            # Default case\n",
    "            transformed_x = x.float().clone()\n",
    "        mean_x = getattr(self, 'mean_module')(transformed_x)\n",
    "        return mean_x\n",
    "\n",
    "    def multi_mean(self,x,x_forward_raw):\n",
    "        mean_x=torch.zeros_like(x[:,-1])\n",
    "        if self.base=='multiple_constant':\n",
    "            for i in range(len(mean_x)):\n",
    "                qq=int(x_forward_raw[i,-1])                        \n",
    "                mean_x[i] = getattr(self, 'mean_module_' + str(qq))(x_forward_raw[i,:].clone().float().reshape(1,-1))\n",
    "        elif self.base=='multiple_polynomial':\n",
    "            for i in range(len(mean_x)):\n",
    "                qq=int(x_forward_raw[i,-1])\n",
    "                # mean_x[i]=getattr(self,'mean_module_'+str(qq))(torch.cat((torch.tensor((x[i,-1].clone().double().reshape(-1,1).float())**2),torch.tensor(x[i,-1].clone().double()).reshape(-1,1).float()),1))\n",
    "                mean_x[i] = getattr(self, 'mean_module_' + str(qq))(torch.cat((x_forward_raw.clone().detach().double().reshape(-1, 1).float() ** 2,\n",
    "                        x[i, -1].clone().detach().double().reshape(-1, 1).float()),1))\n",
    "        \n",
    "        elif self.base=='neural_network':\n",
    "            mean_x = getattr(self, 'mean_module_NN_All')(x.clone()).reshape(-1)\n",
    "        return mean_x \n",
    "\n",
    "    ################################################################ Fit #####################################################################################\n",
    "    def fit(self,add_prior:bool=True,num_restarts:int=64,theta0_list:Optional[List[np.ndarray]]=None,jac:bool=True,\n",
    "            options:Dict={},n_jobs:int=-1,method = 'L-BFGS-B',constraint=False,bounds=False,regularization_parameter:List[int]=[0,0],optim_type='scipy'):\n",
    "        print(\"## Learning the model's parameters has started ##\")\n",
    "\n",
    "        if self.tkwargs['device'].type == 'cuda':\n",
    "            if optim_type == 'adam_torch':\n",
    "                fit_model_torch(model=self,\n",
    "                                model_param_groups=None,\n",
    "                                lr_default=0.01,\n",
    "                                num_iter=100,\n",
    "                                num_restarts=64,\n",
    "                                break_steps=50)\n",
    "            else:\n",
    "                # Issue a warning and proceed with the 'adam_torch' optimizer\n",
    "                warnings.warn('The model is built to run on CUDA (GPU), but the current optimization type is invalid for this configuration. So, the optimizer is now using adam_torch to train the model.')\n",
    "                fit_model_torch(model=self.to(**self.tkwargs),\n",
    "                                model_param_groups=None,\n",
    "                                lr_default=0.01,\n",
    "                                num_iter=100,\n",
    "                                num_restarts=4,\n",
    "                                break_steps=50)\n",
    "        else:\n",
    "            if optim_type=='scipy':\n",
    "                n_jobs = 1\n",
    "                fit_model_scipy(self,add_prior,num_restarts,theta0_list,jac, options,n_jobs,method ,constraint,bounds,regularization_parameter)\n",
    "            elif optim_type=='continuation':\n",
    "                noise_tune2(model=self,add_prior=add_prior,\n",
    "                num_restarts=num_restarts,criterion='NLL',\n",
    "                initial_noise_var=1,\n",
    "                red_factor=math.sqrt(10),\n",
    "                options=options,\n",
    "                n_jobs= n_jobs,\n",
    "                accuracy = 1e-2,\n",
    "                method = method,\n",
    "                constraint=constraint,\n",
    "                regularization_parameter=regularization_parameter,\n",
    "                bounds=bounds\n",
    "                )\n",
    "            elif optim_type=='adam_torch':\n",
    "                fit_model_torch (model=self,\n",
    "                    model_param_groups=None,\n",
    "                    lr_default=0.01,\n",
    "                    num_iter=100,\n",
    "                    num_restarts=num_restarts,\n",
    "                    break_steps= 50)\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    'Invalid optim_type. You must choose one of the following: '\n",
    "                    '\"scipy\" (default), \"continuation\", or \"adam_torch\".\\n'\n",
    "                    '- \"scipy\": Uses the SciPy library for optimization, suitable for most CPU-based computations.\\n'\n",
    "                    '- \"continuation\": A method designed for more complex optimization scenarios, potentially offering better results in most cases with higher computational cost.\\n'\n",
    "                    '- \"adam_torch\": Employs the Adam optimizer from the PyTorch library, optimized for GPU-based computations and large datasets.'\n",
    "                )\n",
    "        print(\"## Learning the model's parameters is successfully finished ##\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def fill_nan_with_mean(self,train_x,cal_ID):\n",
    "        # Check if there are any NaNs in the tensor\n",
    "        if torch.isnan(train_x).any():\n",
    "            if len(cal_ID)==0:\n",
    "                print(\"There are NaN values in the data, which will be filled with column-wise mean values.\")\n",
    "            else:\n",
    "                print(\"There are NaN values in the data, which will be estimated in calibration process\")\n",
    "            # Compute the mean of non-NaN elements column-wise\n",
    "            col_means = torch.nanmean(train_x, dim=0)\n",
    "            # Find indices where NaNs are located\n",
    "            nan_indices = torch.isnan(train_x)\n",
    "            # Replace NaNs with the corresponding column-wise mean\n",
    "            train_x[nan_indices] = col_means.repeat(train_x.shape[0], 1)[nan_indices]\n",
    "\n",
    "        return train_x\n",
    "    ############################  Prediction and Visualization  ###############################\n",
    "    \n",
    "    def predict(self, Xtest,return_std=True, include_noise = True):\n",
    "        with torch.no_grad():\n",
    "            return super().predict(Xtest, return_std = return_std, include_noise= include_noise)\n",
    "    \n",
    "    def predict_with_grad(self, Xtest,return_std=True, include_noise = True):\n",
    "        return super().predict(Xtest, return_std = return_std, include_noise= include_noise)\n",
    "    \n",
    "    def noise_value(self):\n",
    "        noise = self.likelihood.noise_covar.noise.detach() * self.y_std**2\n",
    "        return noise\n",
    "\n",
    "    def score(self, Xtest, ytest, plot_MSE = True, title = None, seperate_levels = False):\n",
    "        ytest=ytest.reshape(-1).to(self.tkwargs['device'])\n",
    "        Xtest=Xtest.to(self.tkwargs['device'])\n",
    "        plt.rcParams.update({'font.size': 14})\n",
    "        ypred = self.predict(Xtest, return_std=False)\n",
    "        mse = ((ytest.reshape(-1)-ypred)**2).mean()\n",
    "        print('################MSE######################')\n",
    "        print(f'MSE = {mse:.5f}')\n",
    "        print('#########################################')\n",
    "        print('################Noise####################')\n",
    "        noise = self.likelihood.noise_covar.noise.detach() * self.y_std**2\n",
    "        \n",
    "        print(f'The estimated noise parameter (varaince) is {noise}')\n",
    "        print(f'The estimated noise std is {np.sqrt(noise.cpu())}')\n",
    "        print('#########################################')\n",
    "\n",
    "        if plot_MSE:\n",
    "            _ = plt.figure(figsize=(8,6))\n",
    "            _ = plt.plot(ytest.cpu().numpy(), ypred.cpu().numpy(), 'ro', label = 'Data')\n",
    "            _ = plt.plot(ytest.cpu().numpy(), ytest.cpu().numpy(), 'b', label = 'MSE = ' + str(np.round(mse.detach().item(),3)))\n",
    "            _ = plt.xlabel(r'Y_True')\n",
    "            _ = plt.ylabel(r'Y_predict')\n",
    "            _ = plt.legend()\n",
    "            if title is not None:\n",
    "                _ = plt.title(title)\n",
    "\n",
    "        if seperate_levels and len(self.qual_index) > 0:\n",
    "            for i in range(self.num_levels_per_var[0]):\n",
    "                index = torch.where(Xtest[:,self.qual_index] == i)[0]\n",
    "                _ = self.score(Xtest[index,...], ytest[index], \n",
    "                    plot_MSE=True, title = 'results' + ' Only Source ' + str(i), seperate_levels=False)\n",
    "        return ypred\n",
    "\n",
    "    def plot_xy(self, Xtest, ytest, input_column):\n",
    "        ytest=ytest.reshape(-1).to(**self.tkwargs)\n",
    "        Xtest=Xtest.to(**self.tkwargs)\n",
    "        if len(input_column) > 2:\n",
    "            raise ValueError(\"Visualization can only be done for one or two input versions.\")\n",
    "\n",
    "        plt.rcParams.update({'font.size': 14})\n",
    "        ypred = self.predict(Xtest, return_std=False)\n",
    "        mse = ((ytest.reshape(-1) - ypred) ** 2).mean()\n",
    "\n",
    "        if len(input_column) == 1:\n",
    "            # 2D Plotting for one input\n",
    "            plt.scatter(Xtest[input_column[0]].cpu().numpy(), ytest.cpu().numpy(), '*r', label='Ground Truth')\n",
    "            plt.scatter(Xtest[input_column[0]].cpu().numpy(), ypred.cpu().numpy(), 'ob', label='Prediction')\n",
    "            plt.xlabel('Input: ' + str(input_column[0]))\n",
    "            plt.ylabel('Output')\n",
    "\n",
    "        elif len(input_column) == 2:\n",
    "            # 3D Plotting for two inputs\n",
    "            fig = plt.figure()\n",
    "            ax = fig.add_subplot(111, projection='3d')\n",
    "            ax.scatter(Xtest[input_column[0]].cpu().numpy(), Xtest[input_column[1]].cpu().numpy(), ytest.cpu().numpy(), c='r', marker='o', label='Ground Truth')\n",
    "            ax.scatter(Xtest[input_column[0]].cpu().numpy(), Xtest[input_column[1]].cpu().numpy(), ypred.cpu().numpy(), c='b', marker='^', label='Prediction')\n",
    "            ax.set_xlabel('Input: ' + str(input_column[0]))\n",
    "            ax.set_ylabel('Input: ' + str(input_column[1]))\n",
    "            ax.set_zlabel('Output')\n",
    "\n",
    "        plt.title('Input(s): ' + str(input_column) + ' versus Output')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    def plot_xy_print_params(self, Xtest, ytest, Xtrain, ytrain, model):\n",
    "            ytest=ytest.reshape(-1).to(self.tkwargs['device'])\n",
    "            Xtest=Xtest.to(self.tkwargs['device'])\n",
    "            Xtest, indices = torch.sort(Xtest, dim = 0)\n",
    "            ytest = ytest[indices]\n",
    "\n",
    "            mean_pred, std_pred = model.predict(Xtest.to(**self.tkwargs), return_std=True)\n",
    "            confidence_interval = 1.96 * std_pred\n",
    "\n",
    "            plt.rcParams.update({'font.size': 14})\n",
    "            plt.scatter(Xtrain, ytrain, marker='x', color='red')\n",
    "            plt.plot(Xtest, ytest,color='black', linewidth=4.0, label='Exact')\n",
    "            plt.plot(Xtest, mean_pred.cpu(), color='green', linestyle='dashed', linewidth=4.0, label = 'Predicted')\n",
    "            plt.fill_between(Xtest.squeeze(), mean_pred.cpu() - confidence_interval.cpu(), mean_pred.cpu() + confidence_interval.cpu(), color='lightblue', alpha=0.7, label='95% CI')\n",
    "            plt.scatter(Xtrain, ytrain, s=100, marker='x', color='red', label='Training data')\n",
    "            plt.xlabel(r'$x$')\n",
    "            plt.ylabel(r'$y$')\n",
    "            title = r\"$\\hat{\\beta}$ = \" + f\"{model.mean_module.constant.item():.3f}\" + r\", $\\hat{\\omega}$ = \" + f\"{model.covar_module.base_kernel.raw_lengthscale.data.item():.3e}\"+ r\", $\\hat{\\delta}$ = \" + f\"{model.noise_value().item():.3e}\"\n",
    "            plt.title(title, fontsize = 15, loc=\"center\")\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "            \n",
    "    def evaluation_2(self,Xtest,ytest,n_FP=1):\n",
    "        ytest=ytest.reshape(-1).to(self.tkwargs['device'])\n",
    "        Xtest=Xtest.to(self.tkwargs['device'])\n",
    "        self.eval()\n",
    "        likelihood=self.likelihood\n",
    "        likelihood.fidel_indices=self.train_inputs[0][:,-1]\n",
    "        output=self(Xtest)\n",
    "        likelihood.fidel_indices=Xtest[:,-1]\n",
    "        ytest_sc = (ytest-self.y_mean)/self.y_std\n",
    "        mean_temp=[]\n",
    "        var_temp=[]\n",
    "        for i in range (n_FP):\n",
    "            with torch.no_grad():\n",
    "                trained_pred_dist = likelihood(output)\n",
    "                mean_temp.append(trained_pred_dist.mean)\n",
    "                var_temp.append(trained_pred_dist.variance)\n",
    "            \n",
    "        sum_list = [mean**2 + var for mean, var in zip(mean_temp, var_temp)]\n",
    "        sum_tensors = sum(sum_list)/n_FP\n",
    "        mean_ensamble=sum(mean_temp)/n_FP\n",
    "        var_ensamble=sum_tensors -mean_ensamble**2\n",
    "        std_ensamble=var_ensamble.sqrt()\n",
    "        mu_low, mu_up=mean_ensamble-1.96*std_ensamble, mean_ensamble+1.96*std_ensamble\n",
    "        final_mse=((ytest_sc.reshape(-1)-mean_ensamble)**2).mean()\n",
    "        def interval_score(y_true,mu_low, mu_up, alpha = 0.05):\n",
    "            out = mu_up - mu_low\n",
    "            out += (y_true > mu_up)* 2/alpha * (y_true - mu_up)\n",
    "            out += (y_true <mu_low)* 2/alpha * (mu_low - y_true)\n",
    "            return out\n",
    "        IS=interval_score(ytest_sc,mu_low, mu_up, alpha = 0.05).mean()\n",
    "        NIS=IS*torch.abs(self.y_std)/ytest.std()\n",
    "        NRMSE=torch.sqrt((final_mse*(self.y_std)**2)/ytest.std()**2)    \n",
    "        table_data = [\n",
    "        ['NRMSE', NRMSE],\n",
    "        ['NIS', NIS],\n",
    "        ]\n",
    "        # Print the table\n",
    "        table = tabulate(table_data, headers=['Metric', 'Value'], tablefmt='fancy_grid', colalign=(\"left\", \"left\"))\n",
    "        print(table)\n",
    "        # return NIS, NRMSE\n",
    "\n",
    "\n",
    "    def rearrange_one_hot(self,tensor):\n",
    "        # Find the indices that sort each row\n",
    "        sorted_indices = torch.argsort(tensor, dim=1, descending=True)\n",
    "        # Generate a new tensor of zeros with the same shape\n",
    "        new_tensor = torch.zeros_like(tensor)\n",
    "        # Place '1's in the appropriate positions based on the sorted indices\n",
    "        for i in range(tensor.size(0)):\n",
    "            new_tensor[i, sorted_indices[i, 0]] = 1\n",
    "\n",
    "        return torch.flip(new_tensor, dims=[0])\n",
    "    def visualize_latent(self,type='cat',rpearts=500):\n",
    "        if self.manifold_type=='deterministic':\n",
    "            if len(self.qual_kernel_columns) > 0:\n",
    "                for i in range(len(self.qual_kernel_columns)):\n",
    "                    zeta = self.zeta[i]\n",
    "                    dimm=zeta.size()[0]\n",
    "                    zeta_epsilon=torch.normal(mean=0,std=1,size=[dimm,2])\n",
    "\n",
    "                    A = getattr(self,'A_matrix')\n",
    "                    positions = A[i](x=zeta.float().to(**self.tkwargs))\n",
    "                    level = torch.max(self.perm[i]+1, axis = 0)[0].tolist()\n",
    "\n",
    "                    perm = self.perm[i]\n",
    "                    plot_sep(type=type,positions = positions, levels = level, perm = perm, constraints_flag=False)\n",
    "        elif self.manifold_type=='probabilistic':\n",
    "            for i in range(len(self.qual_kernel_columns)):\n",
    "                temp= self.transform_categorical(x=self.train_inputs[0][:,self.qual_kernel_columns[i]].clone().type(torch.int64).to(self.tkwargs['device']), perm_dict = self.perm_dict[i], zeta = self.zeta[i])\n",
    "            unique_rows, indices = torch.unique(temp, dim=0, return_inverse=True)\n",
    "            xp=self.rearrange_one_hot(unique_rows)\n",
    "            z_p_list =[]\n",
    "            label=[]\n",
    "            epsilon=torch.normal(mean=0,std=1,size=[rpearts,2])\n",
    "            for i in range(self.num_levels_per_var[0]):\n",
    "                x_0=xp[i]\n",
    "                x_0=x_0.repeat(rpearts, 1)\n",
    "                z_p = getattr(self, 'A_matrix')(x=x_0, epsilon=epsilon)\n",
    "                z_p_list.append(z_p)\n",
    "                label.append(i*torch.ones_like(z_p))\n",
    "            z_p_all = torch.cat(z_p_list, dim=0)\n",
    "            label_ground_truth=torch.cat(label, dim=0)\n",
    "            #########################\n",
    "            plt.rcParams['font.family'] = 'Times New Roman'\n",
    "            plt.rcParams['font.size'] = 25\n",
    "            # plt.rcParams['figure.dpi']=150\n",
    "            tab20 = plt.get_cmap('tab10')\n",
    "            colors = tab20.colors\n",
    "            colors=['deeppink','gold','darkorange','gray','orangered']\n",
    "            plt.figure(figsize=(8,6))\n",
    "\n",
    "            # Assuming z_p_all is a torch.Tensor\n",
    "            z_p_all_np = z_p_all.detach().numpy()\n",
    "            unique_labels = np.unique(label_ground_truth)\n",
    "            markers = ['X','o','s',\"v\", 'p']\n",
    "\n",
    "            for idx, label in enumerate(unique_labels):\n",
    "                mask = (label_ground_truth == label)\n",
    "                plt.scatter(z_p_all_np[mask[:,0], 0], z_p_all_np[mask[:,0], 1], \n",
    "                            c=colors[idx], \n",
    "                            marker=markers[idx], \n",
    "                            alpha=.6,\n",
    "                            s=250, \n",
    "                            label=f'Label {label}')\n",
    "\n",
    "            # Create the legend and get the legend handles and labels\n",
    "            legend=[ 'HF', 'LF1','LF2','LF3']\n",
    "            plt.xlabel(r'$z_1$',labelpad=0,rotation=0,usetex=True)\n",
    "            plt.ylabel(r'$z_2$',labelpad=14,rotation=0,usetex=True)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "    \n",
    "    def evaluation(self,Xtest,ytest):\n",
    "        self.eval()\n",
    "        ytest=ytest.reshape(-1).to(self.tkwargs['device'])\n",
    "        Xtest=Xtest.to(self.tkwargs['device'])\n",
    "        ytest=ytest.reshape(-1)\n",
    "        Xtest=Xtest\n",
    "        likelihood=self.likelihood\n",
    "        ytest_sc = (ytest-self.y_mean)/self.y_std\n",
    "\n",
    "        with torch.no_grad():\n",
    "            trained_pred_dist = likelihood(self(Xtest))\n",
    "        # Negative Log Predictive Density (NLPD)\n",
    "        final_nlpd = gpytorch.metrics.negative_log_predictive_density(trained_pred_dist,ytest_sc)\n",
    "        \n",
    "        # Mean Squared Error (MSE)\n",
    "        final_mse = gpytorch.metrics.mean_squared_error(trained_pred_dist, ytest_sc, squared=True)\n",
    "        # Mean Absolute Error (MAE)\n",
    "        final_mae = gpytorch.metrics.mean_absolute_error(trained_pred_dist, ytest_sc)\n",
    "        def interval_score(y_true, trained_pred_dist, alpha = 0.05):\n",
    "            mu_low, mu_up = trained_pred_dist.confidence_region()\n",
    "            out = mu_up - mu_low\n",
    "            out += (y_true > mu_up)* 2/alpha * (y_true - mu_up)\n",
    "            out += (y_true <mu_low)* 2/alpha * (mu_low - y_true)\n",
    "            return out\n",
    "        IS=interval_score(ytest_sc, trained_pred_dist, alpha = 0.05).mean()\n",
    "\n",
    "        ## back to the original scale:\n",
    "        final_mse=final_mse*(self.y_std)**2\n",
    "        final_mae=final_mae*torch.abs(self.y_std)\n",
    "        IS=IS*torch.abs(self.y_std)\n",
    "        ###    \n",
    "        RRMSE=torch.sqrt(final_mse/torch.var(ytest))\n",
    "        table_data = [\n",
    "            ['Negative Log-Likelihood (NLL)', final_nlpd],\n",
    "            ['Mean Squared Error (MSE)', final_mse],\n",
    "            ['Mean Absolute Error  (MAE)', final_mae],\n",
    "            ['Relative Root Mean Square Error (RRMSE)', RRMSE],\n",
    "            ['Interval Score (IS)', IS]\n",
    "        ]\n",
    "        # Print the table\n",
    "        table = tabulate(table_data, headers=['Metric', 'Value'], tablefmt='fancy_grid', colalign=(\"left\", \"left\"))\n",
    "        print(table)\n",
    "\n",
    "    def calibration_result(self,mean_train,std_train):\n",
    "        self.calibration_id\n",
    "        n_s=0\n",
    "        for n in self.calibration_id:\n",
    "            n_s+=1\n",
    "            if self.calibration_type=='probabilistic':\n",
    "                mean= (getattr(self,'Theta_'+str(n)).weights*std_train[n] +mean_train[n])[0].detach().numpy()\n",
    "                STD= torch.abs((getattr(self,'Theta_'+str(n)).bias)*std_train[n])[0].detach().numpy()\n",
    "                print(\"For Calibration parameter Theta_\"+str(n)+ \" Estimated Mean is \"  + str(mean)+\" and Estimated STD is \"  + str(STD))\n",
    "                x = np.linspace(mean-5*STD,mean+5*STD, 1000)\n",
    "                pdf_values = norm.pdf(x, mean, STD).squeeze()\n",
    "                plt.figure()\n",
    "                plt.plot(x, pdf_values, label='Zeta_'+str(n))\n",
    "                plt.title(r'$\\mathit{\\hat{\\Theta}}_{' + str(n) + '}$')  # Italic LaTeX styled title with hat only on Theta\n",
    "                plt.xlabel('Value')\n",
    "                plt.ylabel('Density')\n",
    "                plt.grid(True) \n",
    "\n",
    "            else:\n",
    "                xx=torch.where(self.train_inputs[0][:,-1]==0)[0][0]\n",
    "                GT=self.train_inputs[0][xx,n]*std_train[n] +mean_train[n]\n",
    "                print(\"=================GP + Results===================\")\n",
    "                print(\"Estimated Calibration parameter for Zeta_\"+str(n_s)+ \" is \"  + str((getattr(self,'Theta_'+str(n)).constant.detach()*std_train[n] +mean_train[n])))\n",
    "\n",
    "    @classmethod\n",
    "    def show(cls):\n",
    "        plt.show()\n",
    "        \n",
    "    def get_params(self, name = None):\n",
    "        params = {}\n",
    "        print('###################Parameters###########################')\n",
    "        for n, value in self.named_parameters():\n",
    "             params[n] = value\n",
    "        if name is None:\n",
    "            print(params)\n",
    "            return params\n",
    "        else:\n",
    "            if name == 'Mean':\n",
    "                key = 'mean_module.constant'\n",
    "            elif name == 'Sigma':\n",
    "                key = 'covar_module.raw_outputscale'\n",
    "            elif name == 'Noise':\n",
    "                key = 'likelihood.noise_covar.raw_noise'\n",
    "            elif name == 'Omega':\n",
    "                for n in params.keys():\n",
    "                    if 'raw_lengthscale' in n and params[n].numel() > 1:\n",
    "                        key = n\n",
    "            print(params[key])\n",
    "            return params[key]\n",
    "    \n",
    "\n",
    "    def sample_y(self, size = 1, X = None, plot = False):\n",
    "        if X == None:\n",
    "            X = self.train_inputs[0]\n",
    "        \n",
    "        self.eval()\n",
    "        out = self.likelihood(self(X))\n",
    "        draws = out.sample(sample_shape = torch.Size([size]))\n",
    "        index = np.argsort(out.loc.detach().numpy())\n",
    "        if plot:\n",
    "            _ = plt.figure(figsize=(12,6))\n",
    "            _ = plt.scatter(list(range(len(X))), out.loc.detach().numpy()[index], color = 'red', s = 20, marker = 'o')\n",
    "            _ = plt.scatter(np.repeat(np.arange(len(X)).reshape(1,-1), size, axis = 0), \n",
    "                draws.detach().numpy()[:,index], color = 'blue', s = 1, alpha = 0.5, marker = '.')\n",
    "        return draws\n",
    "\n",
    "    def get_latent_space(self):\n",
    "        if len(self.qual_index) > 0:\n",
    "            zeta = torch.tensor(self.zeta, dtype = torch.float64).to(**self.tkwargs)\n",
    "            positions = self.nn_model(zeta)\n",
    "            return positions.detach()\n",
    "        else:\n",
    "            print('No categorical Variable, No latent positions')\n",
    "            return None\n",
    "\n",
    "\n",
    "\n",
    "    def LMMAPPING(self, num_features:int, type = 'Linear',lv_dim = 2):\n",
    "\n",
    "        if type == 'Linear':\n",
    "            in_feature = num_features\n",
    "            out_feature = lv_dim\n",
    "            lm = torch.nn.Linear(in_feature, out_feature, bias = False)\n",
    "            return lm\n",
    "\n",
    "        else:\n",
    "            raise ValueError('Only Linear type for now')    \n",
    "\n",
    "    def zeta_matrix(self,\n",
    "        num_levels:int,\n",
    "        lv_dim:int,\n",
    "        batch_shape=torch.Size()\n",
    "    ) -> None:\n",
    "\n",
    "        if any([i == 1 for i in num_levels]):\n",
    "            raise ValueError('Categorical variable has only one level!')\n",
    "\n",
    "        if lv_dim == 1:\n",
    "            raise RuntimeWarning('1D latent variables are difficult to optimize!')\n",
    "        \n",
    "        for level in num_levels:\n",
    "            if lv_dim > level - 0:\n",
    "                lv_dim = min(lv_dim, level-1)\n",
    "                raise RuntimeWarning(\n",
    "                    'The LV dimension can atmost be num_levels-1. '\n",
    "                    'Setting it to %s in place of %s' %(level-1,lv_dim)\n",
    "                )\n",
    "    \n",
    "        from itertools import product\n",
    "        levels = []\n",
    "        for l in num_levels:\n",
    "            levels.append(torch.arange(l))\n",
    "\n",
    "        perm = list(product(*levels))\n",
    "        perm = torch.tensor(perm, dtype=torch.int64)\n",
    "\n",
    "        #-------------Mapping-------------------------\n",
    "        perm_dic = {}\n",
    "        for i, row in enumerate(perm):\n",
    "            temp = str(row.tolist())\n",
    "            if temp not in perm_dic.keys():\n",
    "                perm_dic[temp] = i\n",
    "\n",
    "        #-------------One_hot_encoding------------------\n",
    "        for ii in range(perm.shape[-1]):\n",
    "            if perm[...,ii].min() != 0:\n",
    "                perm[...,ii] -= perm[...,ii].min()\n",
    "            \n",
    "        perm_one_hot = []\n",
    "        for i in range(perm.size()[1]):\n",
    "            perm_one_hot.append( torch.nn.functional.one_hot(perm[:,i]) )\n",
    "\n",
    "        perm_one_hot = torch.concat(perm_one_hot, axis=1)\n",
    "\n",
    "        return perm_one_hot, perm, perm_dic\n",
    "\n",
    "    #################################### transformation functions####################################\n",
    "\n",
    "    def transform_categorical(self, x:torch.Tensor,perm_dict = [], zeta = []) -> None:\n",
    "        if x.dim() == 1:\n",
    "            x = x.reshape(-1,1)\n",
    "        # categorical should start from 0\n",
    "        if self.training == False:\n",
    "            x = torch.tensor(setlevels(x))\n",
    "        if self.encoding_type == 'one-hot':\n",
    "            index = [perm_dict[str(row.tolist())] for row in x]\n",
    "\n",
    "            if x.dim() == 1:\n",
    "                x = x.reshape(len(x),)\n",
    "\n",
    "            return zeta[index,:]  \n",
    "\n",
    "    def transform_categorical_random_varible_for_latent(self,x_raw, x:torch.Tensor,perm_dict = [], zeta = []) -> None:\n",
    "        \n",
    "        dimm=zeta.size()[0]\n",
    "        # zeta=torch.normal(0,1,size=[dimm,2])\n",
    "\n",
    "        self.random_zeta.append(torch.normal(mean=0,std=1,size=[dimm,2]))\n",
    "        \n",
    "        if x_raw.requires_grad:\n",
    "            random_zeta_appy=self.random_zeta[-1]\n",
    "        else:\n",
    "            random_zeta_appy=self.random_zeta[0]\n",
    "        \n",
    "        if x.dim() == 1:\n",
    "            x = x.reshape(-1,1)\n",
    "        # categorical should start from 0\n",
    "        if self.training == False:\n",
    "            x = setlevels(x)\n",
    "        if self.encoding_type == 'one-hot':\n",
    "            index = [perm_dict[str(row.tolist())] for row in x]\n",
    "\n",
    "            if x.dim() == 1:\n",
    "                x = x.reshape(len(x),)\n",
    "\n",
    "            return random_zeta_appy[index,:]  \n",
    "        \n",
    "    def final_transform_categorical_random_varible_for_latent(self,x_raw, x:torch.Tensor,perm_dict = [], zeta = []) -> None:\n",
    "        \n",
    "        if x.dim() == 1:\n",
    "            x = x.reshape(-1,1)\n",
    "        if self.training == False:\n",
    "            x = setlevels(x)\n",
    "        if self.encoding_type == 'one-hot':\n",
    "            index = [perm_dict[str(row.tolist())] for row in x]\n",
    "\n",
    "            if x.dim() == 1:\n",
    "                x = x.reshape(len(x),)\n",
    "\n",
    "        dimm=zeta.size()[0]\n",
    "\n",
    "        self.random_zeta.append(torch.normal(mean=0,std=1,size=[dimm,2]))\n",
    "        \n",
    "        if x_raw.requires_grad:\n",
    "            random_zeta_appy=self.random_zeta[-1]\n",
    "        else:\n",
    "            if x_raw.size()[0]==400:\n",
    "                random_zeta_appy=0*self.random_zeta[0]\n",
    "            else: \n",
    "                random_zeta_appy=self.random_zeta[0]\n",
    "        return random_zeta_appy[index,:]       \n",
    "    \n",
    "\n",
    "    def Sobol(self, N=10000):\n",
    "        \"\"\"\n",
    "\n",
    "        This function calculates the sensitivity indecies for a function\n",
    "        Inputs:\n",
    "            self (GP_Model): The GP model (fitted by GP+) with p inputs and dy outputs.\n",
    "\n",
    "            N: is the size of the Sobol sequence used for evaluating the indecies. Should be larger than 1e5 for accuracy.\n",
    "\n",
    "        Outputs:\n",
    "            S: Matrix of size dy-by-p of main sensitivity indecies. \n",
    "            ST: Matrix of size dy-by-p of total sensitivity indecies.\n",
    "        \"\"\"\n",
    "        if N<1e5:\n",
    "            warnings.warn('Increase N for accuracy!')\n",
    "\n",
    "        p = self.train_inputs[0].shape[1] \n",
    "        dy = 1# self.train_targets.shape[1] \n",
    "\n",
    "        self.qual_index\n",
    "        self.num_levels_per_var\n",
    "        # sequence = torch.from_numpy( sobol_seq.i4_sobol_generate(2*p, N)).to(**self.tkwargs)\n",
    "        sequence = torch.from_numpy( sobol_seq.i4_sobol_generate(2*p, N))\n",
    "        def normalize_sobol_sequence(sequence, train_inputs,p):\n",
    "            \n",
    "            temp_1 = sequence[:,p:]\n",
    "            temp_2 = sequence[:,:p]\n",
    "            \n",
    "            # Normalize the sequence\n",
    "            mins = train_inputs.min(dim=0)[0]\n",
    "            maxs = train_inputs.max(dim=0)[0]\n",
    "\n",
    "            sequence_1= mins + (maxs - mins) * temp_1\n",
    "            sequence_2= mins + (maxs - mins) * temp_2\n",
    "            # Take care of categotrical inputes\n",
    "            j=0\n",
    "            for i in self.qual_index:\n",
    "                temp_1[:,i]= temp_1[:,i]*(self.num_levels_per_var[j]-1)\n",
    "                temp_2[:,i]=temp_2[:,i]*(self.num_levels_per_var[j]-1)\n",
    "                sequence_1[:,i]=temp_1[:,i].round()\n",
    "                sequence_2[:,i]=temp_2[:,i].round()\n",
    "                j+=1\n",
    "                return sequence_1,sequence_2\n",
    "        A,B = normalize_sobol_sequence(sequence, self.train_inputs[0],p)\n",
    "\n",
    "        # # A = A * (self.Y.max(axis=0) - self.Y.min(axis=0)) + self.Y.min(axis=0) ## Normalize genrated data\n",
    "\n",
    "        # B = A[:,p:]\n",
    "        # A = A[:,:p]\n",
    "        \n",
    "        AB = torch.zeros((N,p,p))\n",
    "        for i in range(p):\n",
    "            AB[:,:,i] = A\n",
    "            AB[:,i,i] = B[:,i]\n",
    "            \n",
    "        FA = self.predict(A,return_std=False).detach().cpu().numpy().reshape(-1,1)\n",
    "\n",
    "        FB = self.predict(B,return_std=False).detach().cpu().numpy().reshape(-1,1)\n",
    "\n",
    "        FAB = np.zeros((N, p, dy))\n",
    "        for i in range(p):\n",
    "            temp = self.predict(AB[:, :, i],return_std=False).detach().cpu().numpy()\n",
    "            FAB[:, i, :] = temp.reshape(-1,1)\n",
    "\n",
    "        S = np.zeros((p, dy))\n",
    "        ST = np.zeros((p, dy))\n",
    "\n",
    "        for i in range(p):\n",
    "            temp = FAB[:, i, :]\n",
    "            S[i, :] = np.sum(FB * (temp - FA), axis=0) / N\n",
    "            ST[i, :] = np.sum((FA - temp)**2, axis=0) / (2 * N)\n",
    "            \n",
    "        varY = np.var(np.concatenate([FA,FB]), axis=0)\n",
    "        S = (S / varY).T\n",
    "        ST = (ST / varY).T\n",
    "\n",
    "        return S, ST\n",
    "\n",
    "######################################################################## Other Classes Used in GP_Pluse  #####################################################\n",
    "class FFNN(nn.Module):\n",
    "    def __init__(self, GP_Plus, input_size, num_classes, layers,name):\n",
    "        super(FFNN, self).__init__()\n",
    "        self.hidden_num = len(layers)\n",
    "        if self.hidden_num > 0:\n",
    "            self.fci = nn.Linear(input_size, layers[0], bias=False) \n",
    "            GP_Plus.register_parameter(str(name)+'fci', self.fci.weight)\n",
    "            GP_Plus.register_prior(name = 'latent_prior_fci', prior=gpytorch.priors.NormalPrior(0.,1), param_or_closure=str(name)+'fci')\n",
    "\n",
    "            for i in range(1,self.hidden_num):\n",
    "                setattr(self, 'h' + str(i), nn.Linear(layers[i-1], layers[i], bias=False))\n",
    "                GP_Plus.register_parameter(str(name)+'h'+str(i), getattr(self, 'h' + str(i)).weight )\n",
    "                GP_Plus.register_prior(name = 'latent_prior'+str(i), prior=gpytorch.priors.NormalPrior(0.,1), param_or_closure=str(name)+'h'+str(i))\n",
    "            \n",
    "            self.fce = nn.Linear(layers[-1], num_classes, bias= False)\n",
    "            GP_Plus.register_parameter(str(name)+'fce', self.fce.weight)\n",
    "            GP_Plus.register_prior(name = 'latent_prior_fce', prior=gpytorch.priors.NormalPrior(0.,1), param_or_closure=str(name)+'fce')\n",
    "        else:\n",
    "            self.fci = Linear_MAP(input_size, num_classes, bias = False)\n",
    "            GP_Plus.register_parameter(name, self.fci.weight)\n",
    "            GP_Plus.register_prior(name = 'latent_prior_'+name, prior=gpytorch.priors.NormalPrior(0,1) , param_or_closure=name)\n",
    "\n",
    "    def forward(self, x, transform = lambda x: x):\n",
    "        \"\"\"\n",
    "        x here is the mnist images and we run it through fc1, fc2 that we created above.\n",
    "        we also add a ReLU activation function in between and for that (since it has no parameters)\n",
    "        I recommend using nn.functional (F)\n",
    "        \"\"\"\n",
    "        if self.hidden_num > 0:\n",
    "            x = torch.tanh(self.fci(x))\n",
    "            for i in range(1,self.hidden_num):\n",
    "                #x = F.relu(self.h(x))\n",
    "                x = torch.tanh( getattr(self, 'h' + str(i))(x) )\n",
    "            \n",
    "            x = self.fce(x)\n",
    "        else:\n",
    "            #self.fci.weight.data = torch.sinh(self.fci.weight.data)\n",
    "            x = self.fci(x, transform)\n",
    "        return x\n",
    "    \n",
    "############################################\n",
    "class FFNN_as_Mean(gpytorch.Module):\n",
    "    def __init__(self, GP_Plus, input_size, num_classes, layers,name):\n",
    "        super(FFNN_as_Mean, self).__init__()\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.hidden_num = len(layers)\n",
    "        if self.hidden_num > 0:\n",
    "            self.fci = Linear_class(input_size, layers[0], bias=True, name='fci') \n",
    "            for i in range(1,self.hidden_num):\n",
    "                setattr(self, 'h' + str(i), Linear_class(layers[i-1], layers[i], bias=True,name='h' + str(i)))\n",
    "            \n",
    "            self.fce = Linear_class(layers[-1], num_classes, bias=True,name='fce')\n",
    "        else:\n",
    "            self.fci = Linear_class(input_size, num_classes, bias=True, dtype = torch.float32,name='fci') #Linear_MAP(input_size, num_classes, bias = True)\n",
    "\n",
    "    def forward(self, x, transform = lambda x: x):\n",
    "\n",
    "        if self.hidden_num > 0:\n",
    "            \n",
    "            x = torch.tanh(self.fci(x))\n",
    "            # x = self.dropout(x)\n",
    "            # x = self.fci(x)\n",
    "            for i in range(1,self.hidden_num):\n",
    "                # x = torch.sigmoid( getattr(self, 'h' + str(i))(x) )\n",
    "                # x =  getattr(self, 'h' + str(i))(x) \n",
    "                x = torch.tanh( getattr(self, 'h' + str(i))(x) )\n",
    "                x = self.dropout(x)\n",
    "            x = self.fce(x)\n",
    "        else:\n",
    "            #self.fci.weight.data = torch.sinh(self.fci.weight.data)\n",
    "            x = self.fci(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "############################################\n",
    "class Linear_VAE(Mean):\n",
    "    __constants__ = ['in_features', 'out_features']\n",
    "    in_features: int\n",
    "    out_features: int\n",
    "    weight: Tensor\n",
    "    def __init__(self, in_features: int, out_features: int, bias: bool = True, name=None,\n",
    "                 device=None, dtype=None) -> None:\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super(Linear_VAE, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.name=str(name)\n",
    "        self.register_parameter(name=str(self.name)+'weight',  parameter= Parameter(torch.empty((out_features, in_features), **factory_kwargs)))\n",
    "        self.register_prior(name =str(self.name)+ 'prior_m_weight_fci', prior=gpytorch.priors.NormalPrior(0.,.2), param_or_closure=str(self.name)+'weight')\n",
    "\n",
    "        if bias:\n",
    "\n",
    "            self.register_parameter(name=str(self.name)+'bias',  parameter=Parameter(torch.empty(out_features, **factory_kwargs)))\n",
    "            self.register_prior(name= str(self.name)+'prior_m_bias_fci', prior=gpytorch.priors.NormalPrior(0.,.05), param_or_closure=str(self.name)+'bias')\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self) -> None:                                             \n",
    "\n",
    "        init.kaiming_uniform_( getattr(self,str(self.name)+'weight'), a=math.sqrt(5))\n",
    "        if getattr(self,str(self.name)+'bias') is not None:\n",
    "            fan_in, _ = init._calculate_fan_in_and_fan_out(getattr(self,str(self.name)+'weight'))\n",
    "            bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n",
    "            init.uniform_(getattr(self,str(self.name)+'bias'), -bound, bound)\n",
    "\n",
    "    def forward(self, input) -> Tensor:\n",
    "\n",
    "        return F.linear(input.double(), getattr(self,str(self.name)+'weight').double(), getattr(self,str(self.name)+'bias').double())      ### Forced to Add .double() for NN in mean function\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return 'in_features={}, out_features={}, bias={}'.format(\n",
    "            self.in_features, self.out_features, self.bias is not None\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "class Linear_MAP(nn.Linear):\n",
    "    def __init__(self, in_features: int, out_features: int, bias: bool = True, device=None, dtype=None) -> None:\n",
    "        super().__init__(in_features, out_features, bias, device, dtype)\n",
    "        \n",
    "    def forward(self, input, transform = lambda x: x):\n",
    "        return F.linear(input,transform(self.weight), self.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data generation and prepration:\n",
    "The next step is to generate the data, standardize it and separate it to train-test sets. This time, we convert the first and sixth features of Borehole example to categorical variables to have a mixed-input example.\n",
    "\n",
    "\n",
    "Then, we generate 1000 samples, standardize it and use 1% of theem as the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler, OneHotEncoder\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1843\n",
      "30\n",
      "The list of indices greater than or equal to n_data_th : [1, 3, 5, 7, 8, 9, 10, 11]\n",
      "8\n",
      "224\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "os.chdir('/Users/chenya68/Documents/GitHub/BFO')\n",
    "df = pd.read_csv('data/dps-cleaned-missing.csv')\n",
    "print(len(df))\n",
    "#df.head()\n",
    "\n",
    "cols_group = ['Project','Protein','Modality', 'Stress_Condition','Temp_C_', 'Time_Days_']\n",
    "cols_feature01 = ['Buffer','NaCl_mM_', 'PS80_', 'Sucrose_','Trehalose_','pH']\n",
    "cols_target = ['UP_SEC_Monomer','UP_SEC_HMW']\n",
    "cols_cate = ['Buffer','NaClRange','PS80_Range','Sucrose_Range','Trehalose_Range']\n",
    "\n",
    "# split dataframe by identifiers\n",
    "is_drop_na = True\n",
    "if(is_drop_na):\n",
    "    n_data_th = 20\n",
    "else:\n",
    "    n_data_th = 30\n",
    "splits_identifier = list(df[cols_group + cols_feature01+cols_target].groupby(cols_group,dropna=is_drop_na))\n",
    "#splits_identifier = list(df[cols_group + cols_cate+['pH']+cols_target].groupby(cols_group,dropna=is_drop_na))\n",
    "ndata = [len(df[1]) for df in splits_identifier]\n",
    "ndata_str = [str(e) for e in ndata]\n",
    "#print(','.join(ndata_str))\n",
    "plt.hist(ndata)\n",
    "plt.title('distribution of number of data points in each group')\n",
    "print(max(ndata))#21\n",
    "\n",
    "res = [idx for idx, val in enumerate(ndata) if val >= n_data_th]\n",
    "print(\"The list of indices greater than or equal to n_data_th : \" + str(res))\n",
    "print(len(res))\n",
    "print(np.sum(ndata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "split1 = splits_identifier[1]\n",
    "split3 = splits_identifier[3]\n",
    "split5 = splits_identifier[5]\n",
    "\n",
    "split1_formulation_df = split1[1][cols_feature01]\n",
    "split1_formulation_df.reset_index(drop = True,inplace =True)\n",
    "split3_formulation_df = split3[1][cols_feature01]\n",
    "split3_formulation_df.reset_index(drop = True,inplace =True)\n",
    "split5_formulation_df = split5[1][cols_feature01]\n",
    "split5_formulation_df.reset_index(drop = True,inplace =True)\n",
    "\n",
    "\n",
    "print(split1_formulation_df.equals(split3_formulation_df))\n",
    "print(split3_formulation_df.equals(split5_formulation_df))\n",
    "\n",
    "split1_target_df = split1[1][cols_target]\n",
    "split1_target_df.reset_index(drop = True,inplace =True)\n",
    "split1_target_df.columns = [c+'_1' for c in split1_target_df.columns]\n",
    "\n",
    "split3_target_df = split3[1][cols_target]\n",
    "split3_target_df.reset_index(drop = True,inplace =True)\n",
    "split3_target_df.columns = [c+'_3' for c in split3_target_df.columns]\n",
    "\n",
    "split5_target_df = split5[1][cols_target]\n",
    "split5_target_df.reset_index(drop = True,inplace =True)\n",
    "split5_target_df.columns = [c+'_5' for c in split5_target_df.columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Buffer</th>\n",
       "      <th>NaCl_mM_</th>\n",
       "      <th>PS80_</th>\n",
       "      <th>Sucrose_</th>\n",
       "      <th>Trehalose_</th>\n",
       "      <th>pH</th>\n",
       "      <th>UP_SEC_Monomer_1</th>\n",
       "      <th>UP_SEC_HMW_1</th>\n",
       "      <th>UP_SEC_Monomer_3</th>\n",
       "      <th>UP_SEC_HMW_3</th>\n",
       "      <th>UP_SEC_Monomer_5</th>\n",
       "      <th>UP_SEC_HMW_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Acetate</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>98.09</td>\n",
       "      <td>1.45</td>\n",
       "      <td>97.27</td>\n",
       "      <td>1.06</td>\n",
       "      <td>95.21</td>\n",
       "      <td>3.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Acetate</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>97.96</td>\n",
       "      <td>1.37</td>\n",
       "      <td>94.98</td>\n",
       "      <td>3.30</td>\n",
       "      <td>96.73</td>\n",
       "      <td>2.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Acetate</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>98.15</td>\n",
       "      <td>1.24</td>\n",
       "      <td>94.86</td>\n",
       "      <td>3.60</td>\n",
       "      <td>97.36</td>\n",
       "      <td>1.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Citrate</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>97.66</td>\n",
       "      <td>1.42</td>\n",
       "      <td>94.14</td>\n",
       "      <td>4.39</td>\n",
       "      <td>96.13</td>\n",
       "      <td>3.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Citrate</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>98.03</td>\n",
       "      <td>1.63</td>\n",
       "      <td>94.01</td>\n",
       "      <td>4.59</td>\n",
       "      <td>96.04</td>\n",
       "      <td>2.90</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Buffer  NaCl_mM_  PS80_  Sucrose_  Trehalose_   pH  UP_SEC_Monomer_1  \\\n",
       "0  Acetate         0    0.0       0.0           0  5.0             98.09   \n",
       "1  Acetate         0    0.0       0.0           0  5.5             97.96   \n",
       "2  Acetate         0    0.0       0.0           0  6.0             98.15   \n",
       "3  Citrate         0    0.0       0.0           0  5.5             97.66   \n",
       "4  Citrate         0    0.0       0.0           0  6.0             98.03   \n",
       "\n",
       "   UP_SEC_HMW_1  UP_SEC_Monomer_3  UP_SEC_HMW_3  UP_SEC_Monomer_5  \\\n",
       "0          1.45             97.27          1.06             95.21   \n",
       "1          1.37             94.98          3.30             96.73   \n",
       "2          1.24             94.86          3.60             97.36   \n",
       "3          1.42             94.14          4.39             96.13   \n",
       "4          1.63             94.01          4.59             96.04   \n",
       "\n",
       "   UP_SEC_HMW_5  \n",
       "0          3.86  \n",
       "1          2.43  \n",
       "2          1.82  \n",
       "3          3.53  \n",
       "4          2.90  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n21_total_df = pd.concat((split1_formulation_df,split1_target_df,split3_target_df,split5_target_df),axis = 1)\n",
    "n21_total_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import factorize\n",
    "x_name = 'Buffer'\n",
    "labels, categories = factorize(n21_total_df[x_name])\n",
    "n21_total_df[x_name+\"_label\"] = labels\n",
    "\n",
    "cols_feature02 = ['NaCl_mM_', 'PS80_', 'Sucrose_', 'Trehalose_', 'pH','Buffer_label']\n",
    "#cols_feature02 = ['Buffer_label', 'NaCl_mM_', 'pH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_target_new = ['UP_SEC_Monomer_1','UP_SEC_HMW_1',\t'UP_SEC_Monomer_3',\t'UP_SEC_HMW_3',\t'UP_SEC_Monomer_5',\t'UP_SEC_HMW_5']\n",
    "#cols_target_new = ['UP_SEC_HMW_1','UP_SEC_LMW_1',\t'UP_SEC_HMW_3',\t'UP_SEC_LMW_3','UP_SEC_HMW_5','UP_SEC_LMW_5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NaCl_mM_</th>\n",
       "      <th>PS80_</th>\n",
       "      <th>Sucrose_</th>\n",
       "      <th>Trehalose_</th>\n",
       "      <th>pH</th>\n",
       "      <th>Buffer_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   NaCl_mM_  PS80_  Sucrose_  Trehalose_   pH  Buffer_label\n",
       "0         0    0.0       0.0           0  5.0             0\n",
       "1         0    0.0       0.0           0  5.5             0\n",
       "2         0    0.0       0.0           0  6.0             0\n",
       "3         0    0.0       0.0           0  5.5             1\n",
       "4         0    0.0       0.0           0  6.0             1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_X = n21_total_df[cols_feature02]\n",
    "df_X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UP_SEC_Monomer_1</th>\n",
       "      <th>UP_SEC_HMW_1</th>\n",
       "      <th>UP_SEC_Monomer_3</th>\n",
       "      <th>UP_SEC_HMW_3</th>\n",
       "      <th>UP_SEC_Monomer_5</th>\n",
       "      <th>UP_SEC_HMW_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>98.09</td>\n",
       "      <td>1.45</td>\n",
       "      <td>97.27</td>\n",
       "      <td>1.06</td>\n",
       "      <td>95.21</td>\n",
       "      <td>3.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>97.96</td>\n",
       "      <td>1.37</td>\n",
       "      <td>94.98</td>\n",
       "      <td>3.30</td>\n",
       "      <td>96.73</td>\n",
       "      <td>2.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>98.15</td>\n",
       "      <td>1.24</td>\n",
       "      <td>94.86</td>\n",
       "      <td>3.60</td>\n",
       "      <td>97.36</td>\n",
       "      <td>1.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>97.66</td>\n",
       "      <td>1.42</td>\n",
       "      <td>94.14</td>\n",
       "      <td>4.39</td>\n",
       "      <td>96.13</td>\n",
       "      <td>3.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>98.03</td>\n",
       "      <td>1.63</td>\n",
       "      <td>94.01</td>\n",
       "      <td>4.59</td>\n",
       "      <td>96.04</td>\n",
       "      <td>2.90</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UP_SEC_Monomer_1  UP_SEC_HMW_1  UP_SEC_Monomer_3  UP_SEC_HMW_3  \\\n",
       "0             98.09          1.45             97.27          1.06   \n",
       "1             97.96          1.37             94.98          3.30   \n",
       "2             98.15          1.24             94.86          3.60   \n",
       "3             97.66          1.42             94.14          4.39   \n",
       "4             98.03          1.63             94.01          4.59   \n",
       "\n",
       "   UP_SEC_Monomer_5  UP_SEC_HMW_5  \n",
       "0             95.21          3.86  \n",
       "1             96.73          2.43  \n",
       "2             97.36          1.82  \n",
       "3             96.13          3.53  \n",
       "4             96.04          2.90  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Y = n21_total_df[cols_target_new]\n",
    "df_Y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X_train, df_X_test, df_y_train, df_y_test = train_test_split(df_X, df_Y, test_size=0.2, random_state=0)\n",
    "#df_X_train, df_X_test, df_y_train, df_y_test = train_test_split(df_X, df_Y, test_size=0.2, random_state=0, stratify =df_X[['PS80_','Sucrose_']])\n",
    "# only one datapoint of PS90 and Sucrose are greater than 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "xct = ColumnTransformer([('x_mm_scaler',MinMaxScaler(),df_X_train.columns.difference(['Buffer_label']))], \n",
    "                         remainder = 'passthrough')\n",
    "\n",
    "scaled_X_train=xct.fit_transform(df_X_train) \n",
    "scaled_X_test=xct.transform(df_X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create and fit the GP+ model: Firstly, we create the model using GP_Plus command, and then the model is optimized with model.fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_x_sum.shape torch.Size([16, 6])\n",
      "mean_x.shape torch.Size([16, 6])\n",
      "covar_x.shape torch.Size([96, 96])\n",
      "ensemble_mean.shape torch.Size([16, 6])\n",
      "ensemble_covar.shape torch.Size([96, 96])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chenya68/anaconda3/envs/gpplus-py39/lib/python3.9/site-packages/gpytorch/lazy/triangular_lazy_tensor.py:136: UserWarning: torch.triangular_solve is deprecated in favor of torch.linalg.solve_triangularand will be removed in a future PyTorch release.\n",
      "torch.linalg.solve_triangular has its arguments reversed and does not return a copy of one of the inputs.\n",
      "X = torch.triangular_solve(B, A).solution\n",
      "should be replaced with\n",
      "X = torch.linalg.solve_triangular(A, B). (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp:2121.)\n",
      "  res = torch.triangular_solve(right_tensor, self.evaluate(), upper=self.upper).solution\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(-125.5585, dtype=torch.float64, grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_train_x0  = torch.rand(16,8)\n",
    "t_train_y0 = torch.rand(16,6)\n",
    "#qual_ind_lev = {5: 3}\n",
    "gp_model0 = GP_Plus(t_train_x0, t_train_y0)\n",
    "\n",
    "#print(gp_model0.likelihood)\n",
    "\n",
    "#print(gp_model0.mean_module(t_train_x).shape)\n",
    "#print(gp_model0.covar_module(t_train_x).shape)\n",
    "\n",
    "gp_model_output0 = gp_model0(*gp_model0.train_inputs)\n",
    "gp_lik_out0 = gp_model0.likelihood(gp_model_output0).log_prob(gp_model0.train_targets)\n",
    "gp_lik_out0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chenya68/anaconda3/envs/gpplus-py39/lib/python3.9/site-packages/gpytorch/lazy/triangular_lazy_tensor.py:136: UserWarning: torch.triangular_solve is deprecated in favor of torch.linalg.solve_triangularand will be removed in a future PyTorch release.\n",
      "torch.linalg.solve_triangular has its arguments reversed and does not return a copy of one of the inputs.\n",
      "X = torch.triangular_solve(B, A).solution\n",
      "should be replaced with\n",
      "X = torch.linalg.solve_triangular(A, B). (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp:2121.)\n",
      "  res = torch.triangular_solve(right_tensor, self.evaluate(), upper=self.upper).solution\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(-112.8575, dtype=torch.float64, grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qual_ind_lev = {5: 3}\n",
    "y_ind = 5\n",
    "t_train_x = torch.Tensor(scaled_X_train)\n",
    "#t_train_y = torch.Tensor(df_y_train.to_numpy()[:,y_ind])\n",
    "t_train_y = torch.Tensor(df_y_train.to_numpy())\n",
    "gp_model = GP_Plus(t_train_x, t_train_y, qual_ind_lev=qual_ind_lev)\n",
    "\n",
    "output = gp_model(*gp_model.train_inputs)\n",
    "gp_model.likelihood(output).log_prob(gp_model.train_targets)\n",
    "#gp_model.likelihood(output).log_prob(gp_model.train_targets.contiguous())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from gpytorch import settings as gptsettings\n",
    "from gpytorch.utils.errors import NanError,NotPSDError\n",
    "from scipy.optimize import minimize,OptimizeResult\n",
    "from collections import OrderedDict\n",
    "from functools import reduce\n",
    "from joblib import Parallel,delayed\n",
    "from joblib.externals.loky import set_loky_pickler\n",
    "from typing import Dict,List,Tuple,Optional,Union\n",
    "from copy import deepcopy\n",
    "from scipy.optimize import Bounds\n",
    "from scipy.optimize import NonlinearConstraint\n",
    "from scipy.optimize import BFGS\n",
    "\n",
    "from gpplus.utils.interval_score import interval_score\n",
    "tkwargs = {\n",
    "    \"dtype\": torch.float,\n",
    "    \"device\": torch.device(\"cpu\" if torch.cuda.is_available() else \"cpu\"),\n",
    "}\n",
    "\n",
    "def marginal_log_likelihood(model,add_prior:bool,regularization_parameter=[0,0]):\n",
    "    output = model(*model.train_inputs)\n",
    "    out = model.likelihood(output).log_prob(model.train_targets)\n",
    "    if add_prior:\n",
    "        # add priors\n",
    "        for _, module, prior, closure, _ in model.named_priors():\n",
    "            out.add_(prior.log_prob(closure(module)).sum())\n",
    "    temp = 0\n",
    "    temp_1=0\n",
    "    for name, param in model.named_parameters():\n",
    "        string_list = ['fci', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'h8','h9', 'h10', 'h11', 'h12','fce']\n",
    "        if name in string_list:\n",
    "            temp += torch.norm(param)\n",
    "            temp_1 += torch.sum(torch.abs(param))\n",
    "        elif name in ['nn_model.' + str + '.bias' for str in string_list]:\n",
    "            temp += torch.norm(param)\n",
    "            temp_1 += torch.sum(torch.abs(param))\n",
    "\n",
    "    out -= regularization_parameter[0]*temp_1 + regularization_parameter[1]* temp    \n",
    "    ## Interval Score if neede for BO\n",
    "    if model.IS is True:\n",
    "        score, accuracy = interval_score(output.mean + 1.96 * output.variance.sqrt(), output.mean - 1.96 * output.variance.sqrt(), model.y_scaled)\n",
    "        return out - 0.08*torch.abs(out) * score#- torch.exp(model.interval_alpha) * score\n",
    "    return out \n",
    "\n",
    "class MLLObjective:\n",
    "\n",
    "    def __init__(self,model,add_prior,regularization_parameter):\n",
    "        self.model = model \n",
    "        self.add_prior = add_prior\n",
    "        self.regularization_parameter=regularization_parameter\n",
    "\n",
    "        parameters = OrderedDict([\n",
    "            (n,p) for n,p in self.model.named_parameters() if p.requires_grad\n",
    "        ])\n",
    "        self.param_shapes = OrderedDict()\n",
    "        for n,p in self.model.named_parameters():\n",
    "            if p.requires_grad:\n",
    "                if len(parameters[n].size()) > 0:\n",
    "                    self.param_shapes[n] = parameters[n].size()\n",
    "                else:\n",
    "                    self.param_shapes[n] = torch.Size([1])\n",
    "    \n",
    "    def pack_parameters(self) -> np.ndarray:\n",
    "        parameters = OrderedDict([\n",
    "            (n,p) for n,p in self.model.named_parameters() if p.requires_grad\n",
    "        ])\n",
    "        \n",
    "        return np.concatenate([parameters[n].cpu().data.numpy().ravel() for n in parameters])\n",
    "    \n",
    "    def unpack_parameters(self, x:np.ndarray) -> torch.Tensor:\n",
    "        i = 0\n",
    "        named_parameters = OrderedDict()\n",
    "        for n in self.param_shapes:\n",
    "            param_len = reduce(lambda x,y: x*y, self.param_shapes[n])\n",
    "            # slice out a section of this length\n",
    "            param = x[i:i+param_len]\n",
    "            # reshape according to this size, and cast to torch\n",
    "            param = param.reshape(*self.param_shapes[n])\n",
    "            named_parameters[n] = torch.from_numpy(param).to(**tkwargs)\n",
    "            # update index\n",
    "            i += param_len\n",
    "        return named_parameters\n",
    "\n",
    "    def pack_grads(self) -> None:\n",
    "        \"\"\"Concatenate gradients from the parameters to 1D numpy array\n",
    "        \"\"\"\n",
    "        grads = []\n",
    "        for name,p in self.model.named_parameters():\n",
    "            if p.requires_grad:\n",
    "                grad = p.grad.cpu().data.numpy()\n",
    "                grads.append(grad.ravel())\n",
    "        return np.concatenate(grads).astype(np.float64)\n",
    "\n",
    "    def fun(self, x:np.ndarray,return_grad=True) -> Union[float,Tuple[float,np.ndarray]]:\n",
    "        # unpack x and load into module \n",
    "        state_dict = self.unpack_parameters(x)\n",
    "        old_dict = self.model.state_dict()\n",
    "        old_dict.update(state_dict)\n",
    "        self.model.load_state_dict(old_dict)\n",
    "\n",
    "        self.model.zero_grad()\n",
    "        obj = -marginal_log_likelihood(self.model, self.add_prior,self.regularization_parameter) # negative sign to minimize\n",
    "        \n",
    "        if return_grad:\n",
    "            obj.backward()\n",
    "            \n",
    "            return obj.item(),self.pack_grads()\n",
    "        \n",
    "        return obj.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _sample_from_prior(model) -> np.ndarray:\n",
    "    out = []\n",
    "    for _,module,prior,closure,_ in model.named_priors():\n",
    "        if not closure(module).requires_grad:\n",
    "            continue\n",
    "            \n",
    "        out.append(prior.expand(closure(module).shape).sample().cpu().numpy().ravel())\n",
    "    \n",
    "    return np.concatenate(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----prior-----\n",
      "NormalPrior()\n",
      "-----closure(module)------\n",
      "Parameter containing:\n",
      "tensor([[-0.5125,  0.4809,  0.2351],\n",
      "        [ 0.0534,  0.0958, -0.4216]], requires_grad=True)\n",
      "-----a------\n",
      "latent_prior_latent[5]\n",
      "-----prior-----\n",
      "LogHalfHorseshoePrior(scale: 0.009999999776482582, lb: 9.99999993922529e-09)\n",
      "-----closure(module)------\n",
      "Parameter containing:\n",
      "tensor([-9.2104], requires_grad=True)\n",
      "-----a------\n",
      "likelihood.noise_prior\n",
      "-----prior-----\n",
      "NormalPrior()\n",
      "-----closure(module)------\n",
      "Parameter containing:\n",
      "tensor([[0., 0., 0., 0., 0.]], requires_grad=True)\n",
      "-----a------\n",
      "covar_module.data_covar_module.kernels.1.lengthscale_prior\n"
     ]
    }
   ],
   "source": [
    "out_prior = []\n",
    "for a,module,prior,closure,b in gp_model.named_priors():\n",
    "        print('-----prior-----')\n",
    "        print(prior)\n",
    "        print('-----closure(module)------')\n",
    "        print(closure(module))\n",
    "        print('-----a------')\n",
    "        print(a)\n",
    "        #print(b)\n",
    "        if not closure(module).requires_grad:\n",
    "            continue\n",
    "        tmp = prior.expand(closure(module).shape).sample().cpu().numpy().ravel()    \n",
    "        out_prior.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('latent[5]',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.5125,  0.4809,  0.2351],\n",
       "          [ 0.0534,  0.0958, -0.4216]], requires_grad=True)),\n",
       " ('likelihood.raw_task_noises',\n",
       "  Parameter containing:\n",
       "  tensor([0., 0., 0., 0., 0., 0.], requires_grad=True)),\n",
       " ('likelihood.raw_noise',\n",
       "  Parameter containing:\n",
       "  tensor([-9.2104], requires_grad=True)),\n",
       " ('mean_module.base_means.0.constant',\n",
       "  Parameter containing:\n",
       "  tensor([0.], requires_grad=True)),\n",
       " ('mean_module.base_means.1.constant',\n",
       "  Parameter containing:\n",
       "  tensor([0.], requires_grad=True)),\n",
       " ('mean_module.base_means.2.constant',\n",
       "  Parameter containing:\n",
       "  tensor([0.], requires_grad=True)),\n",
       " ('mean_module.base_means.3.constant',\n",
       "  Parameter containing:\n",
       "  tensor([0.], requires_grad=True)),\n",
       " ('mean_module.base_means.4.constant',\n",
       "  Parameter containing:\n",
       "  tensor([0.], requires_grad=True)),\n",
       " ('mean_module.base_means.5.constant',\n",
       "  Parameter containing:\n",
       "  tensor([0.], requires_grad=True)),\n",
       " ('covar_module.task_covar_module.covar_factor',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.1527],\n",
       "          [ 0.8540],\n",
       "          [ 0.3751],\n",
       "          [ 0.1679],\n",
       "          [ 0.1800],\n",
       "          [ 0.1174]], requires_grad=True)),\n",
       " ('covar_module.task_covar_module.raw_var',\n",
       "  Parameter containing:\n",
       "  tensor([-1.3898, -0.5516, -1.3841,  1.5158, -1.3848,  0.6754],\n",
       "         requires_grad=True)),\n",
       " ('covar_module.data_covar_module.kernels.0.raw_lengthscale',\n",
       "  Parameter containing:\n",
       "  tensor([[0.5413]])),\n",
       " ('covar_module.data_covar_module.kernels.1.raw_lengthscale',\n",
       "  Parameter containing:\n",
       "  tensor([[0., 0., 0., 0., 0.]], requires_grad=True))]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(n,p) for (n,p) in gp_model.named_parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "covar_module.data_covar_module.kernels.1.lengthscale_prior\n",
      "RBFKernel(\n",
      "  (raw_lengthscale_constraint): Positive()\n",
      "  (lengthscale_prior): NormalPrior()\n",
      "  (distance_module): Distance()\n",
      ")\n",
      "NormalPrior()\n",
      "<function Module.register_prior.<locals>.closure at 0x7fda09550550>\n",
      "<function Module.register_prior.<locals>.setting_closure at 0x7fda095503a0>\n"
     ]
    }
   ],
   "source": [
    "#ls_named_prior = [(_,module,prior,closure,_)  for (_,module,prior,closure,_) in gp_model.named_priors()]\n",
    "ls_named_prior = [n for n in gp_model.named_priors()]\n",
    "#len(ls_named_prior)\n",
    "for k in ls_named_prior[2]:\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.named_priors of GP_Plus(\n",
       "  (likelihood): MultitaskGaussianLikelihood(\n",
       "    (raw_task_noises_constraint): GreaterThan(1.000E-08)\n",
       "    (raw_noise_constraint): GreaterThan(1.000E-08)\n",
       "    (noise_prior): LogHalfHorseshoePrior(scale: 0.009999999776482582, lb: 9.99999993922529e-09)\n",
       "  )\n",
       "  (mean_module): MultitaskMean(\n",
       "    (base_means): ModuleList(\n",
       "      (0): ConstantMean()\n",
       "      (1): ConstantMean()\n",
       "      (2): ConstantMean()\n",
       "      (3): ConstantMean()\n",
       "      (4): ConstantMean()\n",
       "      (5): ConstantMean()\n",
       "    )\n",
       "  )\n",
       "  (covar_module): MultitaskKernel(\n",
       "    (task_covar_module): IndexKernel(\n",
       "      (raw_var_constraint): Positive()\n",
       "    )\n",
       "    (data_covar_module): ProductKernel(\n",
       "      (kernels): ModuleList(\n",
       "        (0): RBFKernel(\n",
       "          (raw_lengthscale_constraint): Positive()\n",
       "          (distance_module): Distance()\n",
       "        )\n",
       "        (1): RBFKernel(\n",
       "          (raw_lengthscale_constraint): Positive()\n",
       "          (lengthscale_prior): NormalPrior()\n",
       "          (distance_module): Distance()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (latent_prior_latent[5]): NormalPrior()\n",
       ")>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gp_model.named_priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cons_f(x,likobj):\n",
    "    zeta = torch.tensor(likobj.model.zeta, dtype = torch.float64)\n",
    "    A = likobj.unpack_parameters(x)['fci']\n",
    "    likobj.model.nn_model.fci.weight.data = A\n",
    "    positions = likobj.model.nn_model(zeta)\n",
    "    out_constraint=positions.detach().numpy().reshape(-1,)\n",
    "    return out_constraint[0:8]\n",
    "\n",
    "def get_bounds(likobj, theta):\n",
    "\n",
    "    dic = likobj.unpack_parameters(theta)\n",
    "\n",
    "    minn = np.empty(0)\n",
    "    maxx = np.empty(0)\n",
    "    for name, values in dic.items():\n",
    "        for ii in range(len(likobj.model.qual_kernel_columns)):\n",
    "            if name ==  str(likobj.model.qual_kernel_columns[ii]):\n",
    "                minn = np.concatenate( (minn,  np.repeat(-3, values.numel()) ) )\n",
    "                maxx = np.concatenate( (maxx,  np.repeat(3, values.numel()) ) )\n",
    "        if name == 'likelihood.noise_covar.raw_noise' or name.startswith('[') or name.startswith('latent['):\n",
    "            minn = np.concatenate( (minn,  np.repeat(-np.inf, values.numel()) ) )\n",
    "            maxx = np.concatenate( (maxx,  np.repeat( np.inf, values.numel()) ) )\n",
    "        if 'raw_lengthscale' in name:\n",
    "            minn = np.concatenate( (minn,  np.repeat(-10.0, values.numel()) ) )\n",
    "            maxx = np.concatenate( (maxx,  np.repeat( 3.0, values.numel()) ) )\n",
    "        elif name.startswith('covar_module'):\n",
    "            minn = np.concatenate( (minn,  np.repeat(-10.0, values.numel()) ) )\n",
    "            maxx = np.concatenate( (maxx,  np.repeat( 3.0, values.numel()) ) )\n",
    "            ######################################################################################### For multiple Bases ##################################\n",
    "        elif name.startswith('mean'):\n",
    "            minn = np.concatenate( (minn,  np.repeat(-1.5, values.numel()) ) )\n",
    "            maxx = np.concatenate( (maxx,  np.repeat( 1.5, values.numel()) ) )\n",
    "        elif name.startswith('Theta_'):\n",
    "            minn = np.concatenate( (minn,  np.repeat(-15, values.numel()) ) )\n",
    "            maxx = np.concatenate( (maxx,  np.repeat( 15, values.numel()) ) )\n",
    "        elif name.startswith('encoder'):\n",
    "            minn = np.concatenate( (minn,  np.repeat(-15, values.numel()) ) )\n",
    "            maxx = np.concatenate( (maxx,  np.repeat( 15, values.numel()) ) )\n",
    "            ######################################################################################### For A_matrix and Variationa encoder  #################\n",
    "        elif name.startswith('A_matrix'):\n",
    "            minn = np.concatenate( (minn,  np.repeat(-10, values.numel()) ) )\n",
    "            maxx = np.concatenate( (maxx,  np.repeat( 10, values.numel()) ) )\n",
    "    return np.array(minn).reshape(-1,), np.array(maxx).reshape(-1,)\n",
    "\n",
    "\n",
    "def _fit_model_from_state(likobj,theta0,jac,options, method = 'trust-constr',constraint=False,bounds=False):\n",
    "    \n",
    "    min, max = get_bounds(likobj, theta0)\n",
    "    bounds_acts = Bounds(min, max)\n",
    "    nonlinear_constraint = NonlinearConstraint(lambda x: cons_f(x, likobj), [0,0,0,0,-5,0,-5,-5],[0,0,5,0,5,5,5,5], jac='2-point', hess=BFGS())\n",
    "    '''\n",
    "    if constraint==True:\n",
    "        nonlinear_constraint = NonlinearConstraint(lambda x: cons_f(x, likobj), [0,0,0,0,-inf,0],[0,0,inf,0,inf,inf], jac='2-point', hess=BFGS())\n",
    "    \n",
    "    else:\n",
    "        nonlinear_constraint = NonlinearConstraint(lambda x: cons_f(x, likobj), [-inf,-inf,-inf,-inf,-inf,-inf],[inf,inf,inf,inf,inf,inf], jac='2-point', hess=BFGS())\n",
    "\n",
    "    '''\n",
    "    eq_cons = {'type': 'eq',\n",
    "                'fun' : lambda x: np.array([cons_f(x, likobj)[0],cons_f(x, likobj)[1],cons_f(x, likobj)[3]])}\n",
    "    ineq_cons = {'type': 'ineq',\n",
    "                'fun' : lambda x: np.array([cons_f(x, likobj)[2],cons_f(x, likobj)[5]])}\n",
    "    if constraint==True:\n",
    "        nonlinear_constraint=[nonlinear_constraint]\n",
    "    else:\n",
    "        nonlinear_constraint=[]\n",
    "\n",
    "\n",
    "    if bounds==True:\n",
    "        bounds=bounds_acts\n",
    "    else:\n",
    "        bounds=None\n",
    "\n",
    "\n",
    "    try:\n",
    "        with gptsettings.fast_computations(log_prob=False):\n",
    "            return minimize(\n",
    "                fun = likobj.fun,\n",
    "                x0 = theta0,\n",
    "                args=(True) if jac else (False),\n",
    "\n",
    "                method = method,\n",
    "                jac=jac,\n",
    "                bounds=bounds,\n",
    "                constraints= nonlinear_constraint,\n",
    "                #constraints=[eq_cons, ineq_cons],\n",
    "                options= options \n",
    "            )\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        if isinstance(e,NotPSDError) or isinstance(e, NanError):\n",
    "            # Unstable hyperparameter configuration. This can happen if the \n",
    "            # initial starting point is bad. \n",
    "            return e\n",
    "        else:\n",
    "            # There is some other issue, most likely with the inputs supplied\n",
    "            # by the user. Raise error to indicate the problematic part.\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#fit_model_scipy(self,add_prior,num_restarts,theta0_list,jac, options,n_jobs,method ,constraint,bounds,regularization_parameter)\n",
    "\n",
    "\n",
    "\n",
    "add_prior:bool=True\n",
    "num_restarts:int=1\n",
    "theta0_list:Optional[List[np.ndarray]]=None\n",
    "jac:bool=True\n",
    "options:Dict={}\n",
    "n_jobs = 1\n",
    "method = 'L-BFGS-B'\n",
    "constraint=False\n",
    "bounds=False\n",
    "regularization_parameter:List[int]=[0,0]\n",
    "\n",
    "if method == 'L-BFGS-B':\n",
    "        defaults = {'ftol':1e-6,'gtol':1e-5,'maxfun':5000,'maxiter':2000}\n",
    "\n",
    "if len(options) > 0:\n",
    "        for key in options.keys():\n",
    "            if key not in defaults.keys():\n",
    "                raise RuntimeError('Unknown option %s!'%key)\n",
    "            defaults[key] = options[key]\n",
    "\n",
    "likobj = MLLObjective(gp_model,add_prior,regularization_parameter)\n",
    "\n",
    "\n",
    "if theta0_list is None:\n",
    "        theta0_list = [likobj.pack_parameters()]\n",
    "        if num_restarts > -1:\n",
    "            theta0_list.extend([_sample_from_prior(gp_model) for _ in range(num_restarts+1)])\n",
    "            #theta0_list.pop(0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36,)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = OrderedDict([\n",
    "            (n,p) for n,p in likobj.model.named_parameters() if p.requires_grad\n",
    "        ])\n",
    "#parameters       \n",
    "tmp = np.concatenate([parameters[n].cpu().data.numpy().ravel() for n in parameters])\n",
    "tmp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('latent[5]',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.5125,  0.4809,  0.2351],\n",
       "          [ 0.0534,  0.0958, -0.4216]], requires_grad=True)),\n",
       " ('likelihood.raw_task_noises',\n",
       "  Parameter containing:\n",
       "  tensor([0., 0., 0., 0., 0., 0.], requires_grad=True)),\n",
       " ('likelihood.raw_noise',\n",
       "  Parameter containing:\n",
       "  tensor([-9.2104], requires_grad=True)),\n",
       " ('mean_module.base_means.0.constant',\n",
       "  Parameter containing:\n",
       "  tensor([0.], requires_grad=True)),\n",
       " ('mean_module.base_means.1.constant',\n",
       "  Parameter containing:\n",
       "  tensor([0.], requires_grad=True)),\n",
       " ('mean_module.base_means.2.constant',\n",
       "  Parameter containing:\n",
       "  tensor([0.], requires_grad=True)),\n",
       " ('mean_module.base_means.3.constant',\n",
       "  Parameter containing:\n",
       "  tensor([0.], requires_grad=True)),\n",
       " ('mean_module.base_means.4.constant',\n",
       "  Parameter containing:\n",
       "  tensor([0.], requires_grad=True)),\n",
       " ('mean_module.base_means.5.constant',\n",
       "  Parameter containing:\n",
       "  tensor([0.], requires_grad=True)),\n",
       " ('covar_module.task_covar_module.covar_factor',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.1527],\n",
       "          [ 0.8540],\n",
       "          [ 0.3751],\n",
       "          [ 0.1679],\n",
       "          [ 0.1800],\n",
       "          [ 0.1174]], requires_grad=True)),\n",
       " ('covar_module.task_covar_module.raw_var',\n",
       "  Parameter containing:\n",
       "  tensor([-1.3898, -0.5516, -1.3841,  1.5158, -1.3848,  0.6754],\n",
       "         requires_grad=True)),\n",
       " ('covar_module.data_covar_module.kernels.0.raw_lengthscale',\n",
       "  Parameter containing:\n",
       "  tensor([[0.5413]])),\n",
       " ('covar_module.data_covar_module.kernels.1.raw_lengthscale',\n",
       "  Parameter containing:\n",
       "  tensor([[0., 0., 0., 0., 0.]], requires_grad=True))]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(n,p) for (n,p) in gp_model.named_parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('latent[5]', torch.Size([2, 3])),\n",
       "             ('likelihood.raw_task_noises', torch.Size([6])),\n",
       "             ('likelihood.raw_noise', torch.Size([1])),\n",
       "             ('mean_module.base_means.0.constant', torch.Size([1])),\n",
       "             ('mean_module.base_means.1.constant', torch.Size([1])),\n",
       "             ('mean_module.base_means.2.constant', torch.Size([1])),\n",
       "             ('mean_module.base_means.3.constant', torch.Size([1])),\n",
       "             ('mean_module.base_means.4.constant', torch.Size([1])),\n",
       "             ('mean_module.base_means.5.constant', torch.Size([1])),\n",
       "             ('covar_module.task_covar_module.covar_factor',\n",
       "              torch.Size([6, 1])),\n",
       "             ('covar_module.task_covar_module.raw_var', torch.Size([6])),\n",
       "             ('covar_module.data_covar_module.kernels.1.raw_lengthscale',\n",
       "              torch.Size([1, 5]))])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta0 = theta0_list[0]\n",
    "likobj.param_shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "        x = theta0\n",
    "        i = 0\n",
    "        named_parameters = OrderedDict()\n",
    "        for n in likobj.param_shapes:\n",
    "            #print(n)\n",
    "            param_len = reduce(lambda x,y: x*y, likobj.param_shapes[n])\n",
    "            #print('param_len',param_len)\n",
    "            # slice out a section of this length\n",
    "            param = x[i:i+param_len]\n",
    "            # reshape according to this size, and cast to torch\n",
    "            param = param.reshape(*likobj.param_shapes[n])\n",
    "            named_parameters[n] = torch.from_numpy(param).to(**tkwargs)\n",
    "            # update index\n",
    "            i += param_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(141.3231512264697,\n",
       " array([ 3.49273872, -5.30238485,  1.6735425 ,  3.23938227, -0.0525011 ,\n",
       "        -2.75574279,  5.01273203,  5.63015842,  6.08272457,  5.79794407,\n",
       "         4.72798491,  6.07177448, -0.80268836, -2.20350766, -0.19663157,\n",
       "        -2.63312912, -0.87176365, -1.85206854, -0.62226838, -1.6824882 ,\n",
       "         1.1328187 , -0.17626889, -0.50256026,  2.06249142,  0.19219288,\n",
       "         0.42347366,  1.11890411,  0.625745  ,  1.21735036,  0.34964919,\n",
       "         1.18314683,  4.22098923,  0.33333334,  0.33333334,  0.33333334,\n",
       "         7.33378696]))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "likobj.fun(theta0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.3834844e-01, -5.0322598e-01, -1.7122632e-01,  5.7237506e-01,\n",
       "       -6.8722069e-02, -7.2514474e-02,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "       -9.2104406e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00, -1.3558062e+00,\n",
       "        9.1286159e-01,  4.3602367e-03, -3.6886990e-01,  1.7588873e+00,\n",
       "        1.8035872e-01,  3.1083274e-01, -9.2658818e-02, -5.6834060e-01,\n",
       "        1.8647309e-01, -3.4179326e-02, -2.2391362e-01,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta0_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.49681157,  1.0336995 , -1.8137964 ,  0.51856875, -1.0298015 ,\n",
       "       -2.500112  , -5.153781  , -4.830361  , -1.4799149 , -3.9835544 ,\n",
       "       -3.6200075 ,  0.96849537], dtype=float32)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta0_list[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 0 into shape (1,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m theta0 \u001b[38;5;241m=\u001b[39m theta0_list[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m----> 2\u001b[0m \u001b[43m_fit_model_from_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlikobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtheta0\u001b[49m\u001b[43m,\u001b[49m\u001b[43mjac\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdefaults\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43mconstraint\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[23], line 48\u001b[0m, in \u001b[0;36m_fit_model_from_state\u001b[0;34m(likobj, theta0, jac, options, method, constraint, bounds)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit_model_from_state\u001b[39m(likobj,theta0,jac,options, method \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrust-constr\u001b[39m\u001b[38;5;124m'\u001b[39m,constraint\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,bounds\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m---> 48\u001b[0m     \u001b[38;5;28mmin\u001b[39m, \u001b[38;5;28mmax\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mget_bounds\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlikobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtheta0\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m     bounds_acts \u001b[38;5;241m=\u001b[39m Bounds(\u001b[38;5;28mmin\u001b[39m, \u001b[38;5;28mmax\u001b[39m)\n\u001b[1;32m     50\u001b[0m     nonlinear_constraint \u001b[38;5;241m=\u001b[39m NonlinearConstraint(\u001b[38;5;28;01mlambda\u001b[39;00m x: cons_f(x, likobj), [\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m5\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m5\u001b[39m,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m5\u001b[39m],[\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m5\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m5\u001b[39m,\u001b[38;5;241m5\u001b[39m,\u001b[38;5;241m5\u001b[39m,\u001b[38;5;241m5\u001b[39m], jac\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2-point\u001b[39m\u001b[38;5;124m'\u001b[39m, hess\u001b[38;5;241m=\u001b[39mBFGS())\n",
      "Cell \u001b[0;32mIn[23], line 11\u001b[0m, in \u001b[0;36mget_bounds\u001b[0;34m(likobj, theta)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_bounds\u001b[39m(likobj, theta):\n\u001b[0;32m---> 11\u001b[0m     dic \u001b[38;5;241m=\u001b[39m \u001b[43mlikobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munpack_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtheta\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     minn \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     14\u001b[0m     maxx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty(\u001b[38;5;241m0\u001b[39m)\n",
      "Cell \u001b[0;32mIn[21], line 80\u001b[0m, in \u001b[0;36mMLLObjective.unpack_parameters\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     78\u001b[0m param \u001b[38;5;241m=\u001b[39m x[i:i\u001b[38;5;241m+\u001b[39mparam_len]\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m# reshape according to this size, and cast to torch\u001b[39;00m\n\u001b[0;32m---> 80\u001b[0m param \u001b[38;5;241m=\u001b[39m \u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_shapes\u001b[49m\u001b[43m[\u001b[49m\u001b[43mn\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m named_parameters[n] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(param)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtkwargs)\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m# update index\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 0 into shape (1,)"
     ]
    }
   ],
   "source": [
    "theta0 = theta0_list[1]\n",
    "_fit_model_from_state(likobj,theta0,jac,defaults,method,constraint,bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 0 into shape (1,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m out \u001b[38;5;241m=\u001b[39m [_fit_model_from_state(likobj,theta0,jac,defaults,method,constraint,bounds) \u001b[38;5;28;01mfor\u001b[39;00m theta0 \u001b[38;5;129;01min\u001b[39;00m theta0_list]\n",
      "Cell \u001b[0;32mIn[47], line 1\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0m out \u001b[38;5;241m=\u001b[39m [\u001b[43m_fit_model_from_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlikobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtheta0\u001b[49m\u001b[43m,\u001b[49m\u001b[43mjac\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdefaults\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43mconstraint\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m theta0 \u001b[38;5;129;01min\u001b[39;00m theta0_list]\n",
      "Cell \u001b[0;32mIn[23], line 48\u001b[0m, in \u001b[0;36m_fit_model_from_state\u001b[0;34m(likobj, theta0, jac, options, method, constraint, bounds)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit_model_from_state\u001b[39m(likobj,theta0,jac,options, method \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrust-constr\u001b[39m\u001b[38;5;124m'\u001b[39m,constraint\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,bounds\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m---> 48\u001b[0m     \u001b[38;5;28mmin\u001b[39m, \u001b[38;5;28mmax\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mget_bounds\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlikobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtheta0\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m     bounds_acts \u001b[38;5;241m=\u001b[39m Bounds(\u001b[38;5;28mmin\u001b[39m, \u001b[38;5;28mmax\u001b[39m)\n\u001b[1;32m     50\u001b[0m     nonlinear_constraint \u001b[38;5;241m=\u001b[39m NonlinearConstraint(\u001b[38;5;28;01mlambda\u001b[39;00m x: cons_f(x, likobj), [\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m5\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m5\u001b[39m,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m5\u001b[39m],[\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m5\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m5\u001b[39m,\u001b[38;5;241m5\u001b[39m,\u001b[38;5;241m5\u001b[39m,\u001b[38;5;241m5\u001b[39m], jac\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2-point\u001b[39m\u001b[38;5;124m'\u001b[39m, hess\u001b[38;5;241m=\u001b[39mBFGS())\n",
      "Cell \u001b[0;32mIn[23], line 11\u001b[0m, in \u001b[0;36mget_bounds\u001b[0;34m(likobj, theta)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_bounds\u001b[39m(likobj, theta):\n\u001b[0;32m---> 11\u001b[0m     dic \u001b[38;5;241m=\u001b[39m \u001b[43mlikobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munpack_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtheta\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     minn \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     14\u001b[0m     maxx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty(\u001b[38;5;241m0\u001b[39m)\n",
      "Cell \u001b[0;32mIn[21], line 80\u001b[0m, in \u001b[0;36mMLLObjective.unpack_parameters\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     78\u001b[0m param \u001b[38;5;241m=\u001b[39m x[i:i\u001b[38;5;241m+\u001b[39mparam_len]\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m# reshape according to this size, and cast to torch\u001b[39;00m\n\u001b[0;32m---> 80\u001b[0m param \u001b[38;5;241m=\u001b[39m \u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_shapes\u001b[49m\u001b[43m[\u001b[49m\u001b[43mn\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m named_parameters[n] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(param)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtkwargs)\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m# update index\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 0 into shape (1,)"
     ]
    }
   ],
   "source": [
    "out = [_fit_model_from_state(likobj,theta0,jac,defaults,method,constraint,bounds) for theta0 in theta0_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 0 into shape (1,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_model_from_state\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlikobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtheta0\u001b[49m\u001b[43m,\u001b[49m\u001b[43mjac\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdefaults\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43mconstraint\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtheta0\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtheta0_list\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/gpplus-py39/lib/python3.9/site-packages/joblib/parallel.py:1863\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1861\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1862\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1863\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1865\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1866\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1867\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1868\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1869\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[1;32m   1870\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[0;32m~/anaconda3/envs/gpplus-py39/lib/python3.9/site-packages/joblib/parallel.py:1792\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1790\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1791\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1792\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1793\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1794\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "Cell \u001b[0;32mIn[23], line 48\u001b[0m, in \u001b[0;36m_fit_model_from_state\u001b[0;34m(likobj, theta0, jac, options, method, constraint, bounds)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit_model_from_state\u001b[39m(likobj,theta0,jac,options, method \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrust-constr\u001b[39m\u001b[38;5;124m'\u001b[39m,constraint\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,bounds\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m---> 48\u001b[0m     \u001b[38;5;28mmin\u001b[39m, \u001b[38;5;28mmax\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mget_bounds\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlikobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtheta0\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m     bounds_acts \u001b[38;5;241m=\u001b[39m Bounds(\u001b[38;5;28mmin\u001b[39m, \u001b[38;5;28mmax\u001b[39m)\n\u001b[1;32m     50\u001b[0m     nonlinear_constraint \u001b[38;5;241m=\u001b[39m NonlinearConstraint(\u001b[38;5;28;01mlambda\u001b[39;00m x: cons_f(x, likobj), [\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m5\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m5\u001b[39m,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m5\u001b[39m],[\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m5\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m5\u001b[39m,\u001b[38;5;241m5\u001b[39m,\u001b[38;5;241m5\u001b[39m,\u001b[38;5;241m5\u001b[39m], jac\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2-point\u001b[39m\u001b[38;5;124m'\u001b[39m, hess\u001b[38;5;241m=\u001b[39mBFGS())\n",
      "Cell \u001b[0;32mIn[23], line 11\u001b[0m, in \u001b[0;36mget_bounds\u001b[0;34m(likobj, theta)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_bounds\u001b[39m(likobj, theta):\n\u001b[0;32m---> 11\u001b[0m     dic \u001b[38;5;241m=\u001b[39m \u001b[43mlikobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munpack_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtheta\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     minn \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     14\u001b[0m     maxx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty(\u001b[38;5;241m0\u001b[39m)\n",
      "Cell \u001b[0;32mIn[21], line 80\u001b[0m, in \u001b[0;36mMLLObjective.unpack_parameters\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     78\u001b[0m param \u001b[38;5;241m=\u001b[39m x[i:i\u001b[38;5;241m+\u001b[39mparam_len]\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m# reshape according to this size, and cast to torch\u001b[39;00m\n\u001b[0;32m---> 80\u001b[0m param \u001b[38;5;241m=\u001b[39m \u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_shapes\u001b[49m\u001b[43m[\u001b[49m\u001b[43mn\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m named_parameters[n] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(param)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtkwargs)\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m# update index\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 0 into shape (1,)"
     ]
    }
   ],
   "source": [
    "out = Parallel(n_jobs=1,verbose=0)(\n",
    "        delayed(_fit_model_from_state)(likobj,theta0,jac,defaults, method,constraint,bounds) \\\n",
    "            for theta0 in theta0_list\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Learning the model's parameters has started ##\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 0 into shape (1,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mgp_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 565\u001b[0m, in \u001b[0;36mGP_Plus.fit\u001b[0;34m(self, add_prior, num_restarts, theta0_list, jac, options, n_jobs, method, constraint, bounds, regularization_parameter, optim_type)\u001b[0m\n\u001b[1;32m    563\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m optim_type\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscipy\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    564\u001b[0m     n_jobs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 565\u001b[0m     \u001b[43mfit_model_scipy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43madd_prior\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnum_restarts\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtheta0_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43mjac\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43mconstraint\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\u001b[43mregularization_parameter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m optim_type\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontinuation\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    567\u001b[0m     noise_tune2(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,add_prior\u001b[38;5;241m=\u001b[39madd_prior,\n\u001b[1;32m    568\u001b[0m     num_restarts\u001b[38;5;241m=\u001b[39mnum_restarts,criterion\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNLL\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    569\u001b[0m     initial_noise_var\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    577\u001b[0m     bounds\u001b[38;5;241m=\u001b[39mbounds\n\u001b[1;32m    578\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/gpplus-py39/lib/python3.9/site-packages/gpplus/optim/mll_scipy.py:289\u001b[0m, in \u001b[0;36mfit_model_scipy\u001b[0;34m(model, add_prior, num_restarts, theta0_list, jac, options, n_jobs, method, constraint, bounds, regularization_parameter)\u001b[0m\n\u001b[1;32m    285\u001b[0m         theta0_list\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;241m0\u001b[39m)                                                                     \n\u001b[1;32m    287\u001b[0m set_loky_pickler(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdill\u001b[39m\u001b[38;5;124m\"\u001b[39m) \n\u001b[0;32m--> 289\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_model_from_state\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlikobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtheta0\u001b[49m\u001b[43m,\u001b[49m\u001b[43mjac\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdefaults\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43mconstraint\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtheta0\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtheta0_list\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    293\u001b[0m set_loky_pickler(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpickle\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    295\u001b[0m nlls_opt \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39minf \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(res,\u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01melse\u001b[39;00m res\u001b[38;5;241m.\u001b[39mfun \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m out]\n",
      "File \u001b[0;32m~/anaconda3/envs/gpplus-py39/lib/python3.9/site-packages/joblib/parallel.py:1863\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1861\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1862\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1863\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1865\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1866\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1867\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1868\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1869\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[1;32m   1870\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[0;32m~/anaconda3/envs/gpplus-py39/lib/python3.9/site-packages/joblib/parallel.py:1792\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1790\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1791\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1792\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1793\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1794\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[0;32m~/anaconda3/envs/gpplus-py39/lib/python3.9/site-packages/gpplus/optim/mll_scipy.py:189\u001b[0m, in \u001b[0;36m_fit_model_from_state\u001b[0;34m(likobj, theta0, jac, options, method, constraint, bounds)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit_model_from_state\u001b[39m(likobj,theta0,jac,options, method \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrust-constr\u001b[39m\u001b[38;5;124m'\u001b[39m,constraint\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,bounds\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 189\u001b[0m     \u001b[38;5;28mmin\u001b[39m, \u001b[38;5;28mmax\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mget_bounds\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlikobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtheta0\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    190\u001b[0m     bounds_acts \u001b[38;5;241m=\u001b[39m Bounds(\u001b[38;5;28mmin\u001b[39m, \u001b[38;5;28mmax\u001b[39m)\n\u001b[1;32m    191\u001b[0m     nonlinear_constraint \u001b[38;5;241m=\u001b[39m NonlinearConstraint(\u001b[38;5;28;01mlambda\u001b[39;00m x: cons_f(x, likobj), [\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m5\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m5\u001b[39m,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m5\u001b[39m],[\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m5\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m5\u001b[39m,\u001b[38;5;241m5\u001b[39m,\u001b[38;5;241m5\u001b[39m,\u001b[38;5;241m5\u001b[39m], jac\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2-point\u001b[39m\u001b[38;5;124m'\u001b[39m, hess\u001b[38;5;241m=\u001b[39mBFGS())\n",
      "File \u001b[0;32m~/anaconda3/envs/gpplus-py39/lib/python3.9/site-packages/gpplus/optim/mll_scipy.py:151\u001b[0m, in \u001b[0;36mget_bounds\u001b[0;34m(likobj, theta)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_bounds\u001b[39m(likobj, theta):\n\u001b[0;32m--> 151\u001b[0m     dic \u001b[38;5;241m=\u001b[39m \u001b[43mlikobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munpack_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtheta\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    153\u001b[0m     minn \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    154\u001b[0m     maxx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/gpplus-py39/lib/python3.9/site-packages/gpplus/optim/mll_scipy.py:96\u001b[0m, in \u001b[0;36mMLLObjective.unpack_parameters\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     94\u001b[0m param \u001b[38;5;241m=\u001b[39m x[i:i\u001b[38;5;241m+\u001b[39mparam_len]\n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m# reshape according to this size, and cast to torch\u001b[39;00m\n\u001b[0;32m---> 96\u001b[0m param \u001b[38;5;241m=\u001b[39m \u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_shapes\u001b[49m\u001b[43m[\u001b[49m\u001b[43mn\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m named_parameters[n] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(param)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtkwargs)\n\u001b[1;32m     98\u001b[0m \u001b[38;5;66;03m# update index\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 0 into shape (1,)"
     ]
    }
   ],
   "source": [
    "gp_model.fit(bounds=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_x_sum.shape torch.Size([16, 6])\n",
      "x_new.shape torch.Size([16, 7])\n",
      "mean_x.shape torch.Size([16, 6])\n",
      "covar_x.shape torch.Size([96, 96])\n",
      "ensemble_mean.shape torch.Size([16, 6])\n",
      "ensemble_covar.shape torch.Size([96, 96])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([96, 96])"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gp_model = GP_Plus(t_train_x, t_train_y, qual_ind_lev=qual_ind_lev)\n",
    "#gp_model.mean_module(t_train_x).shape\n",
    "output = gp_model(gp_model.train_inputs[0])\n",
    "output.covariance_matrix.shape\n",
    "#out = gp_model.likelihood(output).log_prob(gp_model.train_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultitaskMultivariateNormal(loc: torch.Size([96]))"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gp_model.likelihood(output)\n",
    "#gp_model.likelihood(output).covariance_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1945, -0.3865],\n",
      "        [-0.1005,  0.0977],\n",
      "        [ 0.2374,  0.2996],\n",
      "        [ 0.2374,  0.2996],\n",
      "        [ 0.2374,  0.2996],\n",
      "        [-0.1005,  0.0977],\n",
      "        [-0.1945, -0.3865],\n",
      "        [-0.1005,  0.0977],\n",
      "        [ 0.2374,  0.2996],\n",
      "        [ 0.2374,  0.2996],\n",
      "        [ 0.2374,  0.2996],\n",
      "        [ 0.2374,  0.2996],\n",
      "        [-0.1005,  0.0977],\n",
      "        [-0.1945, -0.3865],\n",
      "        [-0.1005,  0.0977],\n",
      "        [-0.1945, -0.3865]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x_train_cate_zeta = gp_model.transform_categorical(t_train_x[:,5].clone().type(torch.int64).to('cpu'),perm_dict = gp_model.perm_dict[0], zeta = gp_model.zeta[0])\n",
    "x_train_cate_latent = gp_model.A_matrix[0](x_train_cate_zeta.float().to('cpu'))\n",
    "print(x_train_cate_latent)\n",
    "#ls_x_train_cate_latent.append(x_train_cate_latent.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.2374,  0.2996],\n",
      "        [-0.1005,  0.0977],\n",
      "        [ 0.2374,  0.2996],\n",
      "        [-0.1945, -0.3865],\n",
      "        [-0.1945, -0.3865]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "t_test_x = torch.Tensor(scaled_X_test)\n",
    "x_test_cate_zeta = gp_model.transform_categorical(t_test_x[:,5].clone().type(torch.int64).to('cpu'),perm_dict = gp_model.perm_dict[0], zeta = gp_model.zeta[0])\n",
    "x_test_cate_latent = gp_model.A_matrix[0](x_test_cate_zeta.float().to('cpu'))\n",
    "print(x_test_cate_latent)\n",
    "#ls_x_test_cate_latent.append(x_test_cate_latent.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('data/train_cate_latent.npy',np.array(ls_x_train_cate_latent),allow_pickle= True)\n",
    "np.save('data/test_cate_latent.npy',np.array(ls_x_test_cate_latent),allow_pickle= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "A = model.A_matrix\n",
    "zeta = model.zeta[i]\n",
    "positions = A[i](x=zeta.float().to('cpu'))\n",
    "#positions.shape\n",
    "level = torch.max(model.perm[i]+1, axis = 0)[0].tolist()\n",
    "levels = level\n",
    "perm = model.perm[i]\n",
    "constraints_flag=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7605, -0.1799],\n",
       "        [ 0.3098,  0.0732],\n",
       "        [ 0.4512,  0.1057]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 10])\n",
      "torch.Size([100, 2])\n"
     ]
    }
   ],
   "source": [
    "#x_cate_zeta is the prior embedding for all categorical observations\n",
    "#x_cate_latent are the latent variable values\n",
    "i = 0\n",
    "x_cate_zeta= transform_categorical(x=x[:,qual_kernel_columns[i]].clone().type(torch.int64).to('cpu'), \n",
    "                        perm_dict = self_perm_dict[i], zeta = self_zeta[i])\n",
    "x_cate_latent = model.A_matrix[i](x_cate_zeta.float().to('cpu'))\n",
    "print(x_cate_zeta.shape)\n",
    "print(x_cate_latent.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.sort(\n",
       "values=tensor([-2.6171e-01, -2.1992e-01, -1.9734e-01, -1.8409e-01, -1.8275e-01,\n",
       "        -1.6436e-01, -1.5461e-01, -1.5303e-01, -8.1902e-02, -6.7315e-02,\n",
       "        -5.5410e-02, -4.8922e-02, -4.1810e-02, -3.9174e-02, -3.7587e-02,\n",
       "        -1.3620e-02, -1.9610e-05,  2.2210e-02,  2.4985e-02,  3.5810e-02,\n",
       "         3.7321e-02,  4.8047e-02,  5.3546e-02,  6.6776e-02,  6.8133e-02,\n",
       "         8.6527e-02,  8.9838e-02,  9.6275e-02,  9.7861e-02,  1.0261e-01,\n",
       "         1.2567e-01,  1.5986e-01,  1.7444e-01,  1.9284e-01,  2.0258e-01,\n",
       "         2.0417e-01,  2.4362e-01,  2.5722e-01,  2.7274e-01,  2.8069e-01,\n",
       "         2.9528e-01,  3.1367e-01,  3.2342e-01,  3.4708e-01,  4.7904e-01,\n",
       "         4.9264e-01,  5.5943e-01,  5.8249e-01], grad_fn=<SortBackward0>),\n",
       "indices=tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
       "        36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]))"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_cate_latent.unique().sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.sort(\n",
       "values=tensor([-2.6171e-01, -2.1992e-01, -1.9734e-01, -1.8409e-01, -1.8275e-01,\n",
       "        -1.6436e-01, -1.5461e-01, -1.5303e-01, -8.1902e-02, -6.7315e-02,\n",
       "        -5.5410e-02, -4.8922e-02, -4.1810e-02, -3.9174e-02, -3.7587e-02,\n",
       "        -1.3620e-02, -1.9610e-05,  2.2210e-02,  2.4985e-02,  3.5810e-02,\n",
       "         3.7321e-02,  4.8047e-02,  5.3546e-02,  6.6776e-02,  6.8133e-02,\n",
       "         8.6527e-02,  8.9838e-02,  9.6275e-02,  9.7861e-02,  1.0261e-01,\n",
       "         1.2567e-01,  1.5986e-01,  1.7444e-01,  1.9284e-01,  2.0258e-01,\n",
       "         2.0417e-01,  2.4362e-01,  2.5722e-01,  2.7274e-01,  2.8069e-01,\n",
       "         2.9528e-01,  3.1367e-01,  3.2342e-01,  3.2401e-01,  3.2500e-01,\n",
       "         3.4708e-01,  4.7904e-01,  4.9264e-01,  5.5943e-01,  5.8249e-01],\n",
       "       grad_fn=<SortBackward0>),\n",
       "indices=tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
       "        36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]))"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positions.unique().sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lk/7pxby8y145q_94s_03vn5cv80000gn/T/ipykernel_4050/3697863923.py:6: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap(obj)`` instead.\n",
      "  cmap = cm.get_cmap('tab20', 20)  # Get a colormap from matplotlib, 'tab20' has nice distinct colors\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "type = 'cat'\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "colors_base = ['deeppink', 'gold', 'darkorange', 'gray', 'orangered', 'blue']\n",
    "cmap = cm.get_cmap('tab20', 20)  # Get a colormap from matplotlib, 'tab20' has nice distinct colors\n",
    "colors = colors_base  + [cmap(i) for i in range(len(colors_base), 20)]\n",
    "marker = ['X','o','s',\"v\", 'p', 'v', '^', '<', '>','*', 'h', 'H', '+', 'x', 'X', 'D', 'd', '|', '_','1', '2', '3', '4',]\n",
    "positions = positions.cpu().detach().numpy()\n",
    "\n",
    "if len(levels) > 1:\n",
    "        fig,axs = plt.subplots(1, len(levels),figsize=(10,4))\n",
    "        for j in range(len(levels)):\n",
    "            for i in range(levels[j]):\n",
    "                index = torch.where(perm[:,j] == i) \n",
    "                if i<=10:\n",
    "                    fontsize=5\n",
    "                    s_size=100\n",
    "                else:\n",
    "                    fontsize=5\n",
    "                    s_size=100\n",
    "                if  type=='mf':\n",
    "                    axs[j].scatter(positions[index][...,0], positions[index][...,1], label = legend[i], color = colors[i], marker=marker[i],s=s_size,alpha=.6)#marker=r'$\\clubsuit$'\n",
    "                    plt.xlabel(r'$z_1$',labelpad=0,rotation=0,fontsize=20)\n",
    "                    plt.ylabel(r'$z_2$',labelpad=10,rotation=0, fontsize=20)\n",
    "                \n",
    "                elif type=='cat':\n",
    "                    axs[j].scatter(positions[index][...,0], positions[index][...,1], label = 'level' + str(i+1), color = colors[i], marker=marker[i],s=s_size,alpha=.6)#marker=r'$\\clubsuit$'\n",
    "                    axs[j].set_xlabel(r'$h_1$', labelpad=0, fontsize=20)\n",
    "                    axs[j].set_ylabel(r'$h_2$', labelpad=5,fontsize=20)\n",
    "                else:\n",
    "                    raise ValueError(\"type should be either 'mf' or cat:\")\n",
    "                # plt.tight_layout()\n",
    "                # axs[j].legend()\n",
    "                axs[j].legend(loc='upper right', fontsize='xx-small')\n",
    "\n",
    "                tempxi = np.min(positions[...,0])-0.2 * (np.abs(np.min(positions[...,0])) +5)\n",
    "                tempxx = np.max(positions[...,0]) + 0.2 * (np.abs(np.max(positions[...,0])) +5)\n",
    "                tempyi = np.min(positions[...,1])-0.2 * (np.abs(np.min(positions[...,1])) +5)\n",
    "                tempyx = np.max(positions[...,1]) + 0.2 * (np.abs(np.max(positions[...,1])) +5)\n",
    "                axs[j].set_xlim(tempxi, tempxx)\n",
    "                axs[j].set_ylim(tempyi, tempyx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization and evaluation: As this is a mixed-space problem, we use the 'model.visualize_latent()' command to visualize the learnt latent space of the categorical features.\n",
    "\n",
    "Then, the accuarcy of the model is evaluated using 'model.evaluation(); command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.visualize_latent()\n",
    "#model.evaluation(Utest, ytest)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local-gp-mac-no-tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
