{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "# Suppress all warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from pandas import factorize\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "os.chdir('/Users/chenya68/Documents/GitHub/BFO')\n",
    "df_x = pd.read_excel('data/harpoon-doe.xlsx',sheet_name = 0, usecols = [1,2,3,4])\n",
    "df_x.columns = [re.sub('[^A-Za-z0-9Δ]+', '_', element) for element in df_x.columns]\n",
    "#print(len(df_x))\n",
    "#df_x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_y_1 = pd.read_excel('data/harpoon-doe.xlsx',sheet_name = 1, usecols = [1,4,7],skiprows = lambda x: x in [1])\n",
    "df_y_1.columns = [re.sub('[^A-Za-z0-9Δ]+', '_', element) for element in df_y_1.columns]\n",
    "#print(len(df_y_1))\n",
    "#print(df_y_1.head())\n",
    "\n",
    "df_y_2 = pd.read_excel('data/harpoon-doe.xlsx',sheet_name = 1, usecols = [2,5,8],skiprows = lambda x: x in [1])\n",
    "df_y_2.columns = df_y_1.columns\n",
    "#print(len(df_y_2))\n",
    "#print(df_y_2.head())\n",
    "\n",
    "df_y_3 = pd.read_excel('data/harpoon-doe.xlsx',sheet_name = 1, usecols = [3,6,9],skiprows = lambda x: x in [1])\n",
    "df_y_3.columns = df_y_1.columns\n",
    "#print(len(df_y_3))\n",
    "#print(df_y_3.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_feature01 = list(df_x.columns)\n",
    "cols_target = list(df_y_1.columns)[0:2]\n",
    "#cols_target = list(df_y_1.columns)\n",
    "cols_cate = ['Buffer_Type',\n",
    " 'Sugar_Salt',\n",
    " 'Additive']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_Dimer_HMW_</th>\n",
       "      <th>_Monomer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.4</td>\n",
       "      <td>95.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.8</td>\n",
       "      <td>97.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.2</td>\n",
       "      <td>94.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.4</td>\n",
       "      <td>96.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.2</td>\n",
       "      <td>93.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>3.3</td>\n",
       "      <td>92.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>3.7</td>\n",
       "      <td>92.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>3.6</td>\n",
       "      <td>92.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>4.2</td>\n",
       "      <td>90.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>4.4</td>\n",
       "      <td>90.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>72 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    _Dimer_HMW_  _Monomer\n",
       "0           2.4      95.1\n",
       "1           0.8      97.3\n",
       "2           3.2      94.3\n",
       "3           1.4      96.7\n",
       "4           4.2      93.3\n",
       "..          ...       ...\n",
       "67          3.3      92.3\n",
       "68          3.7      92.3\n",
       "69          3.6      92.2\n",
       "70          4.2      90.8\n",
       "71          4.4      90.3\n",
       "\n",
       "[72 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_df_y_long = pd.concat((df_y_1[cols_target],df_y_2[cols_target],df_y_3[cols_target]),axis = 0)\n",
    "total_df_y_long.reset_index(inplace = True, drop = True)\n",
    "total_df_y_long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_y_1.columns = [c+'_1' for c in df_y_1.columns]\n",
    "df_y_2.columns = [c+'_2' for c in df_y_2.columns]\n",
    "df_y_3.columns = [c+'_3' for c in df_y_3.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_target_new = ['_Dimer_HMW_1', '_Monomer_1',\n",
    "       '_Dimer_HMW_2', '_Monomer_2', \n",
    "       '_Dimer_HMW_3','_Monomer_3']\n",
    "\n",
    "arr_Y = np.concatenate((df_y_1.to_numpy()[:,:2],\n",
    "                            df_y_2.to_numpy()[:,:2],\n",
    "                            df_y_3.to_numpy()[:,:2]),axis = 1)\n",
    "total_df_y_wide = pd.DataFrame(arr_Y,columns= cols_target_new)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pH</th>\n",
       "      <th>Buffer_Type_label</th>\n",
       "      <th>Sugar_Salt_label</th>\n",
       "      <th>Additive_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    pH  Buffer_Type_label  Sugar_Salt_label  Additive_label\n",
       "0  4.5                  0                 0               0\n",
       "1  4.5                  0                 0               1\n",
       "2  5.0                  0                 0               0\n",
       "3  5.0                  0                 0               1\n",
       "4  5.0                  0                 1               1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#convert categorical columns to labels\n",
    "for x_name in cols_cate:\n",
    "    labels, categories = factorize(df_x[x_name])\n",
    "    df_x[x_name+\"_label\"] = labels\n",
    "df_x.drop(cols_cate,axis = 1,inplace = True)\n",
    "display(df_x.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_model = ['simpleGP','multi-task-single-output','multi-task-multi-output','multi-task-multi-input-multi-output']\n",
    "ls_x_scale = ['no-x-scale','x-minmax','x-stand','x-robust']\n",
    "ls_y_scale = ['no-y-scale','y-minmax','y-stand','y-robust']\n",
    "ls_cate_transform = ['label','ohe','LVGP','full-LMGP','partial-LMGP']\n",
    "ls_remove_pred_outlier = [0,1]\n",
    "ls_output_rank_option = [1,2]\n",
    "ls_task_rank_option = [1,2,3,4,5,6,7]\n",
    "ls_lik_rank_option = [0,1,2]\n",
    "ls_split_option = ['mix','separate'] #mix: combine all tasks first then do train, test split (could stratify task?) #separate, do train-test-split first, then combine tasks\n",
    "ls_stratify_task = ['not-stratify','stratify-x','stratify-y','stratify-xy']\n",
    "\n",
    "\n",
    "model_option = 'multi-task-multi-output'\n",
    "multi_task_label = 'hier'\n",
    "\n",
    "x_scale_option = 'x-minmax'\n",
    "y_scale_option = 'y-stand'\n",
    "cate_transform_option = 'partial-LMGP'\n",
    "#remove_pred_outlier_option= 0\n",
    "\n",
    "output_rank_option = 2 #if 0, no correlation between output\n",
    "task_rank_option = 3#if 0, no correlation between tasks\n",
    "lik_rank_option = 1\n",
    "split_option = 'mix'\n",
    "stratify_option = 'stratify-x'\n",
    "\n",
    "noise_option = 0 #noise percentage\n",
    "\n",
    "model_label = model_option\n",
    "x_scale_label = x_scale_option\n",
    "y_scale_label = y_scale_option\n",
    "cate_transform_label = 'cate_transform_'+cate_transform_option\n",
    "#remove_pred_outlier_label = 'remove_pred_outlier_'+str(remove_pred_outlier_option)\n",
    "output_rank_label = 'output_rank_'+str(output_rank_option)\n",
    "task_rank_label = 'task_rank_'+str(task_rank_option)\n",
    "lik_rank_label = 'lik_rank_'+str(lik_rank_option)\n",
    "split_label = split_option\n",
    "stratify_label = stratify_option\n",
    "if noise_option>0:\n",
    "        noise_label = 'noise_'+str(noise_option)\n",
    "else:\n",
    "        noise_label = ''\n",
    "\n",
    "folder_name = '-'.join([model_label,task_rank_label,output_rank_label,lik_rank_label,x_scale_label,y_scale_label,cate_transform_label,\n",
    "                        split_label,stratify_label,noise_label])\n",
    "\n",
    "figPath = 'output/harpoon-lmgp-adapt/'+folder_name\n",
    "if not os.path.exists(figPath):\n",
    "        print(f'Creating folder {figPath}')\n",
    "        os.makedirs(figPath,exist_ok = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if split_label == 'mix':\n",
    "    ls_X = [df_x.copy(),\n",
    "            df_x.copy(),\n",
    "            df_x.copy()]\n",
    "    \n",
    "    for i,tmp_df_x in enumerate(ls_X):\n",
    "        tmp_df_x['task_ind'] = i\n",
    "\n",
    "    df_X = pd.concat(ls_X)\n",
    "    df_X.reset_index(inplace=True, drop = True)\n",
    "    #print(df_X.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add noisy data to datasets\n",
    "np.random.seed(42)\n",
    "N = len(df_X)\n",
    "\n",
    "if noise_option>0:\n",
    "#create data with noise\n",
    "    df_X_syn = df_X.copy()\n",
    "    for col in df_X.columns.difference(['task_ind']):\n",
    "        df_X_syn[col] = df_X_syn[col] + np.random.normal(0, df_X_syn[col].std(), N) * noise_option\n",
    "\n",
    "    df_Y_syn= total_df_y_long[cols_target].copy()\n",
    "    for col in cols_target:\n",
    "        df_Y_syn[col] = df_Y_syn[col] + np.random.normal(0, df_Y_syn[col].std(), N) * noise_option\n",
    "\n",
    "if noise_option>0:\n",
    "    df_total_X = pd.concat((df_X,df_X_syn))\n",
    "    df_total_Y = pd.concat((total_df_y_long[cols_target],df_Y_syn))\n",
    "else:\n",
    "    df_total_X = df_X\n",
    "    df_total_Y = total_df_y_long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from sklearn.cluster import KMeans\n",
    "ls_n_clusters = [4,2,2]\n",
    "for i,x_name in enumerate(['Buffer_Type_label','Sugar_Salt_label','Additive_label']):\n",
    "    best_n_clusters = ls_n_clusters[i]\n",
    "    km = KMeans(n_clusters=best_n_clusters, random_state=10)\n",
    "    kmeans = km.fit(df_total_X[[x_name]])\n",
    "    df_total_X[x_name] = kmeans.labels_\n",
    "\"\"\"\n",
    "if multi_task_label == 'hier':\n",
    "    df_X_train, df_X_test, df_y_train, df_y_test = train_test_split(df_total_X, df_total_Y, test_size=0.2, \n",
    "                                                                random_state=0, \n",
    "                                                                stratify=df_total_X['task_ind'])\n",
    "\n",
    "else:\n",
    "    df_X_train, df_X_test, df_y_train, df_y_test = train_test_split(df_total_X, df_total_Y, test_size=0.2, \n",
    "                                                                random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing features\n",
    "xct = ColumnTransformer([('x_mm_scaler',MinMaxScaler(),\n",
    "                          df_X_train.columns.difference(['Buffer_Type_label','Sugar_Salt_label','Additive_label','task_ind']))], \n",
    "                         remainder = 'passthrough')\n",
    "\n",
    "scaled_X_train=xct.fit_transform(df_X_train) \n",
    "scaled_X_test=xct.transform(df_X_test)\n",
    "\n",
    "t_train_x = torch.Tensor(scaled_X_train)\n",
    "t_test_x = torch.Tensor(scaled_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['pH', 'Buffer_Type_label', 'Sugar_Salt_label', 'Additive_label',\n",
       "       'task_ind'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "qual_ind_lev = {1: 4, 2:2, 3:2} #2nd, 3rd, and 4th columns are categorical vars. col index: number of levels\n",
    "quant_index = [0]#1st column is continous var\n",
    "task_index = 4#fifth column is task column\n",
    "num_tasks = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scale each task separately\n",
    "\n",
    "scaled_y_train = np.zeros_like(df_y_train.to_numpy())\n",
    "scaled_y_test = np.zeros_like(df_y_test.to_numpy())\n",
    "ls_y_task_scaler = []\n",
    "ls_row_idx_train = []\n",
    "ls_row_idx_test = []\n",
    "\n",
    "if y_scale_label == 'y-robust':\n",
    "    y_scaler = RobustScaler()\n",
    "    scaled_y_train = y_scaler.fit_transform(df_y_train)\n",
    "    scaled_y_test= y_scaler.transform(df_y_test)\n",
    "elif y_scale_label == 'y-stand':\n",
    "    #y_scaler = StandardScaler()\n",
    "    #scaled_y_train = y_scaler.fit_transform(df_y_train)\n",
    "    #scaled_y_test= y_scaler.transform(df_y_test)\n",
    "    for task_ind in range(num_tasks):\n",
    "        y_task_scaler = StandardScaler()\n",
    "        row_idx_train = np.where(df_X_train['task_ind']==task_ind)[0]\n",
    "        ls_row_idx_train.append(row_idx_train)\n",
    "        row_idx_test = np.where(df_X_test['task_ind']==task_ind)[0]\n",
    "        ls_row_idx_test.append(row_idx_test)\n",
    "        scaled_y_train_task =y_task_scaler.fit_transform(df_y_train[df_X_train['task_ind']==task_ind])\n",
    "        scaled_y_test_task= y_task_scaler.transform(df_y_test[df_X_test['task_ind']==task_ind])\n",
    "        ls_y_task_scaler.append(y_task_scaler)\n",
    "        scaled_y_train[row_idx_train] = scaled_y_train_task\n",
    "        scaled_y_test[row_idx_test] = scaled_y_test_task\n",
    "elif y_scale_label == 'y-minmax':\n",
    "    y_scaler = MinMaxScaler()\n",
    "    scaled_y_train = y_scaler.fit_transform(df_y_train)\n",
    "    scaled_y_test= y_scaler.transform(df_y_test)\n",
    "else:\n",
    "    scaled_y_train = df_y_train.to_numpy()\n",
    "    scaled_y_test = df_y_test.to_numpy()\n",
    "\n",
    "t_train_y = torch.Tensor(scaled_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.00000</td>\n",
       "      <td>-0.95998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.95998</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0        1\n",
       "0  1.00000 -0.95998\n",
       "1 -0.95998  1.00000"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculate correlations between two outputs\n",
    "pd.DataFrame(t_train_y.numpy()).corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define GP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gpytorch\n",
    "from gpytorch.priors import NormalPrior,LogNormalPrior,SmoothedBoxPrior,HorseshoePrior\n",
    "from gpytorch.constraints import GreaterThan,Positive\n",
    "\n",
    "\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from typing import Dict,List,Optional\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Basic MTMO class\n",
    "\n",
    "class MultiOutputMultiTaskGP(gpytorch.models.ExactGP):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_X,\n",
    "        train_Y,\n",
    "        data_kernel,\n",
    "        noise_indices,\n",
    "        fix_noise:bool=False,\n",
    "        lb_noise:float=1e-4,\n",
    "        task_rank = None,\n",
    "        output_rank = None,\n",
    "        lik_rank = None\n",
    "    ) -> None:\n",
    "\n",
    "        num_outputs = train_Y.shape[-1]\n",
    "        num_tasks = len(torch.unique(train_X[..., -1]))\n",
    "        self._num_tasks = num_tasks\n",
    "        self._num_outputs = num_outputs\n",
    "        \n",
    "        self.task_rank = task_rank if task_rank is not None else num_tasks\n",
    "        self.output_rank = output_rank if output_rank is not None else num_outputs\n",
    "        self.lik_rank = lik_rank if lik_rank is not None else 0\n",
    "        # initializing likelihood\n",
    "        likelihood = gpytorch.likelihoods.MultitaskGaussianLikelihood(num_tasks=num_outputs,rank = self.lik_rank)\n",
    "        super(MultiOutputMultiTaskGP, self).__init__(train_X, train_Y,likelihood)\n",
    "\n",
    "        self.likelihood.register_prior('raw_noise_prior',HorseshoePrior(0.01,lb_noise),'raw_noise')\n",
    "        if self.lik_rank == 0:\n",
    "            self.likelihood.register_prior('raw_task_noises_prior',HorseshoePrior(0.01,lb_noise),'raw_task_noises')    \n",
    "        else:\n",
    "            self.likelihood.register_prior('task_noise_covar_factor_prior',NormalPrior(0.,1),'task_noise_covar_factor')\n",
    "\n",
    "        if fix_noise:\n",
    "            self.likelihood.raw_noise.requires_grad_(False)\n",
    "            self.likelihood.noise_covar.noise =torch.tensor(4.9901e-05)\n",
    "\n",
    "        \n",
    "        #define prior for mean module\n",
    "        mean_list = [gpytorch.means.ConstantMean(NormalPrior(0,1)) for t in range(num_outputs)]\n",
    "        self.mean_module = gpytorch.means.MultitaskMean(\n",
    "            mean_list, num_tasks=num_outputs\n",
    "        )\n",
    "        \n",
    "        self.data_kernel = data_kernel\n",
    "        \n",
    "        \n",
    "        if not isinstance(data_kernel,gpytorch.kernels.Kernel):\n",
    "            raise RuntimeError(\n",
    "                \"specified data kernel is not a `gpytorch.kernels.Kernel` instance\"\n",
    "            )\n",
    "\n",
    "        #define kernel for gplvm on mixed variables\n",
    "        self.data_kernel2 = gpytorch.kernels.RBFKernel()\n",
    "        self.data_kernel2.register_prior(\n",
    "                    'lengthscale_prior',SmoothedBoxPrior(math.log(0.1),math.log(10)),'raw_lengthscale'\n",
    "                )\n",
    "\n",
    "        self.task_kernel = gpytorch.kernels.IndexKernel(num_tasks=num_tasks, rank = self.task_rank) #default rank is 1\n",
    "        self.output_kernel = gpytorch.kernels.IndexKernel(num_tasks=num_outputs, rank = self.output_rank) #default rank is 1\n",
    "        \n",
    "        self.task_kernel.register_prior(\"covar_factor_prior\",NormalPrior(0.,1),lambda m: m._parameters['covar_factor'])\n",
    "        self.task_kernel.register_prior(\"raw_var_prior\",NormalPrior(0.,1),lambda m: m._parameters['raw_var'])\n",
    "\n",
    "        self.output_kernel.register_prior(\"covar_factor_prior\",NormalPrior(0.,1),lambda m: m._parameters['covar_factor'])\n",
    "        self.output_kernel.register_prior(\"raw_var_prior\",NormalPrior(0.,1),lambda m: m._parameters['raw_var'])\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        task_term = self.task_kernel(x[..., -1].long())\n",
    "        data_and_task_x = self.data_kernel(x[..., :-1]).mul(task_term)\n",
    "        output_x = self.output_kernel.covar_matrix\n",
    "        covar_x = gpytorch.lazy.KroneckerProductLazyTensor(data_and_task_x, output_x)\n",
    "        return gpytorch.distributions.MultitaskMultivariateNormal(mean_x, covar_x)\n",
    "    \n",
    "    def predict(\n",
    "        self,x:torch.Tensor,return_std:bool=False,include_noise:bool=False\n",
    "    ):\n",
    "\n",
    "        self.eval()\n",
    "        self.likelihood.eval()\n",
    "        with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "            pred_res = self.likelihood(self.forward(x))   \n",
    "            mean = pred_res.mean\n",
    "            lower, upper = pred_res.confidence_region()\n",
    "        return mean, lower, upper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 1.0, 2.0, 3.0]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X=t_train_x\n",
    "temp = X.numpy()\n",
    "temp.ndim\n",
    "\n",
    "j = 1\n",
    "l = np.sort(np.unique(temp[..., j])).tolist()\n",
    "l\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setlevels(X, qual_index = None, return_label = False):\n",
    "    labels = []\n",
    "    if qual_index == []:\n",
    "        return X\n",
    "    if qual_index is None:\n",
    "        qual_index = list(range(X.shape[-1]))\n",
    "    # if type(X) == np.ndarray:\n",
    "    #     temp = torch.from_numpy(X).detach().clone()\n",
    "    \n",
    "    # Check if X is a PyTorch tensor\n",
    "    if isinstance(X, torch.Tensor):\n",
    "        # Move X to CPU if it's on CUDA\n",
    "        if X.is_cuda:\n",
    "            X = X.cpu()\n",
    "        temp = X.clone()\n",
    "    # Check if X is a NumPy array\n",
    "    elif isinstance(X, np.ndarray):\n",
    "        temp = np.copy(X)\n",
    "    else:\n",
    "        # Handle other types or raise an error\n",
    "        raise TypeError(\"X must be a PyTorch tensor or a NumPy array.\")\n",
    "\n",
    "    # Convert PyTorch tensor to NumPy array if needed\n",
    "    if isinstance(temp, torch.Tensor):\n",
    "        temp = temp.numpy()\n",
    "\n",
    "    if temp.ndim > 1:\n",
    "        for j in qual_index:\n",
    "            l = np.sort(np.unique(temp[..., j])).tolist()\n",
    "            labels.append(l)\n",
    "            #l =  torch.unique(temp[..., j], sorted = True).tolist()\n",
    "            temp[..., j] = torch.tensor([*map(lambda m: l.index(m),temp[..., j])])\n",
    "    else:\n",
    "            l = torch.unique(temp, sorted = True)\n",
    "            temp = torch.tensor([*map(lambda m: l.tolist().index(m), temp)])\n",
    "    \n",
    "    \n",
    "    if temp.dtype == object:\n",
    "        temp = temp.astype(float)\n",
    "        if type(X) == np.ndarray:\n",
    "            temp = torch.from_numpy(temp)\n",
    "        \n",
    "        if return_label:\n",
    "            return temp, labels\n",
    "        else:\n",
    "            return temp\n",
    "    else:\n",
    "        if type(X) == np.ndarray:\n",
    "            temp = torch.from_numpy(temp)\n",
    "        if return_label:\n",
    "            return temp, labels\n",
    "        else:\n",
    "            return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "## optimization function using torch\n",
    "def fit_model_torch(\n",
    "    model,\n",
    "    model_param_groups:Optional[List]=None,\n",
    "    lr_default:float=0.01,\n",
    "    num_iter:int=100,\n",
    "    num_restarts:int=0,\n",
    "    break_steps:int = 50) -> float:\n",
    "    '''Optimize the likelihood/posterior of a standard GP model using `torch.optim.Adam`.\n",
    "\n",
    "    :param model: A model instance derived from the `models.GPR` class.\n",
    "    :type model: models.GPR\n",
    "\n",
    "    :param model_param_groups: list of parameters to optimizes or dicts defining parameter\n",
    "        groups. If `None` is specified, then all parameters with `.requires_grad`=`True` are \n",
    "        included. Defaults to `None`.\n",
    "    :type model_param_groups: list, optional\n",
    "\n",
    "    :param lr_default: The default learning rate for all parameter groups. To use different \n",
    "        learning rates for some groups, specify them `model_param_groups`. \n",
    "    :type lr_default: float, optional\n",
    "\n",
    "    :param num_iter: The number of optimization steps from each starting point. This is the only\n",
    "        termination criterion for the optimizer.\n",
    "    :type num_iter: float, optional\n",
    "\n",
    "    :param num_restarts: The number of times to restart the local optimization from a \n",
    "        new starting point. Defaults to 5\n",
    "    :type num_restarts: int, optional\n",
    "\n",
    "    :returns: the best (negative) log-likelihood/log-posterior found\n",
    "    :rtype: float\n",
    "    '''  \n",
    "    model.train()\n",
    "    \n",
    "    # objective\n",
    "    #mll = ExactMarginalLogLikelihood(model.likelihood, model)\n",
    "    \n",
    "    \n",
    "    f_inc = math.inf\n",
    "    current_state_dict = model.state_dict()\n",
    "\n",
    "\n",
    "    loss_hist_total = []\n",
    "\n",
    "    for i in range(num_restarts+1):\n",
    "        optimizer = torch.optim.Adam(\n",
    "            model.parameters() if model_param_groups is None else model_param_groups, \n",
    "            lr=lr_default)\n",
    "        loss_hist = []\n",
    "        epochs_iter = tqdm(range(num_iter),desc='Epoch',position=0,leave=True)\n",
    "        for j in epochs_iter:\n",
    "            # zero gradients from previous iteration\n",
    "            optimizer.zero_grad()\n",
    "            # output from model\n",
    "            output = model(*model.train_inputs)\n",
    "            # calculate loss and backprop gradients\n",
    "            #loss = -mll(output,model.train_targets)\n",
    "            loss = -model.likelihood(output).log_prob(model.train_targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            acc_loss = loss.item()\n",
    "            desc = f'Epoch {j} - loss {acc_loss:.4f}'\n",
    "            epochs_iter.set_description(desc)\n",
    "            epochs_iter.update(1)\n",
    "            loss_hist.append(acc_loss)\n",
    "\n",
    "            if j > break_steps and j%break_steps == 0:\n",
    "                if ( (torch.mean(torch.Tensor(loss_hist)[j-break_steps:j]) - loss_hist[j]) <= 0 ):\n",
    "                    break\n",
    "        \n",
    "        loss_hist_total.append(loss_hist)\n",
    "\n",
    "        if loss.item()<f_inc:\n",
    "            current_state_dict = deepcopy(model.state_dict())\n",
    "            f_inc = loss.item()\n",
    "    \n",
    "    model.load_state_dict(current_state_dict)\n",
    "\n",
    "    return f_inc, loss_hist_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "class Linear_MAP(nn.Linear):\n",
    "    def __init__(self, in_features: int, out_features: int, bias: bool = True, device=None, dtype=None) -> None:\n",
    "        super().__init__(in_features, out_features, bias, device, dtype)\n",
    "        \n",
    "    def forward(self, input, transform = lambda x: x):\n",
    "        return F.linear(input,transform(self.weight), self.bias)\n",
    "\n",
    "######################################################################## GPLVM  #####################################################\n",
    "class GPLVM(nn.Module):\n",
    "    def __init__(self, GP_Plus, input_size, num_classes, name):\n",
    "        super(GPLVM, self).__init__()\n",
    "        \n",
    "        self.gplvm = Linear_MAP(input_size, num_classes, bias = False)\n",
    "        GP_Plus.register_parameter(name, self.gplvm.weight)\n",
    "        GP_Plus.register_prior(name = 'gplvm_prior_'+name, prior=gpytorch.priors.NormalPrior(0,1) , param_or_closure=name)\n",
    "\n",
    "    def forward(self, x, transform = lambda x: x):\n",
    "        x = self.gplvm(x, transform)\n",
    "        return x\n",
    "\n",
    "\n",
    "######################################################################## Other Classes Used in GP_Pluse  #####################################################\n",
    "class LMGP(nn.Module):\n",
    "    def __init__(self, GP_Plus, input_size, num_classes,name):\n",
    "        super(LMGP, self).__init__()\n",
    "        \n",
    "        \n",
    "        self.fci = Linear_MAP(input_size, num_classes, bias = False)\n",
    "        GP_Plus.register_parameter(name, self.fci.weight)\n",
    "        GP_Plus.register_prior(name = 'latent_prior_'+name, prior=gpytorch.priors.NormalPrior(0,1) , param_or_closure=name)\n",
    "\n",
    "    def forward(self, x, transform = lambda x: x):\n",
    "        x = self.fci(x, transform)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "##GP Plus\n",
    "\n",
    "\n",
    "class GP_Plus(MultiOutputMultiTaskGP):\n",
    "    \"\"\"The GP_Plus which extends GPs to learn nonlinear and probabilistic nmanifold, handle categorical inputs, and  ... ...\n",
    "\n",
    "    :note: Binary categorical variables should not be treated as qualitative inputs. There is no \n",
    "        benefit from applying a latent variable treatment for such variables. Instead, treat them\n",
    "        as numerical inputs.\n",
    "\n",
    "    :param train_x: The training inputs (size N x d). Qualitative inputs needed to be encoded as \n",
    "        integers 0,...,L-1 where L is the number of levels. For best performance, scale the \n",
    "        numerical variables to the unit hypercube.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_x:torch.Tensor,\n",
    "        train_y:torch.Tensor,\n",
    "        quant_correlation_class:str='RBFKernel',\n",
    "        my_nu = 2.5, #nu for MaternKernel\n",
    "        qual_ind_lev = {},\n",
    "        quant_index = [],\n",
    "        task_index = -1,\n",
    "        multiple_noise = False,\n",
    "        lv_dim:int=2,\n",
    "        fix_noise:bool=False,\n",
    "        fixed_length_scale:bool=False,\n",
    "        fixed_omega=torch.tensor([1.0]),\n",
    "        lb_noise:float=1e-4,\n",
    "        encoding_type = 'one-hot',\n",
    "        uniform_encoding_columns = 2,\n",
    "        lv_columns = [] ,\n",
    "        base='single_constant',\n",
    "        seed_number=1,\n",
    "        device=\"cpu\",\n",
    "        dtype= torch.float,\n",
    "        is_mix_reduce = False,\n",
    "        task_rank=None,\n",
    "        output_rank = None,\n",
    "        lik_rank=None\n",
    "    ) -> None:\n",
    "        \n",
    "        tkwargs = {}  # or dict()\n",
    "        tkwargs['dtype'] = dtype\n",
    "        tkwargs['device'] =torch.device(device)\n",
    "        self.tkwargs=tkwargs\n",
    "\n",
    "        if fixed_length_scale:\n",
    "            self.fixed_omega=fixed_omega.to(**self.tkwargs)\n",
    "        else:\n",
    "            self.fixed_omega=None\n",
    "\n",
    "        \n",
    "        train_x=self.fill_nan_with_mean(train_x)\n",
    "        ###############################################################################################\n",
    "        ###############################################################################################\n",
    "        self.seed=seed_number\n",
    "        self.calibration_source_index=0    ## It is supposed the calibration parameter is for high fidelity needs\n",
    "        qual_index = list(qual_ind_lev.keys())\n",
    "        all_index = set(range(train_x.shape[-1]))\n",
    "        #quant_index = list(all_index.difference(qual_index))\n",
    "        num_levels_per_var = list(qual_ind_lev.values())\n",
    "        #------------------- lm columns --------------------------\n",
    "        lm_columns = list(set(qual_index).difference(lv_columns))\n",
    "        if len(lm_columns) > 0:\n",
    "            qual_kernel_columns = [*lv_columns, lm_columns]\n",
    "        else:\n",
    "            qual_kernel_columns = lv_columns\n",
    "        #########################\n",
    "        if len(qual_index) > 0:\n",
    "            train_x = torch.tensor(setlevels(train_x, qual_index=qual_index))#.to(**self.tkwargs)\n",
    "        #\n",
    "        train_x=train_x.to(**self.tkwargs)\n",
    "        train_y=train_y.to(**self.tkwargs)\n",
    "        #train_y=train_y.reshape(-1)#.to(**self.tkwargs)\n",
    "        \n",
    "        if multiple_noise:\n",
    "            noise_indices = list(range(0,num_levels_per_var[-1]))\n",
    "        else:\n",
    "            noise_indices = []\n",
    "\n",
    "        if len(qual_index) == 1 and num_levels_per_var[0] < 2:\n",
    "            temp = quant_index.copy()\n",
    "            temp.append(qual_index[0])\n",
    "            quant_index = temp.copy()\n",
    "            qual_index = []\n",
    "            lv_dim = 0\n",
    "        elif len(qual_index) == 0:\n",
    "            lv_dim = 0\n",
    "\n",
    "        quant_correlation_class_name = quant_correlation_class\n",
    "\n",
    "        if len(qual_index) == 0:\n",
    "            lv_dim = 0\n",
    "    \n",
    "\n",
    "        if len(qual_index) > 0:\n",
    "            ####################### Defined multiple kernels for seperate variables ###################\n",
    "            qual_kernels = []\n",
    "            for i in range(len(qual_kernel_columns)):\n",
    "                qual_kernels.append(gpytorch.kernels.RBFKernel(\n",
    "                    active_dims=torch.arange(lv_dim) + lv_dim * i) )\n",
    "                #qual_kernels[i].initialize(**{'lengthscale':1.0})\n",
    "                #qual_kernels[i].raw_lengthscale.requires_grad_(False)  \n",
    "            qual_kernels[0].register_prior(\n",
    "                    'lengthscale_prior',NormalPrior(-3.0,3.0),'raw_lengthscale'\n",
    "                )\n",
    "\n",
    "        if len(quant_index) == 0:\n",
    "            print('---no numerical columns----')\n",
    "            correlation_kernel = qual_kernels[0]\n",
    "            for i in range(1, len(qual_kernels)):\n",
    "                correlation_kernel *= qual_kernels[i]\n",
    "        else:\n",
    "            try:\n",
    "                quant_correlation_class = getattr(gpytorch.kernels,quant_correlation_class)\n",
    "            except:\n",
    "                raise RuntimeError(\n",
    "                    \"%s not an allowed kernel\" % quant_correlation_class\n",
    "                )\n",
    "            if quant_correlation_class_name == 'RBFKernel':\n",
    "                quant_kernel = quant_correlation_class(\n",
    "                    ard_num_dims=len(quant_index),\n",
    "                    active_dims=len(qual_kernel_columns) * lv_dim+torch.arange(len(quant_index)),\n",
    "                    lengthscale_constraint= Positive(transform= torch.exp,inv_transform= torch.log)\n",
    "                )\n",
    "            \n",
    "            elif quant_correlation_class_name == 'MaternKernel':\n",
    "                quant_kernel = quant_correlation_class(\n",
    "                    nu = my_nu,\n",
    "                    ard_num_dims=len(quant_index),\n",
    "                    active_dims=len(qual_kernel_columns)*lv_dim+torch.arange(len(quant_index)),\n",
    "                    lengthscale_constraint= Positive(transform= lambda x: 2.0**(-0.5) * torch.pow(10,-x/2),inv_transform= lambda x: -2.0*torch.log10(x/2.0))\n",
    "                )\n",
    "                #####################\n",
    "            \n",
    "            if quant_correlation_class_name == 'RBFKernel':\n",
    "                quant_kernel.register_prior(\n",
    "                    'lengthscale_prior', SmoothedBoxPrior(math.log(0.1),math.log(10)),'raw_lengthscale'\n",
    "                )\n",
    "                \n",
    "            elif quant_correlation_class_name == 'MaternKernel':\n",
    "                quant_kernel.register_prior(\n",
    "                    #'lengthscale_prior', SmoothedBoxPrior(math.log(0.1),math.log(10)),'raw_lengthscale'\n",
    "                    'lengthscale_prior',NormalPrior(-3.0,3.0),'raw_lengthscale'\n",
    "                )\n",
    "            \n",
    "            #########Product between qual kernels and quant kernels############\n",
    "            if len(qual_index) > 0:\n",
    "                temp = qual_kernels[0]\n",
    "                for i in range(1, len(qual_kernels)):\n",
    "                    temp *= qual_kernels[i]\n",
    "                correlation_kernel = temp*quant_kernel #+ qual_kernel + quant_kernel\n",
    "            else:\n",
    "                correlation_kernel = quant_kernel\n",
    "            \n",
    "        \n",
    "        super(GP_Plus,self).__init__(\n",
    "            train_X=train_x,train_Y=train_y,noise_indices=noise_indices,\n",
    "            data_kernel=correlation_kernel,\n",
    "            fix_noise=fix_noise,lb_noise=lb_noise,\n",
    "            task_rank= task_rank, output_rank = output_rank, lik_rank = lik_rank\n",
    "        )\n",
    "        \n",
    "        # register index and transforms\n",
    "        self.register_buffer('quant_index',torch.tensor(quant_index))\n",
    "        self.register_buffer('qual_index',torch.tensor(qual_index))\n",
    "\n",
    "        self.quant_kernel = quant_kernel\n",
    "        self.correlation_kernel = correlation_kernel\n",
    "        self.qual_kernels = qual_kernels\n",
    "        \n",
    "        self.qual_kernel_columns = qual_kernel_columns\n",
    "        self.is_mix_reduce = is_mix_reduce\n",
    "        ######################## latent variable mapping  ########################\n",
    "        self.num_levels_per_var = num_levels_per_var\n",
    "        self.lv_dim = lv_dim\n",
    "        self.uniform_encoding_columns = uniform_encoding_columns\n",
    "        self.encoding_type = encoding_type\n",
    "        self.perm =[]\n",
    "        self.zeta = []\n",
    "        self.random_zeta=[]\n",
    "        self.perm_dict = []\n",
    "        self.A_matrix = []\n",
    "        \n",
    "        self.epsilon=None\n",
    "        self.epsilon_f=None\n",
    "        self.embeddings_Dtrain=[]\n",
    "        self.count=train_x.size()[0]\n",
    "        if len(qual_kernel_columns) > 0:\n",
    "            for i in range(len(qual_kernel_columns)):\n",
    "                if type(qual_kernel_columns[i]) == int:\n",
    "                    num = self.num_levels_per_var[qual_index.index(qual_kernel_columns[i])]\n",
    "                    cat = [num]\n",
    "                else:\n",
    "                    cat = [self.num_levels_per_var[qual_index.index(k)] for k in qual_kernel_columns[i]]\n",
    "                    num = sum(cat)\n",
    "\n",
    "                zeta, perm, perm_dict = self.zeta_matrix(num_levels=cat, lv_dim = self.lv_dim)\n",
    "                self.zeta.append(zeta)\n",
    "                self.perm.append(perm)\n",
    "                self.perm_dict.append(perm_dict)       \n",
    "                ###################################  latent map #################################   \n",
    "                model_lmgp = LMGP(self, input_size= num, num_classes=lv_dim, \n",
    "                        name ='latent'+ str(qual_kernel_columns[i])).to(**self.tkwargs)\n",
    "                self.A_matrix.append(model_lmgp)\n",
    "        ######## GPLVM transformation if there are quantitative variables########################\n",
    "        \n",
    "        if is_mix_reduce and len(quant_index) > 0:\n",
    "            dim_x_new = self.lv_dim+len(quant_index)\n",
    "            self.W_matrix= GPLVM(self, input_size=  dim_x_new, num_classes=lv_dim, \n",
    "                        name ='gplvm'+ str(dim_x_new)).to(**self.tkwargs)\n",
    "        \n",
    "        ##################################################################################\n",
    "        if fixed_length_scale == True:\n",
    "            self.covar_module.base_kernel.raw_lengthscale.data = self.fixed_omega #torch.tensor([self.omega, self.omega])  # Set the desired value\n",
    "            self.covar_module.base_kernel.raw_lengthscale.requires_grad = False  # Fix the hyperparameter\n",
    "        ###################################  Mean Function #################################   \n",
    "        #i=0\n",
    "        self.base=base\n",
    "        self.num_sources=int(torch.max(train_x[:,-1]))\n",
    "        size=train_x.shape[1]\n",
    "        self.single_base_register(size,base_type=self.base,wm='mean_module')\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self,x:torch.Tensor):\n",
    "        \n",
    "        Numper_of_pass=1\n",
    "        \n",
    "        size_sigma_sum = x.size(0)*self._num_outputs\n",
    "        #size_sigma_sum = x.size(0)\n",
    "        Sigma_sum=torch.zeros(size_sigma_sum,size_sigma_sum, dtype=torch.float64).to(self.tkwargs['device'])\n",
    "        \n",
    "        mean_x_sum=torch.zeros(x.size(0),self._num_outputs, dtype=torch.float64).to(self.tkwargs['device'])\n",
    "        #print('mean_x_sum.shape',mean_x_sum.shape)\n",
    "        task_term = self.task_kernel(x[..., -1].long())\n",
    "\n",
    "        for NP in range(Numper_of_pass):\n",
    "            x_forward_raw=x[..., :-1].clone()\n",
    "            nd_flag = 0\n",
    "            if x.dim() > 2:\n",
    "                xsize = x.shape\n",
    "                x = x.reshape(-1, x.shape[-1])\n",
    "                nd_flag = 1\n",
    "            \n",
    "            x_new= x\n",
    "            x_gplvm = x\n",
    "            if len(self.qual_kernel_columns) > 0:\n",
    "                embeddings = []\n",
    "                for i in range(len(self.qual_kernel_columns)):\n",
    "                    temp= self.transform_categorical(x=x[:,self.qual_kernel_columns[i]].clone().type(torch.int64).to(self.tkwargs['device']), \n",
    "                        perm_dict = self.perm_dict[i], zeta = self.zeta[i])\n",
    "                dimm=x_forward_raw.size()[0]\n",
    "                \n",
    "                embeddings.append(self.A_matrix[i](temp.float().to(**self.tkwargs)))\n",
    "                x_new= torch.cat([embeddings[0],x[...,self.quant_index.long()].to(**self.tkwargs)],dim=-1)\n",
    "                #print('x_new.shape after reduce cate',x_new.shape)\n",
    "            \n",
    "            if self.is_mix_reduce and len(self.quant_index) > 0:\n",
    "                x_gplvm = self.W_matrix(x_new.float().to(**self.tkwargs))\n",
    "                #print('x_gplvm.shape after reduce all',x_gplvm.shape)\n",
    "                #x_new = x_gplvm\n",
    "            \n",
    "            if nd_flag == 1:\n",
    "                x_new = x_new.reshape(*xsize[:-1], -1)\n",
    "            \n",
    "        #################### Multiple baises (General Case) ####################################  \n",
    "            \n",
    "            if self.is_mix_reduce and len(self.quant_index) > 0:\n",
    "                    mean_x = self.mean_module(x_gplvm).to(**self.tkwargs)\n",
    "            else:\n",
    "                    mean_x = self.mean_module(x_new).to(**self.tkwargs)\n",
    "            \n",
    "            #data_kernel is a product kernel of cate kernel and quant kernel\n",
    "            data_and_task_x = self.data_kernel(x_new).mul(task_term)\n",
    "            output_x = self.output_kernel.covar_matrix\n",
    "            covar_x = gpytorch.lazy.KroneckerProductLazyTensor(data_and_task_x, output_x)\n",
    "\n",
    "            if self.is_mix_reduce and len(self.quant_index) > 0:\n",
    "                #data_kernel2 is one kernel for both cate and quant variables\n",
    "                data_and_task_x_gplvm = self.data_kernel2(x_gplvm).mul(task_term)\n",
    "                covar_x_mixed = gpytorch.lazy.KroneckerProductLazyTensor(data_and_task_x_gplvm, output_x)\n",
    "                covar_x = covar_x.mul(covar_x_mixed)\n",
    "\n",
    "            mean_x_sum+=mean_x\n",
    "            \n",
    "            Sigma_sum += covar_x.evaluate()\n",
    "\n",
    "        # End of the loop for forward pasess ----> Compute ensemble mean and covariance\n",
    "        k = Numper_of_pass\n",
    "        ensemble_mean = mean_x_sum/k\n",
    "        ensemble_covar = torch.zeros_like(Sigma_sum) \n",
    "        ensemble_covar= Sigma_sum/k\n",
    "        ensemble_covar=gpytorch.lazy.NonLazyTensor(ensemble_covar)\n",
    "        Sigma_sum=0\n",
    "        #print('ensemble_mean.shape',ensemble_mean.shape)\n",
    "        #print('ensemble_covar.shape',ensemble_covar.evaluate().shape)\n",
    "\n",
    "        return gpytorch.distributions.MultitaskMultivariateNormal(ensemble_mean.float(),ensemble_covar.float())\n",
    "    \n",
    "    ################################################################ Mean Functions #####################################################################################\n",
    "    \n",
    "    def single_base_register(self,size=1,base_type='single_zero',wm='mean_module'):\n",
    "        if base_type=='single_constant':\n",
    "            #mean_list = [gpytorch.means.ConstantMean(NormalPrior(0.,1)) for t in range(self._num_tasks)]\n",
    "            setattr(self,wm, self.mean_module)\n",
    "        elif base_type=='single_zero':\n",
    "            setattr(self,wm, gpytorch.means.ZeroMean()) \n",
    "\n",
    "    ################################################################ Fit #####################################################################################\n",
    "    def fit(self,add_prior:bool=True,num_restarts:int=64,theta0_list:Optional[List[np.ndarray]]=None,jac:bool=True,\n",
    "            options:Dict={},n_jobs:int=-1,method = 'L-BFGS-B',constraint=False,bounds=False,regularization_parameter:List[int]=[0,0],optim_type='scipy'):\n",
    "        print(\"## Learning the model's parameters has started ##\")\n",
    "        #optim_type=='adam_torch':\n",
    "        fit_model_torch(model=self,\n",
    "                    model_param_groups=None,\n",
    "                    lr_default=0.01,\n",
    "                    num_iter=100,\n",
    "                    num_restarts=num_restarts,\n",
    "                    break_steps= 50)\n",
    "        \n",
    "        print(\"## Learning the model's parameters is successfully finished ##\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def fill_nan_with_mean(self,train_x):\n",
    "        # Check if there are any NaNs in the tensor\n",
    "        if torch.isnan(train_x).any():\n",
    "            # Compute the mean of non-NaN elements column-wise\n",
    "            col_means = torch.nanmean(train_x, dim=0)\n",
    "            # Find indices where NaNs are located\n",
    "            nan_indices = torch.isnan(train_x)\n",
    "            # Replace NaNs with the corresponding column-wise mean\n",
    "            train_x[nan_indices] = col_means.repeat(train_x.shape[0], 1)[nan_indices]\n",
    "\n",
    "        return train_x\n",
    "    ############################  Prediction and Visualization  ###############################\n",
    "    \n",
    "    def predict(self, Xtest,return_std=True, include_noise = True):\n",
    "        with torch.no_grad():\n",
    "            return super().predict(Xtest, return_std = return_std, include_noise= include_noise)\n",
    "    \n",
    "    def predict_with_grad(self, Xtest,return_std=True, include_noise = True):\n",
    "        return super().predict(Xtest, return_std = return_std, include_noise= include_noise)\n",
    "    \n",
    "    @classmethod\n",
    "    def show(cls):\n",
    "        plt.show()\n",
    "        \n",
    "    def get_params(self, name = None):\n",
    "        params = {}\n",
    "        print('###################Parameters###########################')\n",
    "        for n, value in self.named_parameters():\n",
    "             params[n] = value\n",
    "        if name is None:\n",
    "            print(params)\n",
    "            return params\n",
    "        else:\n",
    "            if name == 'Mean':\n",
    "                key = 'mean_module.constant'\n",
    "            elif name == 'Sigma':\n",
    "                key = 'covar_module.raw_outputscale'\n",
    "            elif name == 'Noise':\n",
    "                key = 'likelihood.noise_covar.raw_noise'\n",
    "            elif name == 'Omega':\n",
    "                for n in params.keys():\n",
    "                    if 'raw_lengthscale' in n and params[n].numel() > 1:\n",
    "                        key = n\n",
    "            print(params[key])\n",
    "            return params[key]\n",
    "\n",
    "    def zeta_matrix(self,\n",
    "        num_levels:int,\n",
    "        lv_dim:int,\n",
    "        batch_shape=torch.Size()\n",
    "    ) -> None:\n",
    "\n",
    "        if any([i == 1 for i in num_levels]):\n",
    "            raise ValueError('Categorical variable has only one level!')\n",
    "\n",
    "        if lv_dim == 1:\n",
    "            raise RuntimeWarning('1D latent variables are difficult to optimize!')\n",
    "        \n",
    "        for level in num_levels:\n",
    "            if lv_dim > level - 0:\n",
    "                lv_dim = min(lv_dim, level-1)\n",
    "                raise RuntimeWarning(\n",
    "                    'The LV dimension can atmost be num_levels-1. '\n",
    "                    'Setting it to %s in place of %s' %(level-1,lv_dim)\n",
    "                )\n",
    "    \n",
    "        from itertools import product\n",
    "        levels = []\n",
    "        for l in num_levels:\n",
    "            levels.append(torch.arange(l))\n",
    "\n",
    "        perm = list(product(*levels))\n",
    "        perm = torch.tensor(perm, dtype=torch.int64)\n",
    "\n",
    "        #-------------Mapping-------------------------\n",
    "        perm_dic = {}\n",
    "        for i, row in enumerate(perm):\n",
    "            temp = str(row.tolist())\n",
    "            if temp not in perm_dic.keys():\n",
    "                perm_dic[temp] = i\n",
    "\n",
    "        #-------------One_hot_encoding------------------\n",
    "        for ii in range(perm.shape[-1]):\n",
    "            if perm[...,ii].min() != 0:\n",
    "                perm[...,ii] -= perm[...,ii].min()\n",
    "            \n",
    "        perm_one_hot = []\n",
    "        for i in range(perm.size()[1]):\n",
    "            perm_one_hot.append( torch.nn.functional.one_hot(perm[:,i]) )\n",
    "\n",
    "        perm_one_hot = torch.concat(perm_one_hot, axis=1)\n",
    "\n",
    "        return perm_one_hot, perm, perm_dic\n",
    "\n",
    "    #################################### transformation functions####################################\n",
    "\n",
    "    def transform_categorical(self, x:torch.Tensor,perm_dict = [], zeta = []) -> None:\n",
    "        if x.dim() == 1:\n",
    "            x = x.reshape(-1,1)\n",
    "        # categorical should start from 0\n",
    "        if self.training == False:\n",
    "            x = torch.tensor(setlevels(x))\n",
    "        if self.encoding_type == 'one-hot':\n",
    "            index = [perm_dict[str(row.tolist())] for row in x]\n",
    "\n",
    "            if x.dim() == 1:\n",
    "                x = x.reshape(len(x),)\n",
    "\n",
    "            return zeta[index,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-191.8942, grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#lik_rank_option = 1\n",
    "gp_model = GP_Plus(t_train_x, t_train_y, qual_ind_lev=qual_ind_lev,quant_index = [0],\n",
    "                   quant_correlation_class= 'RBFKernel',is_mix_reduce= False,\n",
    "                   lik_rank= lik_rank_option)\n",
    "output = gp_model(*gp_model.train_inputs)\n",
    "print(gp_model.likelihood(output).log_prob(gp_model.train_targets))\n",
    "#gp_model.likelihood(output).log_prob(gp_model.train_targets.contiguous())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Learning the model's parameters has started ##\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 99 - loss 98.1493: 100%|██████████| 100/100 [00:01<00:00, 85.78it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Learning the model's parameters is successfully finished ##\n"
     ]
    }
   ],
   "source": [
    "#gp_model.fit(bounds=True)\n",
    "#gp_model.fit(bounds=False,)\n",
    "gp_model.fit(optim_type='adam_torch',num_restarts = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-97.7088, grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#likelihood after optimization\n",
    "output = gp_model(*gp_model.train_inputs)\n",
    "print(gp_model.likelihood(output).log_prob(gp_model.train_targets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set into eval mode\n",
    "gp_model.eval()\n",
    "gp_model.likelihood.eval()\n",
    "\n",
    "\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "        test_pred = gp_model.likelihood(gp_model(t_test_x))   \n",
    "        test_mean = test_pred.mean\n",
    "        test_lower, test_upper = test_pred.confidence_region()\n",
    "        train_pred = gp_model.likelihood(gp_model(t_train_x))\n",
    "        train_mean = train_pred.mean\n",
    "        train_lower, train_upper = train_pred.confidence_region()\n",
    "\n",
    "        #transformed categorical value\n",
    "        x_test_cate_zeta = gp_model.transform_categorical(t_test_x[:,gp_model.qual_kernel_columns[0]].clone().type(torch.int64).to('cpu'),\n",
    "                                                  perm_dict = gp_model.perm_dict[0], zeta = gp_model.zeta[0])\n",
    "        x_test_cate_latent = gp_model.A_matrix[0](x_test_cate_zeta.float().to('cpu'))\n",
    "        #print(x_test_cate_latent)\n",
    "        x_test_new= torch.cat([x_test_cate_latent,t_test_x[...,gp_model.quant_index.long()].to(**gp_model.tkwargs)],dim=-1)\n",
    "        \n",
    "\n",
    "        x_train_cate_zeta = gp_model.transform_categorical(t_train_x[:,gp_model.qual_kernel_columns[0]].clone().type(torch.int64).to('cpu'),\n",
    "                                                  perm_dict = gp_model.perm_dict[0], zeta = gp_model.zeta[0])\n",
    "        x_train_cate_latent = gp_model.A_matrix[0](x_train_cate_zeta.float().to('cpu'))\n",
    "        x_train_new= torch.cat([x_train_cate_latent,t_train_x[...,gp_model.quant_index.long()].to(**gp_model.tkwargs)],dim=-1)\n",
    "        if gp_model.is_mix_reduce:\n",
    "                x_test_gplvm = gp_model.W_matrix(x_test_new.float().to('cpu'))\n",
    "                x_train_gplvm = gp_model.W_matrix(x_train_new.float().to('cpu'))\n",
    "        #print(x_train_cate_latent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_test_y = torch.Tensor(df_y_test.to_numpy())\n",
    "if y_scale_label=='no-y-scale':\n",
    "    arr_train_x = t_train_x.detach().numpy()\n",
    "    arr_train_y = t_train_y.detach().numpy()\n",
    "    arr_test_x = t_test_x.detach().numpy()\n",
    "    arr_test_y = t_test_y.detach().numpy()\n",
    "    arr_test_mean = test_mean.detach().numpy()\n",
    "    arr_test_lower = test_lower.detach().numpy()\n",
    "    arr_test_upper = test_upper.detach().numpy()\n",
    "\n",
    "    arr_train_mean = train_mean.detach().numpy()\n",
    "    arr_train_lower = train_lower.detach().numpy()\n",
    "    arr_train_upper = train_upper.detach().numpy()\n",
    "else:\n",
    "    arr_train_x = t_train_x.detach().numpy()\n",
    "    arr_test_x = t_test_x.detach().numpy()\n",
    "    arr_test_y = t_test_y.detach().numpy()\n",
    "\n",
    "    arr_train_y = np.zeros((t_train_y.shape[0],2))\n",
    "    arr_train_mean = np.zeros((t_train_y.shape[0],2))\n",
    "    arr_train_lower = np.zeros((t_train_y.shape[0],2))\n",
    "    arr_train_upper = np.zeros((t_train_y.shape[0],2))\n",
    "    arr_test_mean  = np.zeros((t_test_y.shape[0],2))\n",
    "    arr_test_lower = np.zeros((t_test_y.shape[0],2))\n",
    "    arr_test_upper = np.zeros((t_test_y.shape[0],2))\n",
    "\n",
    "    for task_ind in range(3):\n",
    "        y_task_scaler = ls_y_task_scaler[task_ind]\n",
    "        arr_train_y_task = y_task_scaler.inverse_transform(t_train_y[ls_row_idx_train[task_ind]])\n",
    "        arr_train_mean_task  = y_task_scaler.inverse_transform(train_mean[ls_row_idx_train[task_ind]])\n",
    "        arr_train_lower_task  = y_task_scaler.inverse_transform(train_lower[ls_row_idx_train[task_ind]])\n",
    "        arr_train_upper_task  = y_task_scaler.inverse_transform(train_upper[ls_row_idx_train[task_ind]])\n",
    "        arr_test_mean_task  = y_task_scaler.inverse_transform(test_mean[ls_row_idx_test[task_ind]])\n",
    "        arr_test_lower_task  = y_task_scaler.inverse_transform(test_lower[ls_row_idx_test[task_ind]])\n",
    "        arr_test_upper_task  = y_task_scaler.inverse_transform(test_upper[ls_row_idx_test[task_ind]])\n",
    "        arr_train_y[[ls_row_idx_train[task_ind]]] = arr_train_y_task\n",
    "        arr_train_mean[[ls_row_idx_train[task_ind]]] = arr_train_mean_task\n",
    "        arr_train_lower[[ls_row_idx_train[task_ind]]] = arr_train_lower_task\n",
    "        arr_train_upper[[ls_row_idx_train[task_ind]]] = arr_train_upper_task\n",
    "\n",
    "        arr_test_mean[[ls_row_idx_test[task_ind]]] = arr_test_mean_task\n",
    "        arr_test_lower[[ls_row_idx_test[task_ind]]] = arr_test_lower_task\n",
    "        arr_test_upper[[ls_row_idx_test[task_ind]]] = arr_test_upper_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "uncertainty_figure_option = 'errorbar' #shade or errorbar\n",
    "is_set_axis_limit = False\n",
    "cols_target_new = ['_Dimer_HMW_1', '_Monomer_1',\n",
    "       '_Dimer_HMW_2', '_Monomer_2', \n",
    "       '_Dimer_HMW_3','_Monomer_3']\n",
    "\n",
    "plot_axis_lb = total_df_y_long.min(axis = 0) - 0.4 * total_df_y_long.std(axis = 0)\n",
    "plot_axis_ub = total_df_y_long.max(axis = 0) + 0.4 * total_df_y_long.std(axis = 0)\n",
    "\n",
    "#num_outputs = arr_train_y.shape[-1]\n",
    "#num_tasks = len(np.unique(arr_train_x[..., -1]))\n",
    "num_outputs = len(cols_target)\n",
    "num_tasks = 3\n",
    "\n",
    "num_total_output = num_outputs*num_tasks\n",
    "arr_train_metrics = np.zeros((11,num_total_output))\n",
    "arr_test_metrics = np.zeros((11,num_total_output))\n",
    "\n",
    "# Initialize plots\n",
    "#f, y_axes = plt.subplots(num_tasks, num_outputs, figsize=(16, 48))\n",
    "f, y_axes = plt.subplots(num_tasks, num_outputs, figsize=(num_outputs*8, num_tasks*7))\n",
    "y_axes = y_axes.ravel()\n",
    "for task_ind in range(num_tasks):\n",
    "  for j,_ in enumerate(cols_target):\n",
    "    a = j + len(cols_target)*task_ind\n",
    "    tmp_col_y = cols_target_new[a]\n",
    "\n",
    "    r_train_y = arr_train_y[arr_train_x[:,-1]==task_ind,j]\n",
    "    r_train_mean_y =arr_train_mean[arr_train_x[:,-1]==task_ind,j]\n",
    "    r_train_lower_y =arr_train_lower[arr_train_x[:,-1]==task_ind,j]\n",
    "    r_train_upper_y =arr_train_upper[arr_train_x[:,-1]==task_ind,j]\n",
    "    r_train_std = r_train_mean_y - r_train_lower_y\n",
    "\n",
    "    r_test_y = arr_test_y[arr_test_x[:,-1]==task_ind,j]\n",
    "    r_test_mean_y =arr_test_mean[arr_test_x[:,-1]==task_ind,j]\n",
    "    r_test_lower_y =arr_test_lower[arr_test_x[:,-1]==task_ind,j]\n",
    "    r_test_upper_y =arr_test_upper[arr_test_x[:,-1]==task_ind,j]\n",
    "    r_test_std = r_test_mean_y - r_test_lower_y\n",
    "    \n",
    "    train_comp = np.concatenate((r_train_y.reshape(-1,1),r_train_mean_y.reshape(-1,1)),axis = 1)\n",
    "    df_train_comp = pd.DataFrame(train_comp,columns = ['true','pred'])\n",
    "    df_train_comp['upper'] = r_train_upper_y\n",
    "    df_train_comp['lower'] = r_train_lower_y\n",
    "    df_train_comp['mode'] = 'train'\n",
    "    \n",
    "    test_comp = np.concatenate((r_test_y.reshape(-1,1),r_test_mean_y.reshape(-1,1)),axis = 1)\n",
    "    df_test_comp = pd.DataFrame(test_comp,columns = ['true','pred'])\n",
    "    df_test_comp['upper'] = r_test_upper_y\n",
    "    df_test_comp['lower'] = r_test_lower_y\n",
    "    df_test_comp['mode'] = 'test'\n",
    "    \n",
    "    df_comp = pd.concat([df_train_comp,df_test_comp])\n",
    "    df_comp_sorted = df_comp.sort_values(by = ['true'],ascending=True)\n",
    "\n",
    "    #plot a parity line\n",
    "    y_axes[a].plot(df_comp_sorted['true'], df_comp_sorted['true'], '--',c = 'black')\n",
    "    \n",
    "    # Plot training data as blue stars\n",
    "    y_axes[a].plot(df_train_comp['true'], df_train_comp['pred'], 'k*',c = 'blue',markersize=10)\n",
    "\n",
    "    # Plot training data as red stars\n",
    "    y_axes[a].plot(df_test_comp['true'], df_test_comp['pred'], 'k*',c = 'red',markersize=15)\n",
    "    # Predictive mean as blue line\n",
    "    y_axes[a].plot(df_comp_sorted['true'], df_comp_sorted['pred'],c = 'blue')\n",
    "    \n",
    "    if uncertainty_figure_option == 'shade':\n",
    "    # Shade in confidence\n",
    "      y_axes[a].fill_between(x = df_comp_sorted['true'],y1 = df_comp_sorted['lower'], y2 = df_comp_sorted['upper'], color='b', alpha=.15)\n",
    "    else:\n",
    "      yerr = df_comp_sorted['pred'] - df_comp_sorted['lower']\n",
    "      yerr = yerr.values.tolist()\n",
    "      #y_axes[a].errorbar(x = df_comp_sorted['true'], y = df_comp_sorted['pred'], yerr = yerr, capsize=1, fmt='none', ecolor = 'black')\n",
    "      yerr_train = df_train_comp['pred'] - df_train_comp['lower']\n",
    "      yerr_train = yerr_train.values.tolist()\n",
    "      yerr_test = df_test_comp['pred'] - df_test_comp['lower']\n",
    "      yerr_test = yerr_test.values.tolist()\n",
    "      y_axes[a].errorbar(x = df_train_comp['true'], y = df_train_comp['pred'], yerr = yerr_train, capsize=1, fmt='none', ecolor = 'blue')\n",
    "      y_axes[a].errorbar(x = df_test_comp['true'], y = df_test_comp['pred'], yerr = yerr_test, capsize=1, fmt='none', ecolor = 'red')\n",
    "    \n",
    "    if is_set_axis_limit:\n",
    "      y_axes[a].set_xlim([plot_axis_lb[j],plot_axis_ub[j]])\n",
    "      y_axes[a].set_ylim([plot_axis_lb[j],plot_axis_ub[j]])\n",
    "    else:\n",
    "      arr_comp = df_comp[df_comp.columns.difference(['mode'])].to_numpy()\n",
    "      axis_max = arr_comp.max()\n",
    "      axis_min = arr_comp.min()\n",
    "      y_axes[a].set_xlim([axis_min,axis_max])\n",
    "      y_axes[a].set_ylim([axis_min,axis_max])\n",
    "\n",
    "    y_axes[a].legend(['parity','train','test','GP Mean','GP Confidence'])\n",
    "    y_axes[a].set_title(tmp_col_y)\n",
    "    y_axes[a].set_xlabel('actual')\n",
    "    y_axes[a].set_ylabel('pred')\n",
    "\n",
    "    y_true = r_test_y\n",
    "    y_pred = r_test_mean_y\n",
    "    y_train = r_train_y\n",
    "    mean_train = r_train_mean_y\n",
    "    \n",
    "\n",
    "    arr_test_metrics[0,a] = np.round(metrics.mean_absolute_error(y_true, y_pred),2)\n",
    "    arr_test_metrics[1,a] = np.round(metrics.median_absolute_error(y_true, y_pred),2)\n",
    "    arr_test_metrics[2,a] = np.round(metrics.mean_squared_error(y_true, y_pred),2)\n",
    "    arr_test_metrics[3,a] = round(metrics.root_mean_squared_error(y_true, y_pred),2)\n",
    "    arr_test_metrics[4,a] = round(metrics.mean_absolute_percentage_error(y_true, y_pred),2)\n",
    "    arr_test_metrics[5,a] = round(metrics.max_error(y_true, y_pred),2)\n",
    "    arr_test_metrics[6,a] = round(metrics.explained_variance_score(y_true, y_pred),2)\n",
    "    arr_test_metrics[7,a] = round(metrics.r2_score(y_true, y_pred),2)\n",
    "    arr_test_metrics[8,a] = round(np.mean(r_test_std)/np.mean(y_true),2)\n",
    "    arr_test_metrics[9,a] = round(np.min(r_test_std)/np.mean(y_true),2)\n",
    "    arr_test_metrics[10,a] = round(np.max(r_test_std)/np.mean(y_true),2)\n",
    "\n",
    "    arr_train_metrics[0,a] = round(metrics.mean_absolute_error(y_train, mean_train),2)\n",
    "    arr_train_metrics[1,a] = round(metrics.median_absolute_error(y_train, mean_train),2)\n",
    "    arr_train_metrics[2,a] = round(metrics.mean_squared_error(y_train, mean_train),2)\n",
    "    arr_train_metrics[3,a] = round(metrics.root_mean_squared_error(y_train, mean_train),2)\n",
    "    arr_train_metrics[4,a] = round(metrics.mean_absolute_percentage_error(y_train, mean_train),2)\n",
    "    arr_train_metrics[5,a] = round(metrics.max_error(y_train, mean_train),2)\n",
    "    arr_train_metrics[6,a] = round(metrics.explained_variance_score(y_train, mean_train),2)\n",
    "    arr_train_metrics[7,a] = round(metrics.r2_score(y_train, mean_train),2)\n",
    "    arr_train_metrics[8,a] = round(np.mean(r_train_std)/np.mean(y_train),2)\n",
    "    arr_train_metrics[9,a] = round(np.min(r_train_std)/np.mean(y_train),2)\n",
    "    arr_train_metrics[10,a] = round(np.max(r_train_std)/np.mean(y_train),2)\n",
    "    #arr_train_metrics[11,i] = my_gpr.kernel_.get_params()['k1__constant_value']\n",
    "    #arr_train_metrics[12,i] = my_gpr.kernel_.get_params()['k2__length_scale']\n",
    "\n",
    "\n",
    "if uncertainty_figure_option == 'shade' and is_set_axis_limit:\n",
    "  plt.savefig(figPath+'/true-pred-shade.jpg')\n",
    "elif uncertainty_figure_option == 'shade' and ~is_set_axis_limit:\n",
    "  plt.savefig(figPath+'/true-pred-shade-zoomin.jpg')\n",
    "elif uncertainty_figure_option == 'errorbar' and is_set_axis_limit:\n",
    "  plt.savefig(figPath+'/true-pred-errorbar.jpg')\n",
    "elif uncertainty_figure_option == 'errorbar' and ~is_set_axis_limit:\n",
    "  plt.savefig(figPath+'/true-pred-errorbar-zoomin.jpg')\n",
    "\n",
    "df_test_metrics = pd.DataFrame(arr_test_metrics,columns = cols_target_new, \n",
    "                               index = ['MAE','MAE2','MSE','RMSE','MAPE','MAXE','EVS','R2','AVG_STD_PCT','MIN_STD_PCT','MAX_STD_PCT'])\n",
    "print(df_test_metrics)\n",
    "\n",
    "df_train_metrics = pd.DataFrame(arr_train_metrics,columns = cols_target_new, \n",
    "                               index = ['MAE','MAE2','MSE','RMSE','MAPE','MAXE','EVS','R2','AVG_STD_PCT','MIN_STD_PCT','MAX_STD_PCT'])\n",
    "print(df_train_metrics)\n",
    "\n",
    "\n",
    "df_train_metrics.to_csv(figPath+'/df_train_metrics.csv')\n",
    "df_test_metrics.to_csv(figPath+'/df_test_metrics.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local-gp-mac-no-tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
